1. [1 Introduction](https://arxiv.org/html/2504.15909v2#S1 "In Synergizing RAG and Reasoning: A Systematic Review")
2. [2 Overview](https://arxiv.org/html/2504.15909v2#S2 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [2.1 Definition](https://arxiv.org/html/2504.15909v2#S2.SS1 "In 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [2.2 Taxonomy](https://arxiv.org/html/2504.15909v2#S2.SS2 "In 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [2.2.1 Synergy Purpose](https://arxiv.org/html/2504.15909v2#S2.SS2.SSS1 "In 2.2. Taxonomy ‣ 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [2.2.2 Synergy Paradigms](https://arxiv.org/html/2504.15909v2#S2.SS2.SSS2 "In 2.2. Taxonomy ‣ 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [2.2.3 Synergy Implementation](https://arxiv.org/html/2504.15909v2#S2.SS2.SSS3 "In 2.2. Taxonomy ‣ 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")
3. [3 The purpose of the synergy](https://arxiv.org/html/2504.15909v2#S3 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [3.1 Reasoning-Augmented Retrieval](https://arxiv.org/html/2504.15909v2#S3.SS1 "In 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [3.1.1 Semantic Disparities Between Queries and Documents](https://arxiv.org/html/2504.15909v2#S3.SS1.SSS1 "In 3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [3.1.2 Inflexible Intent Disambiguation](https://arxiv.org/html/2504.15909v2#S3.SS1.SSS2 "In 3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [3.1.3 Inefficient Coordination of Multi-Source Heterogeneous Data](https://arxiv.org/html/2504.15909v2#S3.SS1.SSS3 "In 3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [3.1.4 Incompleteness and Incoherence in Complex Retrieval Tasks](https://arxiv.org/html/2504.15909v2#S3.SS1.SSS4 "In 3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      5. [3.1.5 Trade-offs Between Retrieval Efficiency and Precision](https://arxiv.org/html/2504.15909v2#S3.SS1.SSS5 "In 3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [3.2 Retrieval-Augmented Reasoning](https://arxiv.org/html/2504.15909v2#S3.SS2 "In 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [3.2.1 Knowledge Gap in Multi-step Reasoning](https://arxiv.org/html/2504.15909v2#S3.SS2.SSS1 "In 3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [3.2.2 Reasoning Discontinuity Caused by Domain Knowledge Boundaries](https://arxiv.org/html/2504.15909v2#S3.SS2.SSS2 "In 3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [3.2.3 Search Space Explosion and Local Optima Traps](https://arxiv.org/html/2504.15909v2#S3.SS2.SSS3 "In 3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [3.2.4 Dynamic Knowledge Requirements in Multi-Step Reasoning](https://arxiv.org/html/2504.15909v2#S3.SS2.SSS4 "In 3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      5. [3.2.5 Insufficient Depth and Breadth of Reasoning](https://arxiv.org/html/2504.15909v2#S3.SS2.SSS5 "In 3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
4. [4 Patterns of synergy](https://arxiv.org/html/2504.15909v2#S4 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [4.1 Pre-defined workflow](https://arxiv.org/html/2504.15909v2#S4.SS1 "In 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [4.1.1 Pre-Retrieval Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS1.SSS1 "In 4.1. Pre-defined workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [4.1.2 Post-Retrieval Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS1.SSS2 "In 4.1. Pre-defined workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [4.1.3 Hybrid Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS1.SSS3 "In 4.1. Pre-defined workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [4.2 Dynamic RAG Workflow](https://arxiv.org/html/2504.15909v2#S4.SS2 "In 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [4.2.1 Proactivity-Driven Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS2.SSS1 "In 4.2. Dynamic RAG Workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [4.2.2 Reflection-Driven Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS2.SSS2 "In 4.2. Dynamic RAG Workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [4.2.3 Feedback-Driven Reasoning](https://arxiv.org/html/2504.15909v2#S4.SS2.SSS3 "In 4.2. Dynamic RAG Workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")
5. [5 Implementation and Optimization](https://arxiv.org/html/2504.15909v2#S5 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [5.1 Reasoning Process](https://arxiv.org/html/2504.15909v2#S5.SS1 "In 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [5.1.1 LLM CoT](https://arxiv.org/html/2504.15909v2#S5.SS1.SSS1 "In 5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [5.1.2 Special Token Prediction](https://arxiv.org/html/2504.15909v2#S5.SS1.SSS2 "In 5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [5.1.3 Search-Driven Reasoning](https://arxiv.org/html/2504.15909v2#S5.SS1.SSS3 "In 5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [5.1.4 Reasoning on Graph](https://arxiv.org/html/2504.15909v2#S5.SS1.SSS4 "In 5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      5. [5.1.5 External Solver](https://arxiv.org/html/2504.15909v2#S5.SS1.SSS5 "In 5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [5.2 Reasoning Optimization](https://arxiv.org/html/2504.15909v2#S5.SS2 "In 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [5.2.1 Prompt-Based](https://arxiv.org/html/2504.15909v2#S5.SS2.SSS1 "In 5.2. Reasoning Optimization ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [5.2.2 Tuning-Based](https://arxiv.org/html/2504.15909v2#S5.SS2.SSS2 "In 5.2. Reasoning Optimization ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [5.2.3 RL-Based](https://arxiv.org/html/2504.15909v2#S5.SS2.SSS3 "In 5.2. Reasoning Optimization ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review")
6. [6 Downstream Tasks and Evaluation](https://arxiv.org/html/2504.15909v2#S6 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [6.1 Knowledge-Intensive Tasks](https://arxiv.org/html/2504.15909v2#S6.SS1 "In 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [Limited Challenge](https://arxiv.org/html/2504.15909v2#S6.SS1.SSS0.Px1 "In 6.1. Knowledge-Intensive Tasks ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [Lack of Specificity](https://arxiv.org/html/2504.15909v2#S6.SS1.SSS0.Px2 "In 6.1. Knowledge-Intensive Tasks ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [Task Uniformity](https://arxiv.org/html/2504.15909v2#S6.SS1.SSS0.Px3 "In 6.1. Knowledge-Intensive Tasks ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [Insufficient Dimensions](https://arxiv.org/html/2504.15909v2#S6.SS1.SSS0.Px4 "In 6.1. Knowledge-Intensive Tasks ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [6.2 New Tasks on RAG+Reasoning](https://arxiv.org/html/2504.15909v2#S6.SS2 "In 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [6.2.1 Deep Research](https://arxiv.org/html/2504.15909v2#S6.SS2.SSS1 "In 6.2. New Tasks on RAG+Reasoning ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [6.2.2 PhD (Expert)-Level Complex Reasoning](https://arxiv.org/html/2504.15909v2#S6.SS2.SSS2 "In 6.2. New Tasks on RAG+Reasoning ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
   3. [6.3 Challenges and Future Directions](https://arxiv.org/html/2504.15909v2#S6.SS3 "In 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [6.3.1 Complex Domain Tasks](https://arxiv.org/html/2504.15909v2#S6.SS3.SSS1 "In 6.3. Challenges and Future Directions ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [6.3.2 Decision Support and Active Retrieval](https://arxiv.org/html/2504.15909v2#S6.SS3.SSS2 "In 6.3. Challenges and Future Directions ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")
7. [7 Cost and Risk](https://arxiv.org/html/2504.15909v2#S7 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [7.1 Cost Trade-off in RAG+Reasoning](https://arxiv.org/html/2504.15909v2#S7.SS1 "In 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [7.1.1 Non-Linear Growth of Computational Resources](https://arxiv.org/html/2504.15909v2#S7.SS1.SSS1 "In 7.1. Cost Trade-off in RAG+Reasoning ‣ 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [7.1.2 Implicit Token Inflation](https://arxiv.org/html/2504.15909v2#S7.SS1.SSS2 "In 7.1. Cost Trade-off in RAG+Reasoning ‣ 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [7.1.3 Marginal Decline in Retrieval Efficiency](https://arxiv.org/html/2504.15909v2#S7.SS1.SSS3 "In 7.1. Cost Trade-off in RAG+Reasoning ‣ 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [7.1.4 Toward a Cost Model Framework](https://arxiv.org/html/2504.15909v2#S7.SS1.SSS4 "In 7.1. Cost Trade-off in RAG+Reasoning ‣ 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [7.2 Potential Risk of Over-Thinking](https://arxiv.org/html/2504.15909v2#S7.SS2 "In 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review")
8. [8 Practical Guide](https://arxiv.org/html/2504.15909v2#S8 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [8.1 Domain characteristics](https://arxiv.org/html/2504.15909v2#S8.SS1 "In 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [8.1.1 Finance](https://arxiv.org/html/2504.15909v2#S8.SS1.SSS1 "In 8.1. Domain characteristics ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [8.1.2 Healthcare](https://arxiv.org/html/2504.15909v2#S8.SS1.SSS2 "In 8.1. Domain characteristics ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [8.1.3 Legal Services](https://arxiv.org/html/2504.15909v2#S8.SS1.SSS3 "In 8.1. Domain characteristics ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [8.1.4 Personal Assistants](https://arxiv.org/html/2504.15909v2#S8.SS1.SSS4 "In 8.1. Domain characteristics ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [8.2 Do’s and Don’ts](https://arxiv.org/html/2504.15909v2#S8.SS2 "In 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [8.2.1 Structured Reasoning Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS1 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [8.2.2 Dynamic Demand-Responsive Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS2 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [8.2.3 Deterministic Decision-Making Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS3 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      4. [8.2.4 Time-Sensitive Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS4 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      5. [8.2.5 Risk-Sensitive Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS5 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      6. [8.2.6 Complex Path Exploration Scenarios](https://arxiv.org/html/2504.15909v2#S8.SS2.SSS6 "In 8.2. Do’s and Don’ts ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
   3. [8.3 Opportunity Points](https://arxiv.org/html/2504.15909v2#S8.SS3 "In 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      1. [8.3.1 Data and Indexing](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS1 "In 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         1. [Cold-Hot Tiered Indexing and Dynamic Context Management](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS1.Px1 "In 8.3.1. Data and Indexing ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         2. [Cross-Institution Knowledge Base Construction](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS1.Px2 "In 8.3.1. Data and Indexing ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         3. [Fine-Grained Layering and Confidence Grading](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS1.Px3 "In 8.3.1. Data and Indexing ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      2. [8.3.2 Models and Methodologies](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS2 "In 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         1. [Event-Driven Active Retrieval](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS2.Px1 "In 8.3.2. Models and Methodologies ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         2. [Spatiotemporal-Aware Retrieval and Association](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS2.Px2 "In 8.3.2. Models and Methodologies ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         3. [Multimodal Fusion in Retrieval and Reasoning](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS2.Px3 "In 8.3.2. Models and Methodologies ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         4. [Dynamic Risk Propagation Modeling and Management](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS2.Px4 "In 8.3.2. Models and Methodologies ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
      3. [8.3.3 Application Services](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS3 "In 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         1. [Validation of Logical Chain Completeness](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS3.Px1 "In 8.3.3. Application Services ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         2. [Intervenable Generation During Reasoning](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS3.Px2 "In 8.3.3. Application Services ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         3. [Risk Decision Interception Firewalls](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS3.Px3 "In 8.3.3. Application Services ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
         4. [Edge-Cloud Collaborative Retrieval and Reasoning](https://arxiv.org/html/2504.15909v2#S8.SS3.SSS3.Px4 "In 8.3.3. Application Services ‣ 8.3. Opportunity Points ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")
9. [9 Future Trends](https://arxiv.org/html/2504.15909v2#S9 "In Synergizing RAG and Reasoning: A Systematic Review")
   1. [9.1 The Integration of RAG and Graph](https://arxiv.org/html/2504.15909v2#S9.SS1 "In 9. Future Trends ‣ Synergizing RAG and Reasoning: A Systematic Review")
   2. [9.2 Multi-Model Collaboration](https://arxiv.org/html/2504.15909v2#S9.SS2 "In 9. Future Trends ‣ Synergizing RAG and Reasoning: A Systematic Review")
   3. [9.3 Multi-Modal Collaboration](https://arxiv.org/html/2504.15909v2#S9.SS3 "In 9. Future Trends ‣ Synergizing RAG and Reasoning: A Systematic Review")
   4. [9.4 Customized Reinforcement Learning](https://arxiv.org/html/2504.15909v2#S9.SS4 "In 9. Future Trends ‣ Synergizing RAG and Reasoning: A Systematic Review")
10. [10 Conclusion](https://arxiv.org/html/2504.15909v2#S10 "In Synergizing RAG and Reasoning: A Systematic Review")

\useforestlibrary

edges
\useunder\ul

# Synergizing RAG and Reasoning: A Systematic Review

Yunfan Gao
Shanghai Research Institute for Intelligent Autonomous Systems, Tongji UniversityChina
gaoyunfan1602@gmail.com
,
Yun Xiong
Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan UniversityChina
yunx@fudan.edu.cn
,
Yijie Zhong
College of Design and Innovation, Tongji UniversityChina
dun.haski@gmail.com
,
Yuxi Bi
College of Design and Innovation, Tongji UniversityChina
yuxibi@gmail.com
,
Ming Xue
Percena AIChina
mxue@percena.co
 and
Haofen Wang
College of Design and Innovation, Tongji UniversityChina
carter.whfcarter@gmail.com

###### Abstract.

Recent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining ”reasoning” within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.

††copyright: none
![Refer to caption](extracted/6386523/Images/timeline.png)

Figure 1. Timeline of studies on RAG-reasoning synergy. From a technical perspective, the approaches can be categorized into Prompt-Based, Tuning-Based, and RL-Based methods. A notable trend is the increasing use of Reinforcement Learning to enhance RAG systems, particularly following the prosperity of test-time scaling. Meanwhile, Prompt-Based and Tuning-Based methods continue to evolve in parallel, demonstrating that there are multiple pathways to integrating reasoning capabilities into RAG systems.

## 1. Introduction

Recent breakthroughs in Large Language Models (LLMs) like OpenAI O1 (Jaech et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib40)) and DeepSeek-R1 (Guo et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib26)) have shifted the paradigm from ”pre-training scaling” to ”test-time scaling” (Muennighoff et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib64)). Unlike traditional language models that improve via corpus accumulation during pre-training, these models enhance performance in complex tasks—such as mathematical derivation and code generation (He et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib30))—through post-training innovations during the inference phase (e.g., Long-CoT thinking (Chen et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib9))). This shift has led to the emergence of ”Large Reasoning Models” (LRMs) (Xu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib100)) with advanced internal reasoning abilities.

These advancements have not only boosted basic model capabilities but also opened new avenues for application technologies like Retrieval-Augmented Generation (RAG) (Gao et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib22)). Serving as a key link between language models and external knowledge, RAG overcomes traditional LLMs’ limits in knowledge freshness, domain specificity, and factual accuracy by retrieving real-time non-parametric information and integrating it into the context. This enhances information processing and reduces hallucination risks in knowledge-intensive tasks.

Technological evolution is advancing RAG architectures through innovations like query rewriting (Ma et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib62)), re-ranking (Abdallah et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib2)), and hybrid retrieval (Wang et al., [2024d](https://arxiv.org/html/2504.15909v2#bib.bib89)), creating an Advanced RAG paradigm focused on pre-retrieval optimization and post-retrieval refinement. Modular RAG (Gao et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib23)) further breaks down these systems into component-based, service-oriented architectures, using orchestration to tackle practical challenges.

Despite improvements in query intent recognition and knowledge use, challenges of RAG remain in demanding tasks like deep research and complex decision-making. Key issues include: 1) difficulty capturing intent from ambiguous queries; 2) poor logical coherence in multi-hop reasoning; 3) efficiency limits of traditional retrieval in open domains; and 4) degraded generation quality from noisy retrieved data.

Models like DeepSeek-R1, with strong reasoning capabilities, inspire new directions for RAG systems. As shown in Figure [1](https://arxiv.org/html/2504.15909v2#S0.F1 "Figure 1 ‣ Synergizing RAG and Reasoning: A Systematic Review"), recent research explores integrating formal reasoning frameworks with knowledge retrieval. This approach optimizes retrieval through logic-driven query reformulation and uses reasoning to analyze and validate retrieved knowledge, creating cognitive synergy between retrieval and generation. This paradigm aims to overcome conventional limitations, enabling intelligent systems with rigorous logic and reliable knowledge use. From a trend perspective, an increasing number of methods combine reasoning and retrieval abilities through reinforcement learning (RL), marking a new direction in the LRM era. Meanwhile, prompt-based approaches continue to rapidly evolve, with researchers aiming to achieve results through workflow design while keeping model parameters frozen. Notably, sole reliance on tuning methods is steadily decreasing, suggesting limited improvements from additional fine-tuning at this stage.

Traditional RAG is limited by its unidirectional flow (retrieval → generation). Integrating reasoning capabilities grants the system greater autonomy, unlocking new possibilities. As shown in Figure [2](https://arxiv.org/html/2504.15909v2#S1.F2 "Figure 2 ‣ 1. Introduction ‣ Synergizing RAG and Reasoning: A Systematic Review"), this integration is poised to drive major breakthroughs, enabling practical use in complex real-world scenarios.

1) From Ambiguous Semantic Matching to Logic-Driven Targeted Retrieval. Traditional RAG relies on semantic similarity for retrieval; however, it is sensitive to phrasing variations. Advanced reasoning allows deep logical analysis of queries (e.g., causal links, conditional constraints) to dynamically refine retrieval strategies (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)). For example, to answer ”How to reduce postoperative infection risks in diabetes patients?”, the system prioritizes retrieving ”blood glucose control thresholds” and ”antibiotic usage guidelines” over simply matching ”diabetes postoperative care”. This approach supports multi-hop retrieval by breaking down complex queries into sequential sub-queries while preserving cross-document coherence through reasoning chains.

![Refer to caption](extracted/6386523/Images/advantage.png)

Figure 2. Advantages of Combining RAG with Reasoning

2) From Simple Information Aggregation to Logically Coherent Context Construction. Current RAG systems input all retrieved document chunks into context directly, often causing fragmented or contradictory information that confuses LLMs. Reasoning-enhanced systems integrate evidence chains by logically verifying and inferring causality in retrieved content, filtering conflicts and forming coherent explanations (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)). They also use dynamic knowledge completion to detect missing logical links, prompting iterative retrieval or inference to fill gaps (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)).

3) From Simple and Single-Turn QA to Systemic Decision Support. Traditional RAG performs well in factual QA (Petroni et al., [2020](https://arxiv.org/html/2504.15909v2#bib.bib66)) but struggles with multi-step and complex decision-making. Reasoning-integrated systems produce structured reasoning output, enhancing multi-objective optimization to balance retrieval breadth and solution feasibility under various constraints. For example, multiple constraints under different conditions in engineering construction plans (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)), and the formulation of diagnosis and treatment plans for various diseases in the medical field (Zhao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)).

4) From Indiscriminate Retrieval to Intelligent Resource Allocation. Traditional RAG retrieves documents for all queries, regardless of complexity. Reasoning-enhanced systems use on-demand retrieval, handling simple queries with direct generation and complex ones with multi-round retrieval to reduce latency (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)). Dynamic retrieval pruning uses pre-reasoning predictions to target key information, minimizing unnecessary document and graph traversal (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)).

5) From Passive Knowledge Tool to Proactive Cognitive Assistant. Advancing beyond reactive knowledge retrieval, reasoning-enhanced systems can proactively serve users by asking clarifying questions and anticipating implicit needs. This shift enables human-like assistants that integrate memory, reasoning, and decision-making, proving especially valuable for complex tasks such as deep research (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)), business analytics (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)), personal assistant (Zhong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib108)) and urban planning (Wang et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib86)).

However, the synergistic pathway between RAG and reasoning requires more than simply replacing conventional generative LLMs with LRM modules. It necessitates deep integration of technological evolution insights from LRM - achieved through reconstructing knowledge retrieval mechanisms and strengthening reasoning-generation collaborative linkages - to enable system-level enhancement of cognitive capabilities within the RAG architecture.

Therefore, this paper aims to address the pivotal and forward-looking research question of ”how RAG systems can synergize with reasoning capabilities”. We systematically review current studies after 2024 while establishing explicit definitions for reasoning within RAG contexts. Building on this foundation, we provide an in-depth taxonomy and analysis of the objectives, typical patterns, and implementations underlying RAG-reasoning integration, clarifying key technological trajectories and critical breakthroughs.

As RAG technology enters its next developmental phase, downstream task complexity has escalated significantly - particularly evident in emerging challenges like Deep Research (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)). These advanced applications not only demand enhanced reasoning capacities but also drive RAG’s expansion into multimodal, cross-domain, and dynamic environments. However, while the integration of reasoning capabilities demonstrably improves complex task performance, existing research frequently overlooks associated computational overheads and potential risks. Through systematic examination of these operational constraints and analysis of industry applications, we propose practical guidelines for multiple real-world scenarios with diverse requirements.

Finally, we outline future research directions grounded in current technological evolution, including: 1) RAG-graph architecture integration, 2) coordinated multimodal reasoning frameworks, 3) hybrid model collaboration, and 4) RL optimization specifically designed for RAG systems. This work establishes both theoretical foundations and practical roadmaps for subsequent research in this evolving field.

The contributions of this paper can be summarized as follows:

* •

  Pioneering Review. This work represents the first comprehensive survey focusing on the integration of RAG with reasoning, offering novel insights and forward-looking guidance for advancing this emerging research frontier.
* •

  Systematic Taxonomy. We present a multi-dimensional framework to systematically examine the objectives, paradigms, and methodologies for combining RAG with reasoning capabilities, establishing clear classification criteria across technical dimensions.
* •

  Practical Guidance. Beyond theoretical exploration, we critically discuss the additional cost and potential risks associated with the introduction of reasoning, accompanied by an actionable Practical Guide for real-world scenarios.
* •

  Open Resource Platform111<https://openrag.notion.site/open-rag-base?pvs=4> Through the OpenRAG platform, we provide a rich, multi-dimensional review of related work, which allows readers to quickly search and compare different methods.

## 2. Overview

This chapter establishes a conceptual framework for the paper along two key dimensions. First, it formally defines ”reasoning” and distinguishes it from ”inference.” Second, it organizes a taxonomy of synergy mechanisms between ”RAG and Reasoning.” To construct a clear cognitive pathway, we address three progressive research questions:

* •

  Why synergize RAG and reasoning?
* •

  What are their typical collaboration paradigms?
* •

  How can this integration be realized?

### 2.1. Definition

The definition of reasoning in modern AI systems remains an evolving construct, particularly within the context of LRMs exemplified by DeepSeek R1 and OpenAI O1. Here, under the scope of LLMs, we formalize reasoning as a structured, multi-step process that dynamically decomposes complex problems, generates intermediate hypotheses, and iteratively refines solutions through logical and evidence-based transformations. Mathematically, let a reasoning process ℛℛ\mathcal{R}caligraphic\_R be defined as a tuple ⟨𝒦p,𝒦r,𝒮t,Φ⟩

subscript𝒦𝑝subscript𝒦𝑟subscript𝒮𝑡Φ\langle\mathcal{K}\_{p},\mathcal{K}\_{r},\mathcal{S}\_{t},\Phi\rangle⟨ caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT , caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT , caligraphic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , roman\_Φ ⟩, where 𝒦psubscript𝒦𝑝\mathcal{K}\_{p}caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT denotes parametric knowledge embeddings, 𝒦rsubscript𝒦𝑟\mathcal{K}\_{r}caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT represents retrieved contextual knowledge, 𝒮t={s0,s1,…,sn}subscript𝒮𝑡subscript𝑠0subscript𝑠1…subscript𝑠𝑛\mathcal{S}\_{t}=\{s\_{0},s\_{1},\ldots,s\_{n}\}caligraphic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = { italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , italic\_s start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT } constitutes the evolving state sequence with s0subscript𝑠0s\_{0}italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT as the initial query and snsubscript𝑠𝑛s\_{n}italic\_s start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT as the final response, and Φ:𝒮i×𝒦p×𝒦r→𝒮i+1:Φ→subscript𝒮𝑖subscript𝒦𝑝subscript𝒦𝑟subscript𝒮𝑖1\Phi:\mathcal{S}\_{i}\times\mathcal{K}\_{p}\times\mathcal{K}\_{r}\rightarrow%
\mathcal{S}\_{i+1}roman\_Φ : caligraphic\_S start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT × caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT × caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT → caligraphic\_S start\_POSTSUBSCRIPT italic\_i + 1 end\_POSTSUBSCRIPT defines the state transition function.

The reasoning process exhibits three defining characteristics. First, it is inherently multi-step, systematically decomposing complex problems into intermediate cognitive states (e.g., sub-question generation or temporary conclusions) rather than pursuing direct input-output mapping. Second, it generates novel knowledge or facts — synthesizing implicit relationships, deriving latent constraints, or reformulating problems in ways not explicitly present in the initial input or parametric memory (e.g., transforming ”Is A greater than B?” into comparative subquestions about A and B’s attributes). Crucially, these representations are not merely retrieved but dynamically constructed through the reasoning trajectory. Third, the process is teleological — its architecture and termination conditions are explicitly optimized for complex problem resolution, where complexity is measured by the necessity of state transitions or the insufficiency of direct retrieval from either parametric (𝒦psubscript𝒦𝑝\mathcal{K}\_{p}caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT) or external (𝒦rsubscript𝒦𝑟\mathcal{K}\_{r}caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT) knowledge sources. This stands in stark contrast to atomic inference, which lacks such deliberate state construction and goal-aware iteration.

The distinction between reasoning and inference manifests most saliently in their computational signatures. While inference ℐℐ\mathcal{I}caligraphic\_I constitutes a single-step conditional probability computation P⁢(y|x)=∏t=1TP⁢(yt|x,y<t)𝑃conditional𝑦𝑥superscriptsubscriptproduct𝑡1𝑇𝑃conditionalsubscript𝑦𝑡

𝑥subscript𝑦absent𝑡P(y|x)=\prod\_{t=1}^{T}P(y\_{t}|x,y\_{<t})italic\_P ( italic\_y | italic\_x ) = ∏ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT italic\_P ( italic\_y start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_x , italic\_y start\_POSTSUBSCRIPT < italic\_t end\_POSTSUBSCRIPT ), reasoning ℛℛ\mathcal{R}caligraphic\_R implements a meta-process coordinating multiple inference calls through explicit state management ℛ⁢(x)=Φ1∘Φ2∘⋯∘Φn⁢(x)ℛ𝑥subscriptΦ1subscriptΦ2⋯subscriptΦ𝑛𝑥\mathcal{R}(x)=\Phi\_{1}\circ\Phi\_{2}\circ\cdots\circ\Phi\_{n}(x)caligraphic\_R ( italic\_x ) = roman\_Φ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ∘ roman\_Φ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT ∘ ⋯ ∘ roman\_Φ start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ( italic\_x ). This multi-phase architecture enables systematic error correction through backtracking mechanisms and dynamic retrieval refinement — features fundamentally absent in conventional inference pipelines. The operational boundary emerges when state transitions involve explicit symbolic manipulation (equation restructuring in mathematical reasoning) or knowledge graph traversal (temporal reasoning over retrieved events), distinguishing true reasoning from mere multi-step inference.

{forest}

for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,
calign=edge midpoint,
font=,
[The Synergy of RAG and Reasoning, fill=black!10,rotate=90,text width=6cm,font=[Purpose §[3](https://arxiv.org/html/2504.15909v2#S3 "3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=red!20,font=[ Reasoning-Augmented Retrieval

(Better Retrieval) §[3.1](https://arxiv.org/html/2504.15909v2#S3.SS1 "3.1. Reasoning-Augmented Retrieval ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=3.5cm
[ ARM (Chen et al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)); AdaptiveRAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42));
FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)); LevelRAG (Zhang et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104)); OmniThink (Xi et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95)); PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)); SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)); UAR (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)); O1-Embedeer (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); ITER-RETGEN (Shao et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69));
HiRAG (Zhang et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103));
MetaRAG (Zhou et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109));
MMOA-RAG (Chen et al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13));
ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17));
DeepResearcher (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107));
ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50));
FRAG (Gao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24));
PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74));
Insight-RAG (Pezeshkpour and Hruschka, [2025](https://arxiv.org/html/2504.15909v2#bib.bib67));
ChainRAG (Zhu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib111));
FG-RAG (Hong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33));
MCTS-RAG (Hu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37));
REAPER (Joshi et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47));
DeepNote (Wang et al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)),
text width=10.0cm, node options=align=left
]
]
[ Retrieval-Augmented Reasoning

(Better Reasoning) §[3.2](https://arxiv.org/html/2504.15909v2#S3.SS2 "3.2. Retrieval-Augmented Reasoning ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=3.5cm
[ ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)); AgenticReasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93));
CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)); CR-planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)); DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)); Deepsolution (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)); KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)); OpenRAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)); PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)); R1-Seacher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)); RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)); ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76));
ReSearch (Chen et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7));
MedRAG (Zhao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106));
RARE (Tran et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78));
RARE(Wang et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib91));
RetroRAG (Xiao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96));
KG-RAR (Wu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94));
RetrievalPRM (Zhu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110));
MRD-RAG (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12));
Search-O1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52));
StePO-Rec (Bi et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)),
text width=10.0cm, node options=align=left
]
]
]
[Paradigms §[4](https://arxiv.org/html/2504.15909v2#S4 "4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=orange!20,font=[Pre-defined Workflow § [4.1](https://arxiv.org/html/2504.15909v2#S4.SS1 "4.1. Pre-defined workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm
[Reasoning in Pre-Retrieval,text width=2.1cm
[
LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)); O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); planRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)); UAR (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15));
MMOA-RAG (Chen et al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13));
MedRAG (Zhao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106));
FRAG (Gao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24));
Insight-RAG (Pezeshkpour and Hruschka, [2025](https://arxiv.org/html/2504.15909v2#bib.bib67));
ChainRAG (Zhu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib111));
RetrievalPRM (Zhu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110));
REAPER (Joshi et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47));
AdaptiveRAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)),
text width=8.2cm, node options=align=left
]
]
[Reasoning in Post-Retrieval,text width=2.1cm
[
ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)); ARM (Chen et al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8));
MRD-RAG (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12));
FG-RAG (Hong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33));
ToG2 (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)),
text width=8.2cm, node options=align=left
]
]
[Hybrid Reasoning,text width=2.1cm
[
IR-COT (Trivedi et al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79));
FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51));
ITER-RETGEN (Shao et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69));
LevelRAG (Zhang et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104));
HiRAG (Zhang et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103));
MetaRAG (Zhou et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109));
RetroRAG (Xiao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96));
KG-RAR (Wu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94));
DeepNote (Wang et al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)),
text width=8.2cm, node options=align=left
]
]
]
[Dynamic Workflow §[4.2](https://arxiv.org/html/2504.15909v2#S4.SS2 "4.2. Dynamic RAG Workflow ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm
[Proactivity-Driven Reasoning,text width=2.2cm
[
AgenticReasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93));
DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25));
CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84));
Co-STORM (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44));
PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83));
Search-O1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52));
R1-Searcher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)),
text width=8.1cm, node options=align=left
]
]
[Reflection-Driven Reasoning,text width=2.2cm
[
Flare (Jiang et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib46));
OpenRAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39));
WriteHere (Xiong et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99));
ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50));
Self-RAG (Asai et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)),
text width=8.1cm, node options=align=left
]
]
[Feedback-Driven Reasoning,text width=2.2cm
[
SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21));
CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53));
MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98));
DeepSolution (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55));
RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97));
ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17));
ReSearch (Chen et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7));
RARE (Tran et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78));
DeepResearcher (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107));
PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74));
MCTS-RAG (Hu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37));
KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)),
text width=8.1cm, node options=align=left
]
]
]
]
[Implementation §[5](https://arxiv.org/html/2504.15909v2#S5 "5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=blue!20,font=[Resoning Method §[5.1](https://arxiv.org/html/2504.15909v2#S5.SS1 "5.1. Reasoning Process ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm,
[LLM/CoT, text width=2.0cm
[
ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101));
AdaptiveRAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42));
O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102));
DeepNote (Wang et al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85));
HiRAG (Zhang et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103));
MetaRAG (Zhou et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109));
MMOA-RAG (Chen et al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13));
RetroRAG (Xiao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96));
PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74));
KG-RAR (Wu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94));
MRD-RAG (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12));
PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)),
text width=8.2cm, node options=align=left
]
]
[Special Token Prediction, text width=2.0cm
[
Self-RAG (Asai et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4));
SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21));
OpenRAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39));
R1-Searcher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73));
ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17));
ReSearch (Chen et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7));
ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50));
Search-O1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)),
text width=8.2cm, node options=align=left
]
]
[Search-Driven Reasoning, text width=2.0cm
[
OminiThink (Xi et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95));
DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25));
CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84));
DeepSolution (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55));
ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76));
KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59));
WriteHere (Xiong et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99));
RARE (Tran et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78));
MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98));
StePO-Rec (Bi et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)),
text width=8.2cm, node options=align=left
]
]
[Reasoning on Graph, text width=2.0cm
[
FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51));
ToG2 (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61));
LighRAG (Guo et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib28));
FRAG (Gao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24));
FG-RAG (Hong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33));
MedRAG (Zhao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)),
text width=8.2cm, node options=align=left
]
]
[External Solver,text width=2.0cm
[
ARM (Chen et al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)),
text width=8.2cm, node options=align=left
]
]
]
[Optimization §[5.2](https://arxiv.org/html/2504.15909v2#S5.SS2 "5.2. Reasoning Optimization ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm, for tree=fill=blue!20
[Prompt-Based, text width=2.0cm
[
Co-STORM (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44));
Agentic Reasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93));
FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51));
PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49));
HiRAG (Zhang et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103));
MetaRAG (Zhou et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109));
MedRAG (Zhao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106));
RARE (Tran et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78));
ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50));
RetroRAG (Xiao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96));
FRAG (Gao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24));
KG-RAR (Wu et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94));
MRD-RAG (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12));
FG-RAG (Hong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33));
MCTS-RAG (Hu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37));
DeepSolution (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55));
StePO-Rec (Bi et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)),
text width=8.2cm, node options=align=left
]
]
[Tuning-Based, text width=2.0cm
[
KBQA-Q1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59));
O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102));
DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25));
CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84));
MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98));
RetrievalPRM (Zhu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110));
REAPER (Joshi et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47));
RARE(Wang et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib91));
UAR (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)),
text width=8.2cm, node options=align=left
]
]
[RL-Based, text width=2.0cm
[PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83));
LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35));
RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97));
ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76));
SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21));
CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53));
DeepRetrieval (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43));
DeepNote (Wang et al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85));
MMOA-RAG (Chen et al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13));
ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17));
ReSearch (Chen et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7));
DeepResearcher (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107));
PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74));
R1-Search (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)),
text width=8.2cm, node options=align=left
]
]
]
]
]

Figure 3. A structured taxonomy of synthesizing RAG and Reasoning.

### 2.2. Taxonomy

Integrating RAG with reasoning marks a paradigm shift in tackling complex knowledge-intensive tasks. This work develops a hierarchical taxonomy (Figure [3](https://arxiv.org/html/2504.15909v2#S2.F3 "Figure 3 ‣ 2.1. Definition ‣ 2. Overview ‣ Synergizing RAG and Reasoning: A Systematic Review")) based on three key questions: why reasoning is needed with RAG (Purpose), how they structurally interact (Paradigm), and what methods enable effective integration (Implementation). This framework guides readers through the technical innovations in later chapters, providing a clear conceptual path without premature technical details, and highlighting the field’s evolutionary logic, avoiding delving prematurely into specific technical details.

#### 2.2.1. Synergy Purpose

Integrating reasoning with RAG addresses the limitations of traditional RAG systems, which struggle with multi-step logic, contextual adaptation, and implicit knowledge synthesis due to reliance on superficial semantic matching and fixed knowledge limits. Adding reasoning enables dynamic retrieval planning, logical verification of evidence, and insight generation beyond retrieved data through abductive or counterfactual reasoning. At the same time, the introduction of external knowledge retrieval also helps alleviate reasoning interruptions caused by the knowledge limitations of LRM and reduces the likelihood of hallucinations. This integration occurs in two main ways: Reasoning-Augmented Retrieval, where inference drives context-aware information gathering; Retrieval-Augmented Reasoning, where external knowledge supports and expands the model’s deductive abilities.

#### 2.2.2. Synergy Paradigms

Building upon the above necessity, our taxonomy categorizes RAG+Reasoning systems along the axis of procedural dynamism. Pre-defined workflows employ fixed templates that systematically alternate between retrieval and reasoning phases, with intervention points predetermined at *pre-retrieval reasoning* (e.g., query decomposition), *post-retrieval reasoning* (e.g., evidence synthesis), or *hybrid stages*. While offering operational transparency, these architectures exhibit limited adaptability to emergent task complexities. In contrast, dynamic workflows implement state-contingent reasoning processes where retrieval actions are conditionally triggered through continuous system introspection. This paradigm further branches into *Proactivity-Driven* strategies (self-initiated knowledge requests), *Reflection-driven* mechanisms (error-corrective retrieval based on intermediate result analysis), and *Feedback-driven* approaches (environmental reward signals or external model evaluations). The progression from static to dynamic architectures reflects the field’s maturation toward human-like contextual adaptation in open-world problem-solving.

#### 2.2.3. Synergy Implementation

Operationalizing these synergies requires innovations across reasoning and retrieval strategies. Foundational reasoning architectures span LLM-Based like COT, search-based hypothesis generation (tree search, Monte Carlo methods), symbolic solver integration, and graph-structured multi-hop inference. These capabilities are further enhanced through three principal augmentation strategies: prompt-based techniques that utilize natural language templates and special token (e.g., ¡Plan¿, ¡Verify¿) to steer model behavior, tuning-based methods that inject domain-specific knowledge or distill reasoning capability, and RL-based frameworks that optimize retrieval-reasoning policies through outcome reward models (ORM) or process reward models (PRM). The alignment between these methodologies and the proposed taxonomy is critical—static workflows predominantly rely on predictable prompt-guided reasoning chains, whereas dynamic systems increasingly integrate search-based exploration or solver-augmented strategies to navigate evolving state spaces.

Overall, this tripartite taxonomy—motivational drivers, architectural paradigms, and implementation methodologies—establishes a unified lens for analyzing RAG+Reasoning systems. Subsequent chapters will elaborate on each stratum, progressively revealing how these conceptual distinctions translate into technical innovations that push the boundaries of machine intelligence.

## 3. The purpose of the synergy

![Refer to caption](extracted/6386523/Images/purpose.png)

Figure 4. The purpose of the synergy between RAG and reasoning

The integration of RAG and reasoning marks a crucial advancement in enhancing LLMs’ problem-solving abilities. Their true potential lies not in isolated use but in their synergy, which overcomes key limitations in retrieval and reasoning. This section explains the main motivations for combining RAG with reasoning, emphasizing two primary benefits: (1) enhancing retrieval accuracy and flexibility through reasoning, and (2) reinforcing complex reasoning by using context-rich retrieved knowledge. Figure [4](https://arxiv.org/html/2504.15909v2#S3.F4 "Figure 4 ‣ 3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review") illustrates these collaborative aims and the limitations they address.

The first key benefit is Reasoning-Augmented Retrieval, where reasoning improves the retrieval process. Traditional RAG systems struggle with query formulation, relevance assessment, and iterative refinement—tasks needing logical and contextual analysis. Reasoning enables adaptive retrieval through dynamic query expansion, ambiguity resolution, and multi-hop evidence aggregation, overcoming the limits of keyword- or embedding-based methods and aligning retrieval with the task’s reasoning demands.

The second benefit is Retrieval-Augmented Reasoning, where external knowledge supplements the limitations of purely parametric LLM reasoning. Even advanced models face hallucination, knowledge gaps, and compositional challenges alone. Retrieval grounds reasoning in up-to-date, domain-specific, or rare information absent from model weights, crucial for explainability, multi-step deduction, and integrating diverse sources.

Together, combining RAG and reasoning fills fundamental gaps in both techniques. By enhancing retrieval via reasoning and strengthening reasoning through retrieval, it broadens LLMs’ capacity to address complex real-world problems.

### 3.1. Reasoning-Augmented Retrieval

Reasoning-Augmented Retrieval (RAR) represents a significant advancement in information retrieval by integrating multi-step reasoning to dynamically enhance retrieval quality. Unlike traditional methods that depend on static semantic matching, RAR creates a cognitive feedback loop mimicking human iterative reasoning, surpassing the limitations of simple ”query-document” interactions.

RAR’s effectiveness stems from several key features. It often uses on-demand retrieval, where reasoning—evaluating intent clarity, knowledge state, and temporal factors—guides adaptive search initiation, reducing redundancies present in fixed triggers (e.g., UAR’s classifier (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))). It improves semantic alignment by inferring implicit query logic such as business rules or entity relationships to generate precise retrieval requests aligned with data schemas (e.g., PlanRAG’s plan-retrieval loops (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49))). RAR also applies multi-step iterative refinement, using intermediate reasoning outputs (e.g., chain-of-thought, partial answers (Trivedi et al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79))) to recursively reformulate queries in a closed-loop system essential for resolving multi-hop dependencies (Shao et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)). Furthermore, it adapts to specific domains by tailoring retrieval to vertical contexts (e.g., financial or medical) and balances efficiency and precision through lightweight reasoning strategies (e.g., AdaptiveRAG’s complexity-based selection (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42))).

Traditional retrieval systems, effective for simple queries, struggle with complex information needs due to rigid designs favoring static matching over dynamic reasoning, limiting their adaptability to changing contexts and diverse data. RAR primarily addresses five core challenges inherent in these conventional methods.

#### 3.1.1. Semantic Disparities Between Queries and Documents

A key challenge lies in the mismatch between user queries and documents—whether due to differing expression styles (professional jargon vs. casual language) or implicit contextual gaps—making direct semantic matching unreliable. Importantly, high similarity does not guarantee true relevance, as documents may share keywords or surface features without addressing the underlying intent or logic of the query. Retrieval models must therefore understand deeper semantics beyond superficial similarity.Domain adaptation further complicates this issue. To overcome these gaps, approaches such as reasoning-augmented embeddings (O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) enriches queries with inferred “thinking” text), feedback-driven rewriting (SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) dynamically refines queries based on retrieved results), and pre-planning (PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) extracts business rules to generate SQL queries aligned with database schemas) help better capture domain-specific semantics and ensure relevance beyond mere similarity.

#### 3.1.2. Inflexible Intent Disambiguation

Traditional RAG methods rely on fixed embedding similarity strategies , which fail to dynamically interpret the implicit intent behind complex queries (e.g., multi-hop reasoning or domain-specific requirements). User queries often exhibit semantic complexity that far exceeds their surface text—for instance, a request to ”optimize supply chain costs” may require correlating disparate database fields not explicitly mentioned.
Static retrieval methods lack the adaptability to capture such dynamically evolving information needs. A critical limitation lies in intent dynamicity: as contextual understanding expands, traditional systems generate fixed retrieval results based solely on the initial query. Furthermore, semantic representation limitations of dense retrieval models (e.g., BERT-based models) hinder their ability to encode intricate semantic relationships (e.g., irony, metaphors), leading to misaligned results. Current approaches attempt to mitigate these issues through multi-step intent decomposition (e.g., LevelRAG’s high-level searcher breaks complex queries into multi-hop sub-queries (Zhang et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104))) and dynamic query reformulation (e.g., LeReT’s reinforcement learning generates diversified query candidates (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35))), iteratively refining retrieval strategies to align with document content.

#### 3.1.3. Inefficient Coordination of Multi-Source Heterogeneous Data

Retrieval from diverse sources—text, tables, graphs, web, and APIs—often produces fragmented results due to a lack of global reasoning. The key challenge is modal heterogeneity: different retrieval techniques (dense retrieval for text, SQL for tables, GQL for graphs) operate independently without unified coordination. For example, experiments show standard RAG methods (like dense retrieval with query decomposition) yield only 32.7% perfect recall and 40.9% F1 on the OTT-QA dataset. These outcomes reveal the limitations of traditional approaches in aligning textual queries with structured tables—such as failing to link concepts like ”K-12 student free rates” in text to related ”education expenditure” columns when not explicitly mentioned. Additionally, disconnected entity matching (e.g., relating ”company revenue” in text to financial tables) worsens inefficiencies, as conventional methods depend on semantic similarity and overlook domain-specific relationships and exact-value matches. Advanced techniques—such as reasoning-driven alignment (ARM’s N-gram constraints for cross-modal entity decoding (Chen et al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8))) and unified semantic spaces (LevelRAG’s shared multi-modal representations (Zhang et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104)))—enable more effective, integrated retrieval.

#### 3.1.4. Incompleteness and Incoherence in Complex Retrieval Tasks

Single-step retrieval systems fall short in complex multi-hop reasoning tasks, such as deducing entity chains or conducting decision analysis. Traditional static retrieval conflicts with multi-step cognitive needs, resulting in three main issues: 1) Path dependency, where later retrievals rely on information from earlier steps (e.g., finding ”the most populous county in California” before its education policies), but conventional systems lack state management; 2) Error propagation,early retrieval errors cause mistakes in intermediate results, which then affect the next round of retrieval; 3) Semantic inflexibility of fixed queries, which cannot adapt to dynamic concepts like entity aliases or relational predicates.

Advanced methods address these flaws through integrated strategies. PlanRAG uses iterative ”plan-retrospect-replan” cycles to trigger sub-queries when gaps arise. Reinforcement learning in LeReT improves query generation via reward-driven path selection. Likewise, ITER-RETGEN rebuilds follow-up queries using intermediate answers (e.g., ”award recipient’s height”) to resolve multi-hop dependencies.

#### 3.1.5. Trade-offs Between Retrieval Efficiency and Precision

Complex scenarios face a tension between exhaustive retrieval, which is computationally costly, and restricted retrieval, which risks information loss. Expanding retrieval blindly inflates costs (e.g., LLM API calls) without ensuring relevance. Simple queries suffer from unnecessary multi-step retrieval, wasting resources, while complex queries face quality risks if retrieval is too limited. Adaptive approaches like complexity-aware routing (Adaptive-RAG’s lightweight classifier allocates retrieval budgets (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42))) and cost-sensitive training (SmartRAG’s reinforcement learning balances quality and steps (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21))) dynamically manage this trade-off.

In summary, Reasoning-Augmented Retrieval overcomes traditional RAG’s limitations in dynamic triggering, semantic alignment, multi-hop support, domain adaptation, and efficiency trade-offs by deeply integrating reasoning into the retrieval process. Its key innovation is a bidirectional enhancement between reasoning and retrieval—reasoning refines retrieval strategies, while retrieval supports iterative reasoning—jointly boosting accuracy and efficiency in complex information tasks.

### 3.2. Retrieval-Augmented Reasoning

Retrieval-Augmented Reasoning (ReAR) combines external knowledge retrieval with inherent model reasoning to overcome failures from knowledge gaps or logical discontinuities in complex tasks. Unlike traditional RAG methods that retrieve information once, ReAR uses an iterative, context-sensitive retrieval that continuously provides relevant data to support multi-step reasoning. This approach is crucial for tasks needing strict logic, such as mathematical proofs, where intermediate steps require specific theorems or lemmas. By making retrieval an adaptive, ongoing process rather than a one-time step, ReAR strengthens each reasoning stage with accurate, current information, improving the overall inference’s reliability and robustness.

ReAR’s core feature is dynamic knowledge supplementation, generating retrieval queries in real-time based on the evolving reasoning context. This overcomes the limits of single-round retrieval by enabling knowledge refinement at each step, as seen in process supervision frameworks like RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)). ReAR also improves reasoning paths using methods like search space compression—for example, MCTS-guided heuristics in KBQA—and structured feedback from diverse sources like knowledge graphs (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)). These techniques maintain logical consistency while reducing irrelevant or conflicting information. Importantly, ReAR adapts well across domains, supporting precise knowledge retrieval and tool use for specialized tasks such as industrial problem-solving in PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)) or scientific reasoning (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)).

By integrating retrieval as an active part of the reasoning loop, ReAR addresses LLMs’ temporal and depth constraints, ensuring adherence to domain-specific and time-sensitive requirements. This close coupling turns external knowledge into an on-demand resource, creating a closed-loop system that enhances the model’s ability to handle complex, knowledge-intensive problems. Specifically, ReAR seeks to address the following limitations and challenges:

#### 3.2.1. Knowledge Gap in Multi-step Reasoning

In long-range reasoning, missing intermediate knowledge often breaks logical chains, especially in industrial and scientific contexts requiring multi-source data integration (e.g., text, tables, time-series). Static retrieval methods worsen this by not adapting to the reasoning process’s changing needs. ReAR techniques address this with chained retrieval, as in CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)), which breaks multi-hop questions into sequential sub-queries (e.g., retrieving ”event causes” then their ”impacts”), systematically linking knowledge. Reasoning-state-aware retrieval, used in FLARE (Jiang et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib46)), predicts future information needs by generating interim prompts (e.g., ”the next step requires discussion of …”), enabling dynamic query construction that preserves coherence. Together, these approaches resolve the conflict between discrete retrieval and continuous reasoning.

#### 3.2.2. Reasoning Discontinuity Caused by Domain Knowledge Boundaries

Reasoning discontinuity arises from LLMs’ limited knowledge, struggling with specialized domains (e.g., semiconductor design in PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83))) and real-time data (e.g., medical parameters in Agentic Reasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93))). End-to-end models often produce factual errors, while traditional RAG methods fail to retrieve deep professional knowledge due to coarse retrieval, especially with complex data like tables,charts and images.

ReAR addresses this with two complementary solutions: knowledge atomization and structural organization, as in PIKE’s decomposition of documents into fine-grained units and multi-layer knowledge graphs for semantic and logical retrieval; and dynamic tool integration, as in Agentic Reasoning’s real-time data acquisition via code execution and API calls to compute critical indicators (e.g., medical FiO2). These innovations overcome the challenges of specialized knowledge depth and timely information relevance that limit conventional methods.

#### 3.2.3. Search Space Explosion and Local Optima Traps

The main challenge in multi-step reasoning is the exponential growth of the search space, where methods like Chain-of-Thought (CoT) often yield suboptimal or inconsistent results due to unconstrained hypotheses. Traditional approaches like CoT and Tree-of-Thought (ToT) lack external knowledge constraints, causing invalid assumptions, while purely symbolic reasoning falls short in open-domain tasks. To address this, two strategies are used: knowledge base-anchored heuristic search (KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59))), which limits reasoning actions to subgraphs in knowledge graphs, and a retrieval-verification mechanism (Search-o1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52))) that prunes unsupported reasoning paths using evidence from the knowledge base. Together, these reduce the search space and preserve reasoning coherence.

#### 3.2.4. Dynamic Knowledge Requirements in Multi-Step Reasoning

Complex multi-step reasoning tasks face the challenge of continuously changing knowledge requirements. This is evident in cases like multi-hop reasoning and engineering planning, where each stage generates new sub-problems (e.g., moving from ”architectural design” to ”material cost estimation”). Static knowledge bases or one-time retrieval methods cannot meet this evolving demand. This manifests in two ways: initial knowledge may miss later needs, causing gaps; and fixed knowledge sets may include irrelevant information, reducing reasoning accuracy. To address this, new retrieval-augmented reasoning approaches introduce dynamic solutions: process supervision (e.g., reward models in RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97))) detects knowledge gaps in real time, atomic decision-making (e.g., step decomposition in DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25))) triggers retrieval as needed, and tree-like expansions (e.g., multi-path retrieval in DeepSolution (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55))) enable parallel exploration. By integrating knowledge retrieval within reasoning, these methods let the system identify, supplement, and verify knowledge dynamically—much like a human expert—greatly enhancing the reliability and completeness of complex reasoning.

#### 3.2.5. Insufficient Depth and Breadth of Reasoning

This issue is prominent in expert tasks like medical diagnosis, legal analysis, and research report generation. LLMs’ static knowledge often fails to capture the evolving scope of domain knowledge, resulting in shallow reasoning that misses multi-level, cross-domain connections. For example, when assessing ”Company A is affected by economic recession,” traditional methods rely on superficial statistical patterns and cannot systematically follow the deeper logical chain from ”Company A → industry supply chain → macroeconomic policy → international political landscape,” leading to reasoning that lacks causal depth.

To overcome this, recent advances use structured, retrieval-enhanced frameworks. ToG2.0 (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) models Knowledge Graph relational paths as retrieval guidance vectors, enabling targeted queries along entity paths, surpassing the limits of keyword-based retrieval. This approach complements CR-Planner’s (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) iterative expansion, which triggers retrieval of specialized knowledge (e.g., textbook proofs of algorithm complexity) at critical reasoning points, ensuring accurate domain knowledge integration via multi-round validation. Addressing cross-domain knowledge linkage, CO-STORM (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) employs a multi-agent system whose host module generates cross-modal retrieval commands by analyzing potential semantics in uncited documents.

## 4. Patterns of synergy

![Refer to caption](extracted/6386523/Images/pattern.png)

Figure 5. Patterns of Synergy between RAG and Reasoning

Section [3](https://arxiv.org/html/2504.15909v2#S3 "3. The purpose of the synergy ‣ Synergizing RAG and Reasoning: A Systematic Review") detailed the need and motivation for integrating RAG with reasoning. Building on this, this section presents two core implementation patterns for RAG-reasoning synergy (Figure [5](https://arxiv.org/html/2504.15909v2#S4.F5 "Figure 5 ‣ 4. Patterns of synergy ‣ Synergizing RAG and Reasoning: A Systematic Review")): (1) the Pre-defined Workflow, which uses logical architectures with preset rules for coordination, and (2) Dynamic Workflow, which relies on context-aware, adaptive coordination via real-time decision engines. These patterns illustrate current frameworks combining knowledge retrieval and multi-step reasoning from deterministic and flexible perspectives.

### 4.1. Pre-defined workflow

Pre-defined workflow is a multi-step reasoning approach with a fixed architecture and sequential execution, emphasizing process clarity and operational determinism. It consists of predefined iterative stages, each with strict input-output rules and no dynamic changes based on intermediate results. This modular design ensures controllability and structured reasoning for complex tasks. All steps are executed regardless of intermediate outcomes, guaranteeing repeatability and stability while avoiding uncertainties from dynamic decisions. Although it sacrifices adaptability, this approach offers procedural predictability and is well-suited for scenarios demanding clear reasoning paths, albeit with possible computational redundancy due to lack of real-time adjustments.

Mathematically, the pre-defined RAG workflow can be formalized as a deterministic multi-step operational chain. Given an input query Q𝑄Qitalic\_Q and a predefined sequence of N𝑁Nitalic\_N reasoning steps and the final decision output D𝐷Ditalic\_D, the complete workflow is expressed as:

|  |  |  |  |
| --- | --- | --- | --- |
| (1) |  | D=fN∘⋯∘f2∘f1⁢(Q)𝐷subscript𝑓𝑁⋯subscript𝑓2subscript𝑓1𝑄D=f\_{N}\circ\cdots\circ f\_{2}\circ f\_{1}(Q)italic\_D = italic\_f start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ∘ ⋯ ∘ italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT ∘ italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ( italic\_Q ) |  |

where each fi∈{Ψ,R,Γ}subscript𝑓𝑖Ψ𝑅Γf\_{i}\in\{\Psi,R,\Gamma\}italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ∈ { roman\_Ψ , italic\_R , roman\_Γ } denotes strictly defined functions for reasoning (ΨΨ\Psiroman\_Ψ), retrieval (R𝑅Ritalic\_R), or decision-making (ΓΓ\Gammaroman\_Γ), with ∘\circ∘ representing function composition. This formulation adheres to the fixed mapping sequence Q↦Ψ⁢(Q)↦R⁢(Ψ⁢(Q))↦Γ⁢(R⁢(Ψ⁢(Q)))maps-to𝑄Ψ𝑄maps-to𝑅Ψ𝑄maps-toΓ𝑅Ψ𝑄Q\mapsto\Psi(Q)\mapsto R(\Psi(Q))\mapsto\Gamma(R(\Psi(Q)))italic\_Q ↦ roman\_Ψ ( italic\_Q ) ↦ italic\_R ( roman\_Ψ ( italic\_Q ) ) ↦ roman\_Γ ( italic\_R ( roman\_Ψ ( italic\_Q ) ) ), exhibiting Markovian properties where ft+1subscript𝑓𝑡1f\_{t+1}italic\_f start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT depends solely on ftsubscript𝑓𝑡f\_{t}italic\_f start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT’s output while remaining independent of historical states {f<t}subscript𝑓absent𝑡\{f\_{<t}\}{ italic\_f start\_POSTSUBSCRIPT < italic\_t end\_POSTSUBSCRIPT }. The chained composition guarantees process closure and reproducibility, though constrained by the static combinatorial nature of {fi}i=1Nsuperscriptsubscriptsubscript𝑓𝑖𝑖1𝑁\{f\_{i}\}\_{i=1}^{N}{ italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT } start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT.

In the pre-defined pipeline, based on the position where reasoning is introduced, it can be further divided into Pre-Retrieval, Post-Retrieval, and Hybrid.

#### 4.1.1. Pre-Retrieval Reasoning

For pre-retrieval methods, the sequence is explicitly defined as

|  |  |  |  |
| --- | --- | --- | --- |
| (2) |  | D=Γ∘ℛ∘Ψ⁢(Q)𝐷ΓℛΨ𝑄D=\Gamma\circ\mathcal{R}\circ\Psi(Q)italic\_D = roman\_Γ ∘ caligraphic\_R ∘ roman\_Ψ ( italic\_Q ) |  |

where ΨΨ\Psiroman\_Ψ denotes a reasoning operator that systematically transforms or enriches the query prior to retrieval.
This paradigm enhances retrieval precision by resolving ambiguities, inferring implicit intents, or optimizing query representations. Current research identifies four principal methodological categories for designing ΨΨ\Psiroman\_Ψ:

Query Optimization focuses on generating and selecting query variants to maximize retrieval relevance. Mathematically, this is formalized as Candidates=Generate⁢(Q,C)CandidatesGenerate𝑄𝐶\text{Candidates}=\text{Generate}(Q,C)Candidates = Generate ( italic\_Q , italic\_C ), ΨOptimize⁢(Q,C)=arg⁡maxcandidate∈Candidates⁡Score⁢(candidate)subscriptΨOptimize𝑄𝐶subscriptcandidateCandidatesScorecandidate\quad\Psi\_{\text{Optimize}}(Q,C)=\arg\max\_{\text{candidate}\in\text{Candidates%
}}\text{Score}(\text{candidate})roman\_Ψ start\_POSTSUBSCRIPT Optimize end\_POSTSUBSCRIPT ( italic\_Q , italic\_C ) = roman\_arg roman\_max start\_POSTSUBSCRIPT candidate ∈ Candidates end\_POSTSUBSCRIPT Score ( candidate ), where (Generate) produces candidate queries and (arg⁡max\arg\maxroman\_arg roman\_max) selects optimal variants based on contrastive training or reinforcement learning. Representative implementations, such as LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)), leverage iterative sampling and optimization to balance query diversity and specificity.

Attribute Judgment employs classification mechanisms to dynamically regulate retrieval triggers. This is modeled as ΨClassify⁢(Q)=Classify⁢(Q)subscriptΨClassify𝑄Classify𝑄\Psi\_{\text{Classify}}(Q)=\text{Classify}(Q)roman\_Ψ start\_POSTSUBSCRIPT Classify end\_POSTSUBSCRIPT ( italic\_Q ) = Classify ( italic\_Q ), where Classify evaluates query attributes (e.g., temporal sensitivity, intent complexity) against predefined criteria. Frameworks like UAR (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) and AdaptiveRAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) exemplify this approach by integrating multi-stage classifiers to minimize unnecessary retrievals.

Plan Generation decomposes complex queries into structured sub-task sequences to guide retrieval direction. Formulated as ΨPlan⁢(Q)=Plan⁢(Q)subscriptΨPlan𝑄Plan𝑄\Psi\_{\text{Plan}}(Q)=\text{Plan}(Q)roman\_Ψ start\_POSTSUBSCRIPT Plan end\_POSTSUBSCRIPT ( italic\_Q ) = Plan ( italic\_Q ), the operator Plan generates hierarchical task decompositions, as seen in PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) , which utilizes chain-of-thought reasoning to align retrieval targets with multi-step problem-solving requirements.

Semantic Enhancement enriches query representations using domain-specific or task-aware embeddings. Expressed as ΨEnhance⁢(Q)=Encode⁢(Q,𝒦)subscriptΨEnhance𝑄Encode𝑄𝒦\Psi\_{\text{Enhance}}(Q)=\text{Encode}(Q,\mathcal{K})roman\_Ψ start\_POSTSUBSCRIPT Enhance end\_POSTSUBSCRIPT ( italic\_Q ) = Encode ( italic\_Q , caligraphic\_K ), where 𝒦𝒦\mathcal{K}caligraphic\_K denotes auxiliary knowledge (e.g., reasoning trajectories), methods like O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) integrate latent reasoning patterns into query embeddings to improve retrieval robustness.

Collectively, these methodologies demonstrate that pre-retrieval reasoning serves as a systematic interface to mitigate semantic gaps between raw queries and knowledge bases, establishing a critical component for precision-driven RAG architectures.

#### 4.1.2. Post-Retrieval Reasoning

In pre-defined RAG systems with multi-step reasoning pipelines, the post-retrieval reasoning paradigm represents a critical advancement where cognitive processing occurs after information retrieval from external sources. This approach addresses inherent limitations in conventional RAG, particularly in managing knowledge conflicts, mitigating information insufficiency, and enhancing logical consistency across complex reasoning tasks. Mathematically, this process can be formalized as a deterministic function composition:

|  |  |  |  |
| --- | --- | --- | --- |
| (3) |  | D=Γ∘Ψ∘ℛ⁢(Q)𝐷ΓΨℛ𝑄D=\Gamma\circ\Psi\circ\mathcal{R}(Q)italic\_D = roman\_Γ ∘ roman\_Ψ ∘ caligraphic\_R ( italic\_Q ) |  |

ℛℛ\mathcal{R}caligraphic\_R denotes the retrieval operator, ΨΨ\Psiroman\_Ψ implements the reasoning transformation, and ΓΓ\Gammaroman\_Γ represents the final decision function.

The core characteristic of Post-Retrieval Reasoning lies in its execution of the reasoning process after retrieval, with the reasoning target being the retrieved content. ToG2.0 (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) proposes an iterative multi-step reasoning framework that alternates between graph retrieval and context retrieval, integrating the reasoning judgment of LLMs to progressively expand entities and prune irrelevant information, ultimately generating accurate answers. This approach dynamically addresses the issue of insufficient information through iterative refinement while establishing a dual-evidence verification mechanism via knowledge graph relation pruning and entity-guided context retrieval. Its graph-structured reasoning module transforms the connectivity validation of triple paths into a constraint satisfaction problem, effectively mitigating logical inconsistencies between text fragments and thereby significantly improving the quality of complex question answering.

ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)), on the other hand, employs a predefined three-stage process (Self-Inquiry → Knowledge Assimilation → Thought Accommodation) to structurally comprehend and calibrate retrieved knowledge, resolving conflicts between parametric memory and external knowledge. During the Knowledge Assimilation stage, ActiveRAG enhances the corrective effect of external knowledge on the internal representations of LLMs through multi-instruction fine-tuning strategies (e.g., counterfactual comparison and anchor association), substantially reducing the likelihood of hallucination generation. ARM’s (Chen et al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)) structural alignment and self-verification stages also demonstrate optimization for post-retrieval reasoning. By incorporating domain knowledge via mixed-integer programming (MIP) solvers, ARM ensures the rationality and coverage of retrieval results, providing a scalable optimization framework for multi-source data compatibility and thereby enabling globally optimal cross-modal retrieval.

#### 4.1.3. Hybrid Reasoning

The Hybrid pattern of pre-defined process forms a composite processing paradigm by integrating pre-retrieval reasoning with post-retrieval reasoning. The essence is formalized as a multi-round recursive iterative process, where each iteration cycle strictly comprises three phases: Retrieval, Generation, and Reasoning, executed as structured composite operations. Let the total number of iterations be T𝑇Titalic\_T; the workflow is defined as:

|  |  |  |  |
| --- | --- | --- | --- |
| (4) |  | QT=(○t=1Tℛ𝓉∘Γt∘Ψt)(Q0)Q\_{T}=\left(\bigcirc\_{t=1}^{T}\mathcal{R\_{t}}\circ\Gamma\_{t}\circ\Psi\_{t}% \right)(Q\_{0})italic\_Q start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT = ( ○ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT caligraphic\_R start\_POSTSUBSCRIPT caligraphic\_t end\_POSTSUBSCRIPT ∘ roman\_Γ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ∘ roman\_Ψ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ( italic\_Q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ) |  |

Here, each iterative unit is indexed by t𝑡titalic\_t. The process terminates when a predefined condition 𝒯⁢(Qt,Dt,Ct)𝒯subscript𝑄𝑡subscript𝐷𝑡subscript𝐶𝑡\mathcal{T}(Q\_{t},D\_{t},C\_{t})caligraphic\_T ( italic\_Q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) is met, yielding the final response Γfinal⁢(CT)subscriptΓfinalsubscript𝐶𝑇\Gamma\_{\text{final}}(C\_{T})roman\_Γ start\_POSTSUBSCRIPT final end\_POSTSUBSCRIPT ( italic\_C start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT ). This recursive mechanism enables dynamic synergy between knowledge acquisition and semantic inference, overcoming the linear limitations of single-cycle retrieval-generation frameworks.

IR-CoT (Trivedi et al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79)) leverages chain-of-thought reasoning to iteratively construct intermediate logic chains, enabling multi-hop retrieval guided by progressively refined contextual cues. FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) introduces a dual-phase architecture that first generates structured search graphs to model temporal and entity dependencies, followed by dynamic query rewriting to optimize financial data retrieval. LevelRAG employs hierarchical validation mechanisms, aggregating multi-granular retrieval results and triggering supplementary retrievals based on context completeness assessments. ITER-RETGEN (Shao et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)) utilizes generation-enhanced feedback loops to iteratively refine query representations, enhancing semantic alignment between retrieval and generation phases.

These approaches share a common foundation in structured recursion while diverging in operational mechanisms. By enforcing deterministic iteration cycles, they balance controlled workflow execution with adaptive semantic exploration, addressing challenges such as multi-step reasoning, temporal coherence, and cross-domain knowledge synthesis. The hybrid paradigm’s strength lies in its capacity to decompose complex queries into iterative retrieval-generation units, systematically bridging knowledge gaps while maintaining interpretability and robustness in open-domain problem-solving scenarios.

### 4.2. Dynamic RAG Workflow

The RAG with dynamic workflow represents an autonomous reasoning architecture centered around LLMs, characterized by the integration of non-deterministic operational workflows and real-time decision-making capabilities. Unlike pre-defined pipelines, this architecture enables continuous monitoring of reasoning states to dynamically trigger retrieval, generation, or verification operations. The LLM actively evaluates contextual demands during reasoning processes, autonomously determining optimal moments for invoking external tools or resources through a hybrid feedback coordination mechanism. By eliminating fixed iterative units and pre-determined tool-calling sequences, the framework achieves dynamic evolution of execution pathways, demonstrating superior adaptability in complex cognitive tasks through real-time adjustment of computational workflows based on intermediate reasoning outcomes.

This dynamic architecture manifests three principal characteristics: 1) Operator invocation is governed by the LLM’s contextual state analysis, exemplified through special token prediction (e.g., ‘[Web-Search]‘ or ‘¡begin\_of\_query¿‘) to initiate external operations; 2) Reasoning trajectories exhibit high flexibility, allowing dynamic query reformulation and sub-problem generation to overcome limitations of static workflows; 3) Context-driven decision mechanisms prioritize real-time reasoning states over predefined rules, enhancing systemic responsiveness to emergent task complexities while improving precision.

Defining the reasoning state at time t𝑡titalic\_t as St=(Ht,Ct)subscript𝑆𝑡subscript𝐻𝑡subscript𝐶𝑡S\_{t}=(H\_{t},C\_{t})italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = ( italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ), where Htsubscript𝐻𝑡H\_{t}italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT denotes historical information aggregation and Ctsubscript𝐶𝑡C\_{t}italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT represents contextual embedding vectors, the decision process is modeled as a stochastic system:

|  |  |  |  |
| --- | --- | --- | --- |
| (5) |  | at+1∼π⁢(St;Θ)similar-tosubscript𝑎𝑡1𝜋  subscript𝑆𝑡Θa\_{t+1}\sim\pi(S\_{t};\Theta)italic\_a start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ∼ italic\_π ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ; roman\_Θ ) |  |

|  |  |  |  |
| --- | --- | --- | --- |
| (6) |  | St+1=δ⁢(St,𝒯at+1⁢(St))subscript𝑆𝑡1𝛿subscript𝑆𝑡subscript𝒯subscript𝑎𝑡1subscript𝑆𝑡S\_{t+1}=\delta(S\_{t},\mathcal{T}\_{a\_{t+1}}(S\_{t}))italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_δ ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_T start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ) |  |

Here, π:𝒮→Δ⁢(𝒜):𝜋→𝒮Δ𝒜\pi:\mathcal{S}\to\Delta(\mathcal{A})italic\_π : caligraphic\_S → roman\_Δ ( caligraphic\_A ) constitutes the policy function mapping states to probability distributions over action space 𝒜𝒜\mathcal{A}caligraphic\_A (retrieval, generation, verification, etc.), while 𝒯asubscript𝒯𝑎\mathcal{T}\_{a}caligraphic\_T start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT denotes state transition functions corresponding to action a𝑎aitalic\_a. The non-Markovian nature of the system emerges from St+1subscript𝑆𝑡1S\_{t+1}italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT’s dependence on complete historical trajectories {S≤t}subscript𝑆absent𝑡\{S\_{\leq t}\}{ italic\_S start\_POSTSUBSCRIPT ≤ italic\_t end\_POSTSUBSCRIPT }, with dynamic adaptability ensured through extensible action spaces 𝒜𝒜\mathcal{A}caligraphic\_A and online optimization of policy parameters ΘΘ\Thetaroman\_Θ. This formulation enables context-sensitive state updates via δ:𝒮×𝒪→𝒮:𝛿→𝒮𝒪𝒮\delta:\mathcal{S}\times\mathcal{O}\to\mathcal{S}italic\_δ : caligraphic\_S × caligraphic\_O → caligraphic\_S, establishing a theoretical foundation for open-ended reasoning processes in complex problem domains.

Based on the mode of reasoning initiation, agentic RAG with dynamic workflows can be further categorized into three distinct types: Proactivity-driven, Reflection-driven, and Feedback-driven mechanisms.
The LLM proactivity-driven approach is characterized by the model’s autonomous triggering of actions based on internal assessments, executing operations without external intervention through mechanisms analogous to human intuitive decision-making—for instance, when the model independently identifies insufficient evidentiary support in the current reasoning process, it proactively generates retrieval requests to supplement information. The reflection-driven mode emphasizes self-examination of the reasoning process, dynamically initiating subsequent operations through quantitative evaluation of intermediate result quality (e.g., triggering actions when the calculated reasoning support score of 0.7 exceeds a predefined threshold of 0.6), which simulates the self-optimization logic of expert systems, enabling the model to adjust reasoning pathways through introspection. The feedback-driven mechanism incorporates external intervention, employing independent models or rule-based systems to perform real-time scoring of intermediate states (e.g., an external reward model assigning a 2.5/5 score to reasoning steps) while providing corrective suggestions, operating similarly to a mentor-guided mode that continuously calibrates the reasoning workflow through external feedback signals.

#### 4.2.1. Proactivity-Driven Reasoning

The core innovation of Proactivity-driven Reasoning lies in enabling LLMs to fully govern the reasoning process through self-triggered prediction mechanisms. This active control manifests through three key mechanisms: (1) direct tool invocation via model-generated special tokens (e.g., [Web-Search]), without external intervention, (2) context-aware decision making based on real-time knowledge gaps or hypothesis verification requirements, and (3) Markov Decision Process (MDP)-based dynamic path optimization.

Formally, the reasoning process can be modeled as a state sequence S={s0,s1,…,st}𝑆subscript𝑠0subscript𝑠1…subscript𝑠𝑡S=\{s\_{0},s\_{1},\dots,s\_{t}\}italic\_S = { italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT }, where each state stsubscript𝑠𝑡s\_{t}italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT encapsulates the current reasoning context. At each step t𝑡titalic\_t, the LLM selects an action at∈{retrieve,generate,terminate}subscript𝑎𝑡retrievegenerateterminatea\_{t}\in\{\text{retrieve},\text{generate},\text{terminate}\}italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ∈ { retrieve , generate , terminate } based on stsubscript𝑠𝑡s\_{t}italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, executes the corresponding operation (e.g., document retrieval or answer generation), and updates its state through transition function st+1=δ⁢(st,at,ot)subscript𝑠𝑡1𝛿subscript𝑠𝑡subscript𝑎𝑡subscript𝑜𝑡s\_{t+1}=\delta(s\_{t},a\_{t},o\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_δ ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_o start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) where otsubscript𝑜𝑡o\_{t}italic\_o start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT represents action outcomes. This MDP framework enables dynamic path adjustment through real-time feedback until termination (aT=terminatesubscript𝑎𝑇terminatea\_{T}=\text{terminate}italic\_a start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT = terminate) and final answer generation.

Recent advancements demonstrate significant improvements over conventional RAG approaches. The Agentic Reasoning framework achieves granular control through dynamic tool invocation, eliminating predefined execution sequences. DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) optimizes cost-accuracy tradeoffs via MDP-based imitation learning, addressing the retrieval-generation disconnection in traditional systems. CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)) introduces hybrid-driven mechanisms combining LLM-initiated subqueries with external policy control, enhancing error tolerance for complex queries. Collectively, these approaches establish a paradigm shift from fixed pipelines to context-sensitive, self-optimizing reasoning architectures.

#### 4.2.2. Reflection-Driven Reasoning

The reflection-driven mechanism represents a dynamic reasoning framework that enables iterative self-evaluation and revision of intermediate outputs through model introspection. Common methods include: (1) a evaluation system combining explicit token prediction and implicit confidence scoring, (2) self-monitoring capabilities through grounding tokens for content-document consistency verification and utility tokens for answer effectiveness assessment, and (3) adaptive routing mechanisms that automatically select single-hop or multi-hop reasoning paths based on contextual complexity. The mathematical formalism of this process can be expressed as:

|  |  |  |  |
| --- | --- | --- | --- |
| (7) |  | 𝒫=⋃t=1T[G⁢(𝐂t)→E⁢(𝐇t,𝒟)→ψ⁢(ϕ⁢(𝐞t),τ)]𝒫superscriptsubscript𝑡1𝑇delimited-[]→𝐺subscript𝐂𝑡𝐸subscript𝐇𝑡𝒟→𝜓italic-ϕsubscript𝐞𝑡𝜏\mathcal{P}=\bigcup\_{t=1}^{T}\left[G(\mathbf{C}\_{t})\rightarrow E(\mathbf{H}\_{% t},\mathcal{D})\rightarrow\psi(\phi(\mathbf{e}\_{t}),\tau)\right]caligraphic\_P = ⋃ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT [ italic\_G ( bold\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) → italic\_E ( bold\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_D ) → italic\_ψ ( italic\_ϕ ( bold\_e start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) , italic\_τ ) ] |  |

where G𝐺Gitalic\_G denotes the generation function operating on current context 𝐜tsubscript𝐜𝑡\mathbf{c}\_{t}bold\_c start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, E𝐸Eitalic\_E represents the evaluation function that assesses hidden states 𝐡tsubscript𝐡𝑡\mathbf{h}\_{t}bold\_h start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT against external knowledge base 𝒟𝒟\mathcal{D}caligraphic\_D, ϕitalic-ϕ\phiitalic\_ϕ serves as the confidence mapping function, τ𝜏\tauitalic\_τ is the decision threshold, and ψ𝜓\psiitalic\_ψ functions as the branch selector.

In practical implementations like Self-RAG (Asai et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)), this framework generates candidate responses alongside reflection tokens, computes passage relevance scores (ISREL∈[0,1]ISREL01\text{ISREL}\in[0,1]ISREL ∈ [ 0 , 1 ]) and factual support metrics (ISSUP), and employs weighted aggregation of token probabilities in ϕitalic-ϕ\phiitalic\_ϕ to determine retrieval activation or generation revision through threshold-based δ𝛿\deltaitalic\_δ operations. Meanwhile, Open-RAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) incorporates hybrid threshold mechanisms and Mixture-of-Experts architecture to enforce counterfactual verification through non-retrieval confidence scoring (PrNoRTsubscriptPrNoRT\text{Pr}\_{\text{NoRT}}Pr start\_POSTSUBSCRIPT NoRT end\_POSTSUBSCRIPT), enabling dynamic expansion of complex reasoning capabilities while preserving base model efficiency. ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) utilizes knowledge-guided reasoning chains combined with external knowledge sources to perform reflection-driven reasoning. In each iteration, it adjusts the reasoning path through the ”Thought-Action-Observation” paradigm, effectively preventing error propagation and improving answer accuracy.

The paradigm’s innovation lies in reconstructing traditional sequential processes into conditional Markov decision processes, where state transition probabilities P⁢(st+1|st)𝑃conditionalsubscript𝑠𝑡1subscript𝑠𝑡P(s\_{t+1}|s\_{t})italic\_P ( italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) are dynamically determined by model self-evaluation outcomes. Compared to proactive LLM-driven methods (e.g., Toolformer’s direct API invocation), the reflection-driven approach establishes closed-loop control through explicit evaluation stages (function E𝐸Eitalic\_E), effectively mitigating hallucination risks while maintaining computational efficiency.

#### 4.2.3. Feedback-Driven Reasoning

The feedback-driven dynamic RAG system establishes closed-loop control over reasoning processes through external signals, formally modeled as a Partially Observable Markov Decision Process. The system state st=(qt,𝒦t,Ht)subscript𝑠𝑡subscript𝑞𝑡subscript𝒦𝑡subscript𝐻𝑡s\_{t}=(q\_{t},\mathcal{K}\_{t},H\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) evolves through iterative interactions, comprising the current query representation qtsubscript𝑞𝑡q\_{t}italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, dynamic knowledge base 𝒦tsubscript𝒦𝑡\mathcal{K}\_{t}caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, and historical trajectory ℋtsubscriptℋ𝑡\mathcal{H}\_{t}caligraphic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT. Initialized with q0subscript𝑞0q\_{0}italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT and 𝒦0=∅subscript𝒦0\mathcal{K}\_{0}=\emptysetcaligraphic\_K start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT = ∅, the policy function π⁢(at|st)𝜋conditionalsubscript𝑎𝑡subscript𝑠𝑡\pi(a\_{t}|s\_{t})italic\_π ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) generates actions from the operational space 𝒜={Retrieve,Reason,Verify,Answer,∅}𝒜RetrieveReasonVerifyAnswer\mathcal{A}=\{\text{Retrieve},\text{Reason},\text{Verify},\text{Answer},\emptyset\}caligraphic\_A = { Retrieve , Reason , Verify , Answer , ∅ }. State transitions follow st+1=δ⁢(st,at)subscript𝑠𝑡1𝛿subscript𝑠𝑡subscript𝑎𝑡s\_{t+1}=\delta(s\_{t},a\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_δ ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) with knowledge base updates

|  |  |  |  |
| --- | --- | --- | --- |
| (8) |  | 𝒦t+1=𝒦t⊕Retrieve⁢(qt)⋅𝕀⁢(at=Retrieve)subscript𝒦𝑡1direct-sumsubscript𝒦𝑡⋅Retrievesubscript𝑞𝑡𝕀subscript𝑎𝑡Retrieve\mathcal{K}\_{t+1}=\mathcal{K}\_{t}\oplus\text{Retrieve}(q\_{t})\cdot\mathbb{I}(a% \_{t}=\text{Retrieve})caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ⊕ Retrieve ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ⋅ blackboard\_I ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = Retrieve ) |  |

where ⊕direct-sum\oplus⊕ denotes incremental updates and 𝕀𝕀\mathbb{I}blackboard\_I represents an indicator function. The reward function R⁢(st,at,st+1)→rt→𝑅subscript𝑠𝑡subscript𝑎𝑡subscript𝑠𝑡1subscript𝑟𝑡R(s\_{t},a\_{t},s\_{t+1})\rightarrow r\_{t}italic\_R ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ) → italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT drives policy optimization through

|  |  |  |  |
| --- | --- | --- | --- |
| (9) |  | πt+1=Ω⁢(πt,∇θ𝔼a∼πt⁢[R⁢(st,a,st+1)])subscript𝜋𝑡1Ωsubscript𝜋𝑡subscript∇𝜃subscript𝔼similar-to𝑎subscript𝜋𝑡delimited-[]𝑅subscript𝑠𝑡𝑎subscript𝑠𝑡1\pi\_{t+1}=\Omega(\pi\_{t},\nabla\_{\theta}\mathbb{E}\_{a\sim\pi\_{t}}[R(s\_{t},a,s\_% {t+1})])italic\_π start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = roman\_Ω ( italic\_π start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , ∇ start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT blackboard\_E start\_POSTSUBSCRIPT italic\_a ∼ italic\_π start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT [ italic\_R ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a , italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ) ] ) |  |

forming an adaptive control loop. Three distinct feedback mechanisms emerge within this framework.

Explicit reward feedback employs specialized models πrewardsubscript𝜋reward\pi\_{\text{reward}}italic\_π start\_POSTSUBSCRIPT reward end\_POSTSUBSCRIPT for quantitative evaluation, exemplified by RAG-Gym’s process rewards (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)). The reward function combines immediate and terminal rewards:

|  |  |  |  |
| --- | --- | --- | --- |
| (10) |  | rt=λ1⁢πreward⁢(st)+λ2⁢𝔼st+k⁢[γk⁢Rterminal]subscript𝑟𝑡subscript𝜆1subscript𝜋rewardsubscript𝑠𝑡subscript𝜆2subscript𝔼subscript𝑠𝑡𝑘delimited-[]superscript𝛾𝑘subscript𝑅terminalr\_{t}=\lambda\_{1}\pi\_{\text{reward}}(s\_{t})+\lambda\_{2}\mathbb{E}\_{s\_{t+k}}[% \gamma^{k}R\_{\text{terminal}}]italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = italic\_λ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT italic\_π start\_POSTSUBSCRIPT reward end\_POSTSUBSCRIPT ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) + italic\_λ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT blackboard\_E start\_POSTSUBSCRIPT italic\_s start\_POSTSUBSCRIPT italic\_t + italic\_k end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT [ italic\_γ start\_POSTSUPERSCRIPT italic\_k end\_POSTSUPERSCRIPT italic\_R start\_POSTSUBSCRIPT terminal end\_POSTSUBSCRIPT ] |  |

with discount factor γ𝛾\gammaitalic\_γ. SmartRAG extends this through policy gradient optimization

|  |  |  |  |
| --- | --- | --- | --- |
| (11) |  | ∇θJ⁢(θ)=𝔼τ∼πθ⁢[∑t=0T∇θlog⁡πθ⁢(at|st)⁢A^t]subscript∇𝜃𝐽𝜃subscript𝔼similar-to𝜏subscript𝜋𝜃delimited-[]superscriptsubscript𝑡0𝑇subscript∇𝜃subscript𝜋𝜃conditionalsubscript𝑎𝑡subscript𝑠𝑡subscript^𝐴𝑡\nabla\_{\theta}J(\theta)=\mathbb{E}\_{\tau\sim\pi\_{\theta}}[\sum\_{t=0}^{T}% \nabla\_{\theta}\log\pi\_{\theta}(a\_{t}|s\_{t})\hat{A}\_{t}]∇ start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT italic\_J ( italic\_θ ) = blackboard\_E start\_POSTSUBSCRIPT italic\_τ ∼ italic\_π start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT [ ∑ start\_POSTSUBSCRIPT italic\_t = 0 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT ∇ start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT roman\_log italic\_π start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) over^ start\_ARG italic\_A end\_ARG start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ] |  |

where the advantage function A^tsubscript^𝐴𝑡\hat{A}\_{t}over^ start\_ARG italic\_A end\_ARG start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT integrates temporal feedback.

Implicit environmental feedback derives from knowledge base validation, as implemented in KBQA-o1’s SPARQL verification and SolutionRAG’s pruning mechanisms (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)). This feedback is formalized as rt=𝕀(𝒦t⊧q0)⋅cvalid−𝕀(⊥∈𝒦t)⋅cinvalidr\_{t}=\mathbb{I}(\mathcal{K}\_{t}\models q\_{0})\cdot c\_{\text{valid}}-\mathbb{I%
}(\bot\in\mathcal{K}\_{t})\cdot c\_{\text{invalid}}italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = blackboard\_I ( caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ⊧ italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ) ⋅ italic\_c start\_POSTSUBSCRIPT valid end\_POSTSUBSCRIPT - blackboard\_I ( ⊥ ∈ caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ⋅ italic\_c start\_POSTSUBSCRIPT invalid end\_POSTSUBSCRIPT with validation function 𝕀⁢(⋅)𝕀⋅\mathbb{I}(\cdot)blackboard\_I ( ⋅ ) and penalty coefficients c𝑐citalic\_c. ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) introduces threshold-triggered correction: when rt<τsubscript𝑟𝑡𝜏r\_{t}<\tauitalic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT < italic\_τ, it activates refinement loops 𝒦t+1=PEM⁢(𝒦t,q0)⊕Retrieve⁢(PRM⁢(st))subscript𝒦𝑡1direct-sumPEMsubscript𝒦𝑡subscript𝑞0RetrievePRMsubscript𝑠𝑡\mathcal{K}\_{t+1}=\text{PEM}(\mathcal{K}\_{t},q\_{0})\oplus\text{Retrieve}(\text%
{PRM}(s\_{t}))caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = PEM ( caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ) ⊕ Retrieve ( PRM ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ).

Structured rule feedback encodes domain knowledge through differentiable scoring functions. MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)) implements depth-attenuated rewards

|  |  |  |  |
| --- | --- | --- | --- |
| (12) |  | rt=11+α⁢dt⁢∑i=1nLLMscore⁢(at(i))subscript𝑟𝑡11𝛼subscript𝑑𝑡superscriptsubscript𝑖1𝑛subscriptLLMscoresuperscriptsubscript𝑎𝑡𝑖r\_{t}=\frac{1}{1+\alpha d\_{t}}\sum\_{i=1}^{n}\text{LLM}\_{\text{score}}(a\_{t}^{(% i)})italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = divide start\_ARG 1 end\_ARG start\_ARG 1 + italic\_α italic\_d start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_ARG ∑ start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT LLM start\_POSTSUBSCRIPT score end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT ( italic\_i ) end\_POSTSUPERSCRIPT ) |  |

with search depth dtsubscript𝑑𝑡d\_{t}italic\_d start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPTand decay coefficient α𝛼\alphaitalic\_α. CR-Planner’s hierarchical critique combines subgoal and execution scores: rttotal=β1⁢πsub⁢(st)+β2⁢πexec⁢(at|st)superscriptsubscript𝑟𝑡totalsubscript𝛽1subscript𝜋subsubscript𝑠𝑡subscript𝛽2subscript𝜋execconditionalsubscript𝑎𝑡subscript𝑠𝑡r\_{t}^{\text{total}}=\beta\_{1}\pi\_{\text{sub}}(s\_{t})+\beta\_{2}\pi\_{\text{exec%
}}(a\_{t}|s\_{t})italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT total end\_POSTSUPERSCRIPT = italic\_β start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT italic\_π start\_POSTSUBSCRIPT sub end\_POSTSUBSCRIPT ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) + italic\_β start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT italic\_π start\_POSTSUBSCRIPT exec end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) through weighted fusion.

These feedback mechanisms interact through a unified strategy update framework, where external feedback-driven approaches achieve controllable optimization of the reasoning process through interpretable feedback signals while maintaining the generative capabilities of LLMs. Overall, the dynamic process of RAG, by endowing the model with autonomy in the reasoning process, not only enhances adaptability to complex tasks but also provides a new solution for efficient reasoning in resource-constrained environments.

## 5. Implementation and Optimization

Building upon preceding sections, this section systematically analyzes the concrete implementation and optimization strategies for reasoning within the RAG paradigm. In contrast to existing surveys that predominantly focus on post-training methodologies or isolated LLM reasoning mechanisms, our analysis maintains a dedicated focus on the synergistic integration of RAG with reasoning examining their co-adaptive implementations through a structural lens.

![Refer to caption](extracted/6386523/Images/implementation.png)

Figure 6. Implementation and optimization of the synergy between RAG and Reasoning

### 5.1. Reasoning Process

#### 5.1.1. LLM CoT

Integrating Chain-of-Thought (CoT) reasoning with LLMs is key to combining RAG with complex reasoning tasks. Research shows CoT enhances RAG systems by explicitly guiding multi-step reasoning and dynamically incorporating external knowledge. For example, ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)) uses a ”Self-Inquiry → Knowledge Assimilation → Thought Accommodation” chain to align knowledge and reasoning: a knowledge assimilation agent merges external documents with LLM memory via operations like association and reflection, creating structured knowledge. Meanwhile, a reasoning adaptation agent refines inference chains from Self-Inquiry to ensure answers align with retrieved knowledge and address reasoning gaps. Similarly, Adaptive-RAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) alternates between CoT and retrieval, breaking down multi-hop reasoning into steps such as entity localization and document correlation, refining retrieval and generation based on prior results.

At the knowledge and reasoning level, O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) drives RAG through open-ended long-text reasoning, extending CoT beyond fixed triggers via coherent thought processes like problem decomposition. PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) explicitly uses CoT to produce executable multi-step plans, adjusting operations dynamically through a closed-loop ”plan-execute-feedback” cycle. Despite different implementations, these methods share two CoT strengths: breaking down complex problems into clear intermediate steps and guiding external knowledge selection through reasoning states. Studies show these approaches outperform traditional RAG in multi-hop QA and knowledge-intensive tasks by enhancing both LLMs’ reasoning and adaptability to external knowledge.

#### 5.1.2. Special Token Prediction

Recent advances active RAG also highlight special token prediction as a key method for dynamically linking external knowledge retrieval with multi-step reasoning (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)). By embedding domain- or action-specific tokens (e.g., ‘[Web-search]‘, ‘[Retrieve=Yes]‘, ‘¡begin\_of\_query¿‘) into LLM vocabularies, models can autonomously trigger tools or self-reflect during text generation. Frameworks like Self-RAG (Asai et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)) and SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) use dedicated tokens (‘Retrieve‘, ‘ISREL‘, ‘[RETRIEVE]‘) to manage retrieval activation, relevance checks, and output verification, turning static reasoning chains into conditional workflows. The innovation lies in predicting these tokens within generated sequences, segmenting tasks into retrieval initiation, document evaluation, and knowledge grounding phases.

Hybrid models such as Open-RAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) combine token control with mixture-of-experts (MoE) routing, sparsely activating experts aligned with token-predicted reasoning. Unlike traditional chain-of-thought or search tree methods, special token prediction offers finer control and interpretability by encoding decision logic explicitly in token sequences while maintaining end-to-end training. This approach also overcomes latency and inflexibility of preset retrieval schedules by enabling context-aware, on-demand tool use. For example, R1-Searcher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)) and Search-o1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) use token boundaries like ‘¡end\_of\_query¿‘ to coordinate retrieval pauses and resume generation after knowledge integration.

Together, these systems show that token-level prediction not only bridges reasoning and retrieval but also creates a scalable framework for tool-enhanced language agents, preserving generative fluency while enabling systematic external knowledge integration and procedural reasoning.

#### 5.1.3. Search-Driven Reasoning

Recent advancements in search-driven reasoning have significantly improved RAG frameworks by employing structured search strategies for dynamic information exploration and multi-step reasoning with external knowledge. Current approaches mainly follow three paradigms: tree-based search, MCTS, and reinforcement learning-optimized policy networks.

Tree-based methods organize reasoning hierarchically through structured path exploration. For example,StePO-Rec (Bi et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)) uses a multi-step tree-structured reasoning method that iteratively retrieves different outfit matching knowledge and user preferences at each node, ultimately achieving generative recommendations for complementary items. OmniThink (Xi et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95)) uses an information tree to expand topic analysis by generating subqueries that guide breadth-first or depth-first retrievals. DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) applies a binary tree search within a Markov decision process to explore parametric knowledge and retrieval paths in parallel, selecting optimal branches. DeepSolution’s (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)) bidirectional thinking tree alternates expanding solution and critique nodes with scoring for path pruning, aligning naturally with MCTS evaluation. These methods balance exploration efficiency with solution coverage through explicit tree structures.

MCTS enhances robustness by optimizing long-term decisions via simulation, evaluation, and backpropagation. CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) integrates MCTS with the UCB strategy to balance exploration and exploitation while estimating optimal subgoals through multi-step simulations. KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)) and MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)) generate candidate actions using policy models and combine reward models to globally assess logical forms, reducing local optima. ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) innovatively merges MCTS with procedural reward models (PRMs), interleaving retrieval and reasoning steps, and filtering high-reward paths to form a closed-loop ”reason-retrieve-reason” cycle. These methods probabilistically explore paths and use reinforcement learning feedback to improve global reasoning for complex tasks.

Reinforcement learning-optimized policy networks adaptively refine search strategies. LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)) replaces fixed search algorithms with reinforcement learning (e.g., IPO) to dynamically optimize query generation based on rewards like retrieval accuracy, implicitly learning optimal search patterns without explicit tree or graph structures, thus offering greater flexibility and scalability.

In summary, search-driven reasoning unites inference and retrieval through structured strategies, combining multi-path exploration, dynamic evaluation, and adaptive optimization to deliver interpretable, efficient solutions for knowledge-intensive tasks. Future work may focus on hybrid paradigms (e.g., integrating MCTS and reinforcement learning) and lightweight algorithms to balance performance with computational efficiency.

#### 5.1.4. Reasoning on Graph

Graph-structured reasoning offers a novel approach for multi-hop inference in RAG systems by explicitly modeling knowledge interaction paths through topology. Current methods fall into two categories: query-flow-oriented search graphs (e.g. FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51))) and knowledge-association-based expansion graphs (ToG-2.0 (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61))). FinSearch builds a directed acyclic graph (DAG) where nodes are atomic subqueries (e.g., stock prices, financial reports) and edges capture logical and temporal dependencies. A pre-planner breaks down queries into subquery sequences, using graph traversal to control information flow and dynamically adjust paths, such as backtracking when conflicts arise—substantially surpassing linear chain-of-thought methods in handling complex logic.

#### 5.1.5. External Solver

The integration of RAG and reasoning is also can be achieved by incorporating external solvers, where specialized solvers, such as the Alignment-Oriented LLM-based Retrieval Method (ARM), are employed to handle the reasoning component. The retrieval process for complex problems is formulated as a global optimization task, leveraging external solvers like mixed-integer programming (MIP) to achieve structural alignment and joint optimization of data objects. Specifically, ARM first decomposes user queries into keywords that match N-grams in the dataset through an information alignment module, generating an initial set of retrieval candidates via constrained decoding. Subsequently, in the structural alignment phase, the MIP solver performs global filtering on candidate objects based on a predefined objective function that maximizes both the relevance of retrieved objects to the query and their mutual compatibility. This ensures that the selected objects not only cover the requirements of the query but also form a coherent information chain through entity or inter-table linkages. Finally, the self-verification mechanism of the LLM, combined with a beam search-based aggregation strategy, dynamically refines and consolidates multiple candidate sets, ultimately producing a retrieval collection that satisfies both semantic matching and the structural organization of the data.

ToG-2.0 achieves multi-hop expansion by integrating knowledge graphs with documents, starting from an initial entity and iteratively extending relevant entities and relations (such as corporate ownership chains and technology dependency networks) via the Edge function. This process constructs structured triple paths while simultaneously retrieving and verifying document content. By tuning the width and depth parameters, the method emulates human reasoning: broadly exploring potential associations before deeply verifying high-confidence paths. FRAG (Gao et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)) dynamically adjusts retrieval strategies by predicting the hop range of reasoning paths based solely on the query text, thereby enhancing retrieval quality without requiring additional fine-tuning or invocation of large language models, enabling flexible and efficient retrieval optimization. FG-RAG (Hong et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)) further expands entity coverage in graph retrieval through context-aware entity expansion, providing richer background information. Combined with query-level fine-grained summary generation, FG-RAG transforms coarse-grained graph information into highly relevant detailed content, effectively improving the performance of query-focused summarization tasks.

Although differing in design from workflow-based methods, ToG-2.0 shares key advantages with other graph-structured approaches: explicitly modeling reasoning state dependencies, supporting dynamic path generation and optimization, and enabling closed-loop interaction between retrieval and reasoning. This effectively overcomes the limitations of traditional RAG in implicit relation inference and counterfactual analysis, thereby establishing an interpretable theoretical and practical framework for knowledge reasoning.

### 5.2. Reasoning Optimization

In the previous chapter, we focused on introducing several approaches to integrate reasoning with RAG. This chapter shifts attention to how to augment the reasoning capabilities, specifically including Prompt-Based, Tuning-Based, and RL-Based strategies.

#### 5.2.1. Prompt-Based

Prompt-Based optimization is a key approach to improving RAG and reasoning system performance by using carefully designed natural language prompts. These prompts break down complex reasoning tasks into manageable steps and guide LLMs to follow specific logical structures during generation. The main advantage is that control over reasoning flow is achieved solely through prompt design, without parameter fine-tuning or reinforcement learning, preserving the model’s generalization while enhancing task-specific results.

This approach has three main features. First, task structuring: prompts explicitly decompose and control reasoning chains via zero-shot or templated designs. Techniques like Co-STORM (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) and WriteHere (Xiong et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)) use role assignments, stage divisions, and operation-specific instructions to guide multi-step reasoning—such as proposal generation, knowledge retrieval, refinement, and validation—improving interpretability by representing intermediate steps clearly.

Second, result reliability is improved by standardizing outputs and reducing hallucinations. Strategies include requiring citation of retrieval results, enforcing specific output formats, and integrating reflection and calibration based on retrieved knowledge. Systems like FinSearch (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) and ActiveRAG (Xu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)) incorporate temporal weighting, deduplication, and domain rules through prompts, enhancing consistency and logical coherence, especially in complex domains.

Third, interactive adaptability allows dynamic prompt adjustments. Special tokens (e.g., `<Search>`, `[Web-search]`) enable models to trigger tools or revise queries in real time based on intermediate results. Methods such as Agentic Reasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) and PlanRAG (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) use context-sensitive prompts and feedback loops to refine reasoning paths dynamically, maintaining coherence and accuracy in multi-hop tasks and outperforming traditional RAG methods in complex, evolving scenarios.

In summary, prompt-based optimization offers an efficient, flexible, and reliable approach to enhancing RAG+Reasoning by emphasizing task structuring, result standardization, and interactive adaptability. Its non-intrusive and broadly applicable design has established it as a mainstream strategy for optimizing LLM reasoning and serves as a foundation for future hybrid methods integrating fine-tuning and reinforcement learning. By systematically optimizing reasoning without altering model parameters through semantic structures, dynamic feedback, and symbolic constraints, this paradigm effectively manages macro-level controls like task decomposition and knowledge integration while addressing key challenges such as generation consistency, logical coherence, and external knowledge alignment. This makes prompt-based optimization a lightweight yet powerful solution for complex reasoning tasks.

Table 1. Comparison of RL-based RAG with Reasoning Methods

| Method | Base Model | RL | Parameter | Supervision | Reward Function | Policy Strategy |
| --- | --- | --- | --- | --- | --- | --- |
| PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)) | Qwen2.5/Llama3.2 | GRPO | QLoRA | ORM | |  | | --- | | Dual rewards: | | 1. Retrieval fidelity (Rfidsubscript𝑅fidR\_{\text{fid}}italic\_R start\_POSTSUBSCRIPT fid end\_POSTSUBSCRIPT) | | 2. Response quality (Rqualsubscript𝑅qualR\_{\text{qual}}italic\_R start\_POSTSUBSCRIPT qual end\_POSTSUBSCRIPT) | | Combined: R=α⁢Rfid+β⁢Rqual𝑅𝛼subscript𝑅fid𝛽subscript𝑅qualR=\alpha R\_{\text{fid}}+\beta R\_{\text{qual}}italic\_R = italic\_α italic\_R start\_POSTSUBSCRIPT fid end\_POSTSUBSCRIPT + italic\_β italic\_R start\_POSTSUBSCRIPT qual end\_POSTSUBSCRIPT | | |  | | --- | | • Group-based advantage normalization | | • PPO-style clipped objective | | • KL regularization | |
| DeepResearcher (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)) | Qwen2.5-7B | GRPO | Full | ORM | |  | | --- | | Format compliance penalty (-1) | | + Answer F1 score | | |  | | --- | | • Reference policy constraints | | • KL divergence penalty | |
| ReSearch (Chen et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7)) | Qwen2.5-7B | GRPO | Full | ORM | |  | | --- | | Hybrid rewards: | | • Answer F1 (vs ground truth) | | • Format compliance check | | |  | | --- | | • GRPO with clip ratio 0.2 | | • Group advantage normalization (G=5) | | • β=0.001𝛽0.001\beta=0.001italic\_β = 0.001 KL penalty | |
| ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)) | Llama3.2-3B | GRPO | Full | ORM+PRM | |  | | --- | | • Answer correctness | | • Format compliance | | • Search diversity | | • Chunk matching | | • Retry behavior | | • Strategy compliance | | |  | | --- | | • Intra-group reward comparison | | • Noise-injected robustness training | | • KL constraints | |
| MMOA-RAG (Chen et al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13)) | Llama-3-8B | MAPPO | Full | ORM | |  | | --- | | Shared F1 reward + penalties: | | • Excessive sub-questions | | • Document ID errors | | • Answer verbosity | | |  | | --- | | • MAPPO actor-critic updates | | • Cosine learning rate scheduling | |
| DeepNote (Wang et al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)) | Qwen2.5/Llama3.1 | DPO | Full | ORM | |  | | --- | | Implicit preference modeling | | via likelihood contrast | | |  | | --- | | • Direct Preference Optimization | | • Preference gap maximization | |
| R1-Searcher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)) | Qwen2.5/Llama3.1 | Reinforce++ | Full | ORM | |  | | --- | | Two-stage rewards: | | 1. Retrieval count + format | | 2. F1 score + format penalty | | |  | | --- | | • RAG-based rollout | | • Retrieval-masked loss | |
| KBQA-O1 (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)) | Llama3/Qwen2.5/Gemma2 | MCTS | DoRA | ORM+PRM | |  | | --- | | Composite reward: | | • Stepwise policy model score | | • Final reward model score | | |  | | --- | | • MCTS trajectory optimization | | • Q-value backpropagation | |
| DeepRetrieval (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)) | Qwen2.5-3B | PPO | Full | ORM | |  | | --- | | Task metrics: | | • Recall@k/NDCG | | • Syntax validity | | |  | | --- | | • GAE advantage estimation | | • Distributed HybridFlow | |
| LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)) | Llama3-8B/Gemma-9B | IPO | Full | PRM | |  | | --- | | Average Precision (AP) | | of retrieved documents | | |  | | --- | | • Identity Policy Optimization | | • Context distillation | |
| SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) | Flan-T5-L/Llama2-7B | PPO | Full/LoRA | ORM | |  | | --- | | Action-specific: | | • EM+F1 for answers | | • Cost penalty for retrievals | | |  | | --- | | • On-policy sampling | | • PPO updates | |
| ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) | LLaMA3.1-8B | MCTS | LoRA | ORM+PRM | |  | | --- | | Monte Carlo step scoring | | + TD look-ahead | | |  | | --- | | • Iterative preference optimization | | • KTO loss | |
| DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) | Qwen2.5-7B/Llama3.1-8B | Hybrid | Full | ORM+PRM | |  | | --- | | Cost-aware accuracy: | | R=−C⁢(o)×T⁢(st)𝑅𝐶𝑜𝑇subscript𝑠𝑡R=-C(o)\times T(s\_{t})italic\_R = - italic\_C ( italic\_o ) × italic\_T ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) | | C⁢(o)𝐶𝑜C(o)italic\_C ( italic\_o ): Answer correctness | | T⁢(st)𝑇subscript𝑠𝑡T(s\_{t})italic\_T ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ): Total retrieval cost | | |  | | --- | | • Imitation + contrastive learning | | • PPO-like calibration | |
| RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)) | LLaMA3.1-8B | Hybrid | LoRA | PRM | |  | | --- | | Triple criteria: | | • Sufficiency | | • Utility | | • Redundancy | | |  | | --- | | • SFT + DPO | | • PRM-guided selection | |
| CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) | Skywork-Llama3.1-8B | MCTS | LoRA | PRM | |  | | --- | | Critic-estimated rewards: | | • Stepwise correctness | | • Global impact | | |  | | --- | | • MCTS simulation | | • Pairwise ranking loss | |

1ORM: Outcome-based Reward Model; PRM: Process-based Reward Model.
2Full: Full parameter tuning.

#### 5.2.2. Tuning-Based

The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-of-thought mechanism within LLMs. Current research mainly targets three goals: *retrieval pathway optimization*, *structured generation enhancement*, and *collaborative training with external modules*.

For retrieval pathway optimization, methods like CoRAG (Wang et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)) and DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model’s ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.

To improve structured generation, MCTS-KBQA (Xiong et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98))and Self-RAG (Asai et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)) fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances self-supervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder (Yan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) and Open-RAG (Islam et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA (Dettmers et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib18)) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.

In collaborative optimization with external modules, AdaptiveRAG (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) and CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) apply parameter isolation to balance generality and adaptability. AdaptiveRAG fine-tunes a lightweight classifier to select retrieval strategies dynamically. CR-Planner introduces a Critic model trained with contrastive loss on MCTS trajectory data to assess the long-term value of reasoning actions, prioritizing efficient solutions in tasks like mathematical reasoning.

Together, these tuning strategies restructure the parameter space to internalize retrieval-reasoning interactions effectively, enhancing the model’s ability to solve complex problems while ensuring computational efficiency and broad applicability across domains.

#### 5.2.3. RL-Based

As shown in Table [1](https://arxiv.org/html/2504.15909v2#S5.T1 "Table 1 ‣ 5.2.1. Prompt-Based ‣ 5.2. Reasoning Optimization ‣ 5. Implementation and Optimization ‣ Synergizing RAG and Reasoning: A Systematic Review"), Reinforcement learning (RL) has recently become pivotal for tackling long-chain reasoning in modern inference models and optimizing RAG combined with reasoning tasks. Central to these advances is the use of dynamic reward mechanisms that guide LLMs to balance knowledge retrieval and logical reasoning adaptively. RL optimization objectives generally fall into two categories: outcome-based reward modeling (ORM) and process-based reward modeling (PRM), with some hybrid approaches blending both to balance global goals and local optimizations.

The ORM paradigm focuses solely on the quality of the final output and its adherence to standards. For example, R1-Searcher (Song et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)) employs a two-stage Reinforce++ (Hu, [2025](https://arxiv.org/html/2504.15909v2#bib.bib36)) training where rewards in the first stage depend on correct retrieval calls and special token generation, while the second stage directly optimizes the F1 score of answers. This encourages the model to develop strategies maximizing knowledge integration, reducing hallucinations, and enhancing accuracy in multi-hop QA beyond traditional RAG methods. Similarly, KBQA-O1  (Luo et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59))uses MCTS with a policy network for candidate reasoning paths and a reward model evaluating logical consistency, effectively balancing exploration and exploitation in knowledge base QA.

Conversely, PRM emphasizes detailed supervision of intermediate reasoning steps. LeReT (Hsu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)) uses the Identity Policy Optimization (IPO) algorithm, optimizing query quality by rewarding average precision (AP) of retrieved documents, boosting retrieval recall and overall multi-hop task performance. ReARTeR (Sun et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) extends this with a step-level binary reward model, combining Monte Carlo scoring and temporal difference (TD) methods to evaluate reasoning paths proactively, reducing logical errors and redundant retrievals, and improving accuracy on benchmarks like HotpotQA.

Moreover, influenced by DeepSeek-R1, GRPO (Shao et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib70)) is also gradually being applied in scenarios combining RAG and Reasoning. GRPO is a variant of the Proximal Policy Optimization (PPO) reinforcement learning algorithm that abandons the critic model and instead estimates the baseline from group scores, significantly reducing training resources. For example, ReZero (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)) uses GRPO to introduce a ”retry” mechanism for LLMs, incentivizing LLMs to keep trying after an initial search failure by rewarding retry search queries. This mechanism simulates the human strategy of ”if at first you don’t succeed, try again” in information retrieval. PORAG (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)), based on GRPO, directly optimizes retrieval quality, contextual relevance, and generation coherence through a dual reward mechanism (retrieval fidelity and response quality).

Hybrid methods merge ORM and PRM to optimize both final outcomes and intermediate steps via composite rewards. SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) applies Proximal Policy Optimization (PPO), combining answer-level F1 rewards with penalties for excessive retrievals, balancing knowledge completeness and efficiency. RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97))advances this with multidimensional process rewards (sufficiency, utility, redundancy) and techniques like contrastive loss and Best-of-N sampling to promote efficient search decisions, even zero-shot. These hybrid strategies markedly lower retrieval costs while sustaining accuracy in complex tasks.

In addition, we can also observe that in current RL-based methods, academia focuses more on exploration with small-scale LLMs (¡8B), among which the Qwen and Llama series are the most widely used. Overall, RL provides a flexible, scalable framework for integrating RAG and reasoning. ORM guides the discovery of globally optimal strategies, PRM enhances reasoning robustness via local refinements, and their combination addresses modular system limits. Future work may explore collaborative rewards in multi-agent settings, offline RL based on world models, and hierarchical reward decomposition for open-domain applications.

## 6. Downstream Tasks and Evaluation

While previous chapters focused on methodologies and advances in RAG combined with reasoning, this chapter shifts to tasks and evaluation. It provides a comprehensive overview and analysis of existing tasks, datasets, their current status, and emerging trends. By reviewing these resources, we highlight the landscape’s gaps and limitations in current evaluation methods. The chapter also explores key challenges in assessment frameworks, identifying shortcomings and suggesting potential improvements.

![Refer to caption](extracted/6386523/Images/sankey_diagram.png)

Figure 7. The current downstream tasks and datasets related to the combination of RAG and Reasoning show that multi-hop question answering tasks still dominate. Correspondingly, HotpotQA, 2WikiMultihopQA, and MuSiQue remain the most commonly used evaluation datasets.

### 6.1. Knowledge-Intensive Tasks

In the evaluation for RAG systems, knowledge-intensive question answering (QA) remains the primary focus (Figure [7](https://arxiv.org/html/2504.15909v2#S6.F7 "Figure 7 ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review")). As LLMs improve in semantic understanding and reasoning, benchmarks have expanded to cover tasks from simple fact retrieval to complex multi-step reasoning. However, evaluation methods specifically designed for RAG lag behind due to the dual challenge of assessing both retrieval-generation coherence and adaptability to dynamic knowledge bases. For example, multi-hop QA requires integrating dispersed knowledge through multi-stage retrieval while verifying logical consistency between answers and retrieval paths. This complexity increases dataset construction costs compared to purely generative tasks, keeping research centered on knowledge-intensive QA subcategories such as open-domain QA, knowledge-base QA, and multi-hop QA.

Commonly used datasets include Natural Questions (NQ) (Kwiatkowski et al., [2019](https://arxiv.org/html/2504.15909v2#bib.bib48)) for single-hop factual queries, HotpotQA, 2WikiMultiHopQA (Ho et al., [2020](https://arxiv.org/html/2504.15909v2#bib.bib32)) and Musique (Trivedi et al., [2022b](https://arxiv.org/html/2504.15909v2#bib.bib80)) for multi-hop QA. These benchmarks are mostly based on Wikipedia and fail to reflect the RAG demands and corresponding complexity in real-world scenarios. Some efforts have pushed evaluation boundaries, like CRUD-RAG’s (Lyu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib60)) operational metrics and DomainRAG’s (Wang et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib87)) domain-specific evaluations, but high costs and metric-task interdependencies limit progress. As a result, knowledge-intensive QA remains central for testing RAG robustness and practicality, highlighting a critical bottleneck: the need for innovative frameworks that balance retrieval flexibility and controlled generation to support new developments like Agentic RAG .Overall, many evaluation benchmarks are lagging behind rapid RAG+Reasoning advances, especially as LLMs grow more powerful. Specifically, the current evaluation of RAG faces the following challenges.

##### Limited Challenge

With improving LLM capabilities, many knowledge-based questions are no longer difficult, as they can be answered without external retrieval. Current multi-hop reasoning datasets, often built from artificial templates, offer limited challenge. There is an urgent need for more complex datasets reflecting real-world scenarios and practical use.

##### Lack of Specificity

Existing evaluation tasks are still predominantly focused on factual assessment and knowledge retrieval, lacking evaluations that probe deeper analytical thinking. This constraint limits the ability to measure a model’s capacity for profound reasoning and cognitive depth.

##### Task Uniformity

The majority of benchmarks are overly dependent on QA tasks, focusing on reactive, question-and-answer-based interactions. There is a pressing need to introduce tasks aligned with real-world applications, such as active information retrieval tasks based on personal knowledge or proactive knowledge discovery.

##### Insufficient Dimensions

Evaluations are primarily end-to-end, focusing solely on final outcomes. However, with the introduction of reasoning processes, RAG+Reasoning systems have become iterative, multi-step frameworks. Current evaluations are unable to assess intermediate reasoning steps or retrieval chains effectively. The absence of step-by-step supervision data limits both research and training of related methods. Furthermore, current evaluation methodologies lack comprehensive assessments of system performance trade-offs, such as computational cost and efficiency, which are critical for practical deployment.

This emergent landscape necessitates the creation of a new generation of evaluation frameworks that can address these shortcomings. Such frameworks must not only ensure the adaptability of retrieval and the controllability of generation but also integrate intermediate reasoning evaluation and efficiency metrics, paving the way for the development of more robust and efficient RAG systems suited to diverse real-world applications.

### 6.2. New Tasks on RAG+Reasoning

Recently, combining RAG with reasoning has significantly improved models’ ability to tackle more realistic and challenging tasks, raising the standards for evaluation methods. This subsection examines emerging tasks that assess their combined strengths, related tasks and datasets are shown in Table [2](https://arxiv.org/html/2504.15909v2#S6.T2 "Table 2 ‣ 6.2. New Tasks on RAG+Reasoning ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review"). Here, ”emerging” refers not to entirely new tasks but to those with unprecedented complexity and demands. These include Deep Research tasks requiring multi-layered information integration and reasoning; PhD (Expert)-Level Complex Reasoning tasks targeting advanced scenario reasoning; and critical; domain-specific decision support tasks like medical diagnosis and legal analysis. Such tasks demand not only external knowledge retrieval but also logical consistency, coherence, and depth in reasoning.

Table 2. Tasks and Datasets under the New Trend of RAG Combined with Reasoning

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Task Type | Sub-Task | Dataset | Description | Scale | Construction By | Evaluation | Paper |
| Deep Research | Deep Research | Agentic Reasoning Deep Research (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) | PHD-level dataset covering finance, medicine, and law. | 15-30 domains | PhD Experts | Expert pass rate | (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) |
| Report Generation | WildSeek (Jiang et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib45)) | Info-seeking task–goal pairs for document generation. | 100 samples | Rules/LLM/Manual | LLM | (Xiong et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)) |
| Report Generation | TELL ME A STORY (Huot et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib38)) | fiction writing evaluation dataset: detailed prompts and long-form narratives. | 230 samples | Manual | LLM | (Xiong et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)) |
| Peer Review | Review-5k (Weng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92)) | ICLR 2024 peer review dataset: paper metadata and structured reviewer feedback. | 4,991 papers | OpenReview/arXiv | MSE/MAE/Acc | (Weng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92)) |
| Report Generation | Research-14k (Weng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92)) | 2022–2024 Accepted ML papers: outlines, full texts, and cited abstracts. | 14,911 papers | Semantic Scholar + arXiv | Simulated review scores | (Weng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92)) |
| Report Generation | SolutionBench (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)) | Engineering benchmark: constrained solutions across 8 real-world domains. | 1,050 datapoints | Manual/LLM extraction | Analytical/ Technical scores | (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)) |
| Mathematics &  Reasoning | Math Reasoning | GPQA (Rein et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib68)) | PHD-level MCQs in physics, chemistry, and biology. | 744 sets | PhD Experts | Accuracy | (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) |
| Math Reasoning | MATH500 (Lightman et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib56)) | 500 math problems from the MATH test set. | 500 problems | Public repos | Pass@K | (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) |
| Programming | LiveCodeBench (Jain et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib41)) | Programming benchmark with easy, medium, and hard problems. | 1,055 problems | Competition platforms | Pass@K | (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) |
| Programming | USACO (Shi et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib71)) | USA Computing Olympiad problems, testing algorithms and coding. | 307 problems | USA Computing Olympiad | Pass@K | (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) |
| Math Reasoning | TheoremQA-Math (Hongjin et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib34)) | BRIGHT subset: theorem-based math problems. | 206 problems | STEM datasets | Accuracy | (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) |
| Programming | Gorilla (Patil et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib65)) | API-aware code generation from HuggingFace, Torch Hub, TensorFlow Hub docs. | 1,600 APIs | Manual | AST matching | (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)) |
| Math Reasoning | OlympiadBench (He et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib30)) | Olympiad-level math competition problems. | 1,000 problems | Competitions | Accuracy/F1 | (Zhu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110)) |
| Complex Reasoning | ComplexWebQA (Talmor and Berant, [2018](https://arxiv.org/html/2504.15909v2#bib.bib77)) | Multi-step reasoning over web queries with cross-document integration. | 34,689 queries | Web snippets | Accuracy | (Hu et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37)) |
| Demanding  Retrieval | Domain Retrieval | StackEcon & StackBio (Hongjin et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib34)) | Biology and economics StackExchange questions for complex retrieval. | 206 queries | StackExchange | nDCG@K | (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) |
| Active Retrieval | AR-Bench (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) | Active retrieval benchmark with four sub-tasks. | 8k/sub-task | Synthetic | Accuracy | (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) |
| Real-time | TAQA (Zhao et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib105)) | QA dataset with time-evolving answers. | 10K-100K rows | Human-curated | LLM | (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) |
| Real-time | FreshQA (Vu et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib81)) | Dynamic fact QA benchmark with evolving answers | 600 samples | Mixed sources | LLM | (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) |
| Domain Retrieval | PubMed (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)) | PICO-based medical search dataset linking reviews to PubMed. | 21k+ samples | Systematic reviews | Recall@K | (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)) |
| Domain Retrieval | Trial search (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)) | PICO-based clinical trial search linked to ClinicalTrials.gov. | 7k+ samples | Manually | Recall@K | (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)) |
| Domain Retrieval | FinSearchBench-24 (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) | Financial retrieval benchmark covering stocks, rates, policy, trends. | 1,500 queries | Manually | Accuracy | (Li et al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) |
| Decision &  QA | Business | DQA (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) | Decision QA benchmark with business scenarios in enterprise settings. | 301 pairs | video games | Accuracy | (Lee et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) |
| Medical | CMB-Clin (Wang et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib88)) | CMB subset for clinical diagnosis reasoning in Chinese medical cases. | 74 cases | Textbooks/diagnostic materials | LLM/Expert | (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)) |
| Medical | MM-Cases (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)) | Medicine cases generated by GPT-4o-mini, verified by doctors. | 609 cases | LLM/doctor-reviewed | LLM/Expert | (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)) |
| Medical | TCM-Cases (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)) | TCM patient cases generated by GPT-4o-mini, verified by doctors. | 130 cases | LLM/doctor-reviewed | LLM/Expert | (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)) |

#### 6.2.1. Deep Research

From the perspective of integrating RAG and reasoning, Deep Research tasks exemplify complex downstream applications. They require models to handle open-ended retrieval, produce long-form, structured text, and synthesize multi-source information through deep reasoning. This section analyzes their key features, evaluation datasets, and metrics.

At the core of Deep Research tasks lies the mission of addressing complex informational queries. These tasks are distinguished by several key attributes:

First, dynamic interactivity is essential. Models engage in iterative dialogue to uncover latent user needs or ”unknown unknowns”. For example, the Co-Storm (Jiang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) framework enables collaboration with multiple language model agents to explore information gradually, easing user cognitive load and capturing unmet needs more accurately.

Second, integrating information from multiple sources is crucial. Models must consolidate diverse data to provide comprehensive coverage. For instance, uses dynamic mind maps to structure knowledge and produce cohesive reports, ensuring accuracy and completeness.

Third, expert-level accuracy is required. Many tasks demand domain expertise, expecting models to perform like human specialists. The Agentic Reasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) framework illustrates this with high-stakes scenarios like medical treatment design or legal analysis, where outputs are judged on correctness, depth, and coherence.

Fourth, multi-modal reasoning is often necessary. Deep Research tasks involve varied data types—text, code, knowledge graphs—and dynamic tool use such as web searches or code execution to enhance reasoning.

Finally, handling multiple real-world constraints is vital. Tasks may require generating practical solutions under specific conditions, like designing hospitals in challenging environments with factors like heavy rainfall and seismic activity, as seen in the DeepSolution framework. This ensures outputs are feasible and relevant.

To ensure the diversity and complexity of Deep Research tasks, their evaluation relies on datasets drawn from multiple domains. A few notable examples include:

WildSeek Dataset (Jiang et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib45)): This dataset is constructed from real-world user information-seeking scenarios and comprises 100 data points covering 24 fields, including economics, computer science, and law. Each data point is characterized by a topic, user goal, and domain label. For example: ”Domain: Economics; Topic: Development of a Shared Trading Currency; Goal: Investigate how a new shared currency could eliminate transaction costs”. WildSeek effectively evaluates models’ competence in dynamic interaction and multi-source information integration.

GAIA (Mialon et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib63)). The GAIA Benchmark, developed jointly by Meta AI, Hugging Face, and others, is a comprehensive evaluation framework designed to assess general AI assistants’ ability to handle real-world problems. It features 466 carefully crafted tasks spanning language reasoning, visual perception, multi-agent collaboration, and adaptability, focusing on key skills like reasoning, multimodal processing, web browsing, and tool use. GAIA measures performance across dimensions such as task execution, adaptability, collaboration, generalization, and real-world reasoning with metrics like completion rate, response quality, efficiency, and robustness. Unlike traditional benchmarks, it emphasizes robustness and reliability in everyday scenarios, supports zero-shot evaluation, prevents data contamination, and is widely used in research and industry to guide AI development.

SolutionBench (Li et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)): This dataset spans eight engineering domains, including environmental, mining, and transportation engineering. Each instance presents a complex engineering problem with specific constraints. For example: ”Design a safe and efficient hospital construction plan in a region with 3000mm annual rainfall, expansive soils, and frequent seismic activity.”\* SolutionBench evaluates models’ ability to address multi-constraint problems and integrate specialized knowledge effectively.

The current evaluation system for DeepResearch faces the dual challenges of scarce specialized testing tasks and the difficulty of assessing complex, lengthy reports: On one hand, existing benchmark tests only cover basic capabilities and lack systematic evaluation standards in specialized scenarios like business analysis and policy assessment; on the other hand, the multimodal integration, logical chain verification, and domain adaptability testing of long reports pose technical bottlenecks for traditional assessment methods, necessitating the development of new evaluation tools that integrate logic graphs, dynamic scenario simulation, and domain knowledge bases.

In the future, the evaluation system will evolve into a multidimensional framework, including the construction of a three-level indicator matrix covering basic capabilities, reasoning levels, and application value. Overcoming these evaluation bottlenecks requires both technological innovation and joint standard-building efforts. This concerns not only the reliability validation of intelligent research tools but also the reshaping of research evaluation paradigms and industrial application boundaries.

#### 6.2.2. PhD (Expert)-Level Complex Reasoning

The integration of RAG with advanced reasoning has become essential for tackling expert-level, complex cognitive tasks, particularly at the PhD level. These tasks, including competitive programming, theorem-driven proof reasoning, and cross-disciplinary knowledge retrieval, require multi-layered logical inference and precise coordination between dynamic retrieval and domain-specific knowledge. PhD-level reasoning differs from standard evaluations across three dimensions: knowledge intensity, procedural rigor, and domain specificity. Knowledge intensity demands dynamic access to deep, specialized knowledge, such as analyzing dynamic programming time complexity or applying algebraic topology theorems—needs that surpass general corpora and call for domain-specific knowledge graphs and retrieval methods. Procedural rigor involves mathematical precision in multi-step proofs, requiring logical consistency in symbolic manipulation, theorem use, and counterexample refutation, as seen in international math competitions. Domain specificity reflects tailored reasoning methods, e.g., handling synchronization in concurrent programming or employing tensor calculus in quantum field theory.

Evaluation systems for such tasks are inherently multi-layered and multimodal. The USACO Benchmark (Shi et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib72)) offers a graduated difficulty scale for programming reasoning, testing both correctness and algorithmic constraints like time complexity. TheoremQA-Math (Chen et al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib10)) links formalized math problems to theorem libraries, demanding verifiable mappings between theorem applications and calculations. Cross-disciplinary datasets like StackBio and StackEcon (Li et al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib54)) assess models’ ability to extract critical knowledge from dense, domain-rich documents, serving as strong tests for domain-oriented retrieval accuracy.

Modern evaluation surpasses traditional end-to-end tests by combining process and outcome validation. Frameworks like CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) use dual models—a Sub-Goal Critic to score reasoning chains and an Execution Critic to evaluate retrieval—allowing fine-grained step monitoring. For example, in dynamic programming, key steps like formulating state transitions and retrieving boundary conditions receive targeted feedback. Similarly, Search-O1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) quantifies knowledge completeness by tracking uncertainty indicators (e.g., tentative language), measuring confidence and accuracy. Outcome validation maintains strict correctness benchmarks in programming and combines metrics like F1 scores with expert review in open-domain scientific QA to ensure precise understanding of domain-specific terms.

### 6.3. Challenges and Future Directions

#### 6.3.1. Complex Domain Tasks

Recent advances in RAG have provided novel solutions for more complex tasks in professional domains. These downstream tasks transcend the limitations of traditional question-answering models that rely solely on simple retrieval-generation patterns, involving challenges such as real-time information acquisition, integration of domain expertise, and dynamic decision-making support. The nature of these tasks can be characterized along three interrelated dimensions: (1) temporal dynamics, emphasizing the rapid changes in data and reasoning environment; (2) domain specificity, focusing on deep integration of industry knowledge and structured data; and (3) reasoning chain complexity, reflecting requirements for multi-stage reasoning and fine-grained decomposition of queries.

To rigorously evaluate such systems, innovative benchmarking approaches have been proposed. The FinSearchBench-24 dataset, for example, encompasses five months of market data variations, integrating multi-variable interactions across stock, policy, and industrial sectors, and includes over 1,500 multiple-choice questions, thereby surpassing the constraints of traditional static benchmarks. The evaluation adopts a hierarchical and quantitative methodology: the foundational level measures model accuracy and response latency; the intermediate layer assesses the temporal sensitivity of information relevance and the contribution of retrieval mechanisms to reasoning outcomes; and the advanced layer employs ablation studies to highlight performance variances under dynamic temporal decay. This multifaceted evaluation not only differentiates surface-level retrieval capabilities but also rigorously measures the synergy between reasoning quality and temporal context, furnishing theoretical and practical foundations for long-term stability and predictive accuracy in complex domain systems.

Experimental findings further reveal that establishing long-term evaluation protocols with temporal weighting functions is indispensable for adapting to realistic dynamic environments. Nonlinear declines in decision accuracy, observed when extending relevance windows from 72 to 168 hours, emphasize the importance of factoring temporal decay into assessment frameworks. Future work should extend these evaluation protocols to high-stakes domains such as medical diagnostics and legal consultation, where the standardization of interpretability metrics will critically support the evolution of RAG+ reasoning systems toward robust and trustworthy decision-assistance platforms.

![Refer to caption](extracted/6386523/Images/cost.png)

Figure 8. From LLM to RAG and then to RAG+Reasoning, performance improvement comes with additional cost.

#### 6.3.2. Decision Support and Active Retrieval

The expansion of RAG+Reasoning frameworks into specialized tasks has fostered two complementary research paradigms: decision optimization and active retrieval. In the decision optimization category, systems must leverage heterogeneous structured data, rule bases, and objective functions to formulate optimal strategies. Representative systems like PlanRAG formalize Decision Question Answering (Decision QA) tasks targeting enterprise-level scenarios including supply chain optimization, industrial resource allocation, and market price regulation. These tasks require planning multimodal reasoning paths where models iteratively retrieve data from relational and graph databases, integrate intricate business rules, and iteratively refine decision-making paths through replanning mechanisms. To evaluate such capabilities, the Decision QA (DQA) benchmark creates dual database versions (MySQL and Neo4j) derived from economic systems in strategy games, assessing cross-structured generalization. The evaluation consists of a three-tier framework: the core tier measures answer accuracy; the intermediate layer diagnoses error types to identify system bottlenecks; and the foundational tier focuses on retrieval efficiency and the impact of replanning frequency. This structured evaluation framework not only tracks performance but also offers actionable insights for system refinement.

Conversely, the active retrieval evaluation addresses the challenge of dynamically determining when and how to invoke retrieval under complex multimodal contexts. Unlike rigid traditional RAG systems, UAR applies lightweight classifiers for fast, accurate triggers, improving performance in time-sensitive or creative tasks. Tested on AR-Bench, it combines binary trigger accuracy with GPT assessments, exact matches, and human reviews, boosting adaptability across diverse contexts.

Emerging trends in these evaluation paradigms indicate a shift from static, rule-based frameworks to dynamic system simulations, as exemplified by DQA’s use of game engine-generated datasets to simulate realistic environments. Similarly, active retrieval tasks progress from simple retrieval trigger decisions toward collaborative multi-criteria decision-making. Evaluation methodologies are concurrently evolving from singular performance metrics to multidimensional matrices comprising core effectiveness, diagnostic error distributions, and economic cost measures.

## 7. Cost and Risk

Integrating reasoning into RAG systems is neither effortless nor purely beneficial. Recent trends have exaggerated its advantages while downplaying the costs and risks. This trade-off between performance and cost is crucial. This section examines the expenses and misuse risks linked to adding reasoning to RAG systems. As shown in Figure [8](https://arxiv.org/html/2504.15909v2#S6.F8 "Figure 8 ‣ 6.3.1. Complex Domain Tasks ‣ 6.3. Challenges and Future Directions ‣ 6. Downstream Tasks and Evaluation ‣ Synergizing RAG and Reasoning: A Systematic Review"), the cost of moving from LLM to RAG, then to RAG + Reasoning, incurs an inevitable ”invisible tax”. Though often hidden by performance gains, this cost is vital in assessing these methods’ overall practicality and efficiency.

The shift from LLM to RAG moves from simplicity to enhanced knowledge handling by incorporating external information. A basic LLM provides direct, efficient answers with low latency and token use but is limited to pre-trained knowledge, restricting complex or up-to-date queries. RAG overcomes this by adding a vector database for external retrieval, vastly expanding response scope and reliability. However, this requires substantial data processing, storage, and introduces higher latency and token costs due to data chunking, encoding, indexing, and retrieval overhead.

Advancing from RAG to RAG + Reasoning adds multi-step reasoning capabilities, enabling complex task handling, autonomous decisions, and more context-aware responses through intricate reasoning. This comes at the expense of increased delays, token consumption, processing demands, and greater complexity in system integration and maintenance. The reasoning layer’s autonomy also brings opaqueness, unpredictability, and heightened security and reliability risks. These challenges highlight the necessity of carefully balancing effectiveness against costs when adopting RAG + Reasoning in real-world applications.

![Refer to caption](extracted/6386523/Images/cost_diagram.png)

Figure 9. Cost quadrant diagram of retrieval and reasoning requirements

### 7.1. Cost Trade-off in RAG+Reasoning

Figure [9](https://arxiv.org/html/2504.15909v2#S7.F9 "Figure 9 ‣ 7. Cost and Risk ‣ Synergizing RAG and Reasoning: A Systematic Review") illustrates typical works combining RAG and Reasoning, showing retrieval and reasoning demands alongside token consumption. While integrating dynamic knowledge retrieval with multi-step reasoning greatly improves accuracy in more complex tasks, the resulting systemic costs are often underestimated in research and practice. These costs grow non-linearly, causing serious efficiency bottlenecks in real-world use. The tradeoff between effectiveness and efficiency stems from RAG+Reasoning’s architecture: multi-stage task decoupling, dynamic path planning, and intermediate state preservation. These features improve reasoning quality but trigger cascading increases in computational resources, token usage, and reduced retrieval efficiency. This section explores these implicit tradeoffs from the angles of resource use, token consumption, and retrieval efficiency.

#### 7.1.1. Non-Linear Growth of Computational Resources

The RAG+Reasoning framework separates retrieval and reasoning into multiple stages, causing computational demands to grow non-linearly. Dynamic chain-of-reasoning methods execute multiple LLM generations and retrievals per inference, resulting in complexity far exceeding baseline models. Fixed-length reasoning chains trigger repeated retrieval and generation calls, increasing resource needs with task complexity. More advanced techniques like MCTS-guided methods add rounds of candidate path generation and evaluation, further multiplying runtime and memory usage on GPUs compared to linear methods. Even simpler multi-step planning tasks incur much higher overhead than single-stage retrieval models due to extra graph construction and analysis. While this resource intensity improves inference accuracy, it poses serious scalability challenges under limited resources as computational costs grow superlinearly with model size, retrieval chain length, and task complexity.

#### 7.1.2. Implicit Token Inflation

Multi-step reasoning frameworks inherently cause significant token inflation through iterative intermediate processes like thought chains, retrieved documents, and verification feedback. Active learning setups consolidate multiple intermediate results—retrieved documents, counterfactuals, multi-round validations—leading to token usage well beyond typical limits. Chain-based retrieval also generates token bloat due to exhaustive candidate path exploration. Iterative reasoning path selection, expansion, and evaluation add heavy token overhead in tasks needing deep reasoning chains involving extensive sequence generation and evaluation. Token usage grows exponentially with task complexity and increases further when intermediate reasoning favors depth or breadth. This inflation raises API costs and memory demands, especially in long-text generation like Deep Research  (Zheng et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)).

#### 7.1.3. Marginal Decline in Retrieval Efficiency

Dynamic retrieval improves knowledge precision but suffers diminishing efficiency as task complexity increases. Adaptive methods reduce retrievals for simple tasks but still require multiple iterations for complex ones, adding significant overhead compared to standard RAG. The tradeoff between retrieval quality and frequency further limits efficiency. High-accuracy retrieval methods incur heavy computational and time costs, negating their efficiency benefits. Even advanced retrieval-trigger optimizations can’t fully remove this overhead due to extra training and deployment costs (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)). This natural efficiency ceiling highlights ongoing challenges in balancing retrieval accuracy and resource use, especially in large, complex tasks.

#### 7.1.4. Toward a Cost Model Framework

Against this backdrop, the development of fine-grained cost models becomes a necessary precondition for balancing effectiveness and efficiency. Existing evaluation metrics, which often rely on single-task performance indicators (such as Exact Match or F1) or coarse-grained runtime statistics, lack the comprehensiveness to jointly model computational resources, token flow, and retrieval overhead. Consequently, they fail to quantify the true tradeoffs in reasoning mechanisms. For instance, while multi-hop reasoning may improve task accuracy, these improvements are frequently offset by exponential growth in token consumption and latency relative to baseline methods. A fine-grained cost model would enable researchers and practitioners to more accurately evaluate the real benefits of reasoning-centric frameworks while addressing the underexplored interplay between computational cost and task performance.

### 7.2. Potential Risk of Over-Thinking

In the process of developing deep thinking models, ”overthinking” poses a key risk to system efficiency and reliability (Fan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib20); Sui et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib75); Chen et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib11); He et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib31); Wang et al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib82); Cuadron et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib16)), and this issue is further amplified after combining with RAG. It appears as redundant reasoning steps, excessive validation of known conclusions, or unnecessarily broad retrieval scopes, wasting computational resources, increasing error propagation, and degrading performance. For example, in financial risk assessment, an LLM with RAG might retrieve multiple similar market reports and repeatedly verify the same economic indicators rather than focusing on core risks, leading to delayed decisions. This stems from an imbalance between reasoning and retrieval: after accessing external knowledge, the model can enter a ”self-validation loop,” repeatedly parsing overlapping or contradictory documents. The generation module, seeking reliability, may trigger further retrievals, creating a feedback loop that worsens inefficiency. This issue is critical in real-time systems like medical diagnosis, where over-retrieval of irrelevant literature can delay urgent decisions.

Case studies show the impact of overthinking (Sui et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib75)). In legal document interpretation, early reasoning errors can amplify through the retrieval-generation loop, causing retrieval along incorrect paths and yielding illogical conclusions. This error propagation is evident in systems like the Search-o1 (Li et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)), where flawed information extraction misguides subsequent reasoning. In industrial equipment manual interpretation, overextended reasoning with highly similar documents risks obscuring critical parameter differences, increasing procedural errors. These examples illustrate that overthinking not only hampers knowledge integration but also creates safety hazards in practical applications.

To mitigate these risks, researchers propose multiple optimization frameworks. ReaRAG (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) limits reasoning chain length and incorporates self-reflection to prune invalid branches. A simple and effective way is to use a two-stage filtering process, first narrowing documents by metadata, then validating fragment relevance, reducing redundant information—for instance, retrieving only relevant legal clauses rather than entire regulatory texts. The DeepSeek R1 (Guo et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib27)) applies reinforcement learning with distillation to penalize redundant steps, cutting repeated formula validation in math proofs by over 40%. These approaches transform open-ended reasoning into controlled, goal-directed processes, using methods like attention weight analysis to measure information gain or confidence functions to evaluate reasoning paths.

Current research balances constraints with model creativity. Knowledge graph-guided reasoning is tested in clinical trials to prioritize key medical features over exhaustive literature retrieval (Chen et al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)). Causal reasoning models aim to break error chains; for example, in financial forecasting, causal graphs restrict reasoning to logically relevant macroeconomic links. Adaptive stopping strategies adjust reasoning depth in customer service—simple queries use preset templates, complex issues activate multi-hop reasoning. These advances reshape retrieval-augmented reasoning, with the core challenge being to develop evaluation frameworks that avoid both ”cognitive stagnation” from excessive constraints and ”cognitive overload” from insufficient control.

Future progress will integrate cognitive science with computational modeling. By mimicking human ”intuition-verification” decision-making, LLMs could switch seamlessly between rapid response and deep reasoning. In high-risk fields like industrial fault diagnosis, such hybrid models can quickly propose contingency plans after initial retrieval while verifying their validity through deeper analysis. This layered approach reduces overthinking risks and offers a safe, controllable path for applying LLMs in critical industries.

## 8. Practical Guide

![Refer to caption](extracted/6386523/Images/practical_guide.png)

Figure 10. Practical guide to synergizing RAG and Reasoning

The combination of RAG and Reasoning is not a one-size-fits-all solution; it requires careful evaluation of each scenario’s unique needs. As a rapidly evolving and relatively new field, practical applications are still limited, making best practices hard to define. This chapter abstracts and summarizes the key traits of typical RAG+Reasoning application domains and offers practical guidelines for system design based on these features. It provides recommendations on leveraging RAG’s strengths with Reasoning, highlighting priorities, pitfalls to avoid, and current opportunities (Figure [10](https://arxiv.org/html/2504.15909v2#S8.F10 "Figure 10 ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review")). The goal is to promote wider adoption and effective use of this technology in diverse, complex real-world settings.

### 8.1. Domain characteristics

As illustrated in the left part of Figure [10](https://arxiv.org/html/2504.15909v2#S8.F10 "Figure 10 ‣ 8. Practical Guide ‣ Synergizing RAG and Reasoning: A Systematic Review"), we develop a seven-dimensional feature system based on the three core stages of RAG—query, retrieval, and generation—to systematically analyze challenges and adaptation needs across various industries. The query stage emphasizes the complexity of intent understanding and the demand for advanced reasoning, recognizing that industries differ in query abstraction and specificity; some require quickly capturing implicit, deep intentions, while others need complex reasoning. Effective preservation of original semantic meaning during understanding and reasoning is key to improving RAG performance. Retrieval focuses on the system’s adaptability to diverse and dynamic knowledge sources, which vary from rich multi-domain data to rapidly updating information; frequent updates and fragmented knowledge present challenges that demand effective integration to ensure consistent support for generation. The generation stage requires high-quality outputs, with strict control over hallucinations—especially critical in sensitive fields like healthcare and law—along with varying latency requirements for real-time or delayed responses. Explainability and traceability at this stage are essential for system credibility and serve as key evaluation metrics. This comprehensive framework reveals technical bottlenecks and guides improvements, and is applied to analyze four representative domains: finance, healthcare, law, and personal assistants.

#### 8.1.1. Finance

In the finance domain, user queries typically focus on structured needs like investment decisions and risk forecasting. While intent understanding is moderately complex, the system must perform advanced reasoning amid rapidly changing market conditions, relying heavily on external knowledge and frequent updates. For example, portfolio return forecasting integrates time series analysis, policy interpretation, and cross-market reasoning. Retrieval demands handling diverse data sources—real-time market data, annual reports, and regulatory filings—with update cycles often measured in minutes. During generation, strict latency and hallucination control are crucial, as outputs must include decision-making suggestions with full data traceability. Investment research reports, for instance, require annotated key indicators, their data sources, and computation logic to ensure transparency and regulatory compliance. High latency control and robust traceability are essential to maintain transparency and adherence to financial regulations.

#### 8.1.2. Healthcare

Healthcare queries involve complex medical semantic parsing, often with ambiguous terms or incomplete symptoms. For example, ”persistent chest pain with shortness of breath” requires multi-hop reasoning across cardiology, pulmonology, and emergency medicine. Retrieval must integrate electronic health records, medical imaging, and up-to-date clinical guidelines. In generation, hallucination tolerance is minimal—errors in drug dosages or protocols risk malpractice. Therefore, accuracy, timeliness, and explainability are paramount, with every decision step traceable and verifiable.

#### 8.1.3. Legal Services

Legal consultations often require interpreting statutes and citing cases, balancing precise legal terms with natural language nuances. Retrieval depends on structured, infrequently updated sources like case law databases and local regulations. Generation demands accuracy—for instance, drafting contract clauses must precisely cite specific statutes (e.g., Article 472 of the Civil Code) down to the paragraph level for traceability. Explainability is essential, with traceability usually above 95%, and probabilistic language avoided to comply with strict judicial documentation standards.

#### 8.1.4. Personal Assistants

This domain features diverse, dynamic user needs, including schedule management, real-time navigation, and open-domain conversations. Accurate intent disambiguation through contextual awareness is crucial. Retrieval integrates fragmented sources like user behavior logs, geolocation, and social media. Generation latency varies: weather updates require sub-second responses, while travel planning can tolerate 5+ seconds. Hallucination tolerance depends on context—creative outputs are acceptable for recipes but not for flight information, which demands full accuracy. This necessitates adaptive verification in the RAG system. Though intent complexity is lower than in healthcare or legal fields, the domain’s interaction diversity requires heavy reliance on external knowledge and dynamic balancing of latency and accuracy.

### 8.2. Do’s and Don’ts

Building on aforementioned domain characteristics, we further identify six common scenarios, and derive technical adaptation principles for each. This section outlines key optimization strategies (Do’s) and prohibitions (Don’ts) , to guide the co-design of RAG and reasoning.

#### 8.2.1. Structured Reasoning Scenarios

For scenarios requiring multi-step logical decomposition and structured knowledge dependency, such as *portfolio return prediction*, Chain-of-Thought (CoT) task decomposition and knowledge graph (KG)-driven graph reasoning approaches should be employed. Complex problems should be broken into verifiable sub-tasks, such as coupling market trend analysis with policy impact assessment, while leveraging knowledge graph constraints to ensure logical completeness and auditability. It is essential to incorporate a temporal validation layer to cross-check the consistency of timestamp-sensitive information (e.g., real-time market data or emergent regulatory policies) within a dynamic knowledge base. Approaches that exclude retrieval-based verification of salient features must be avoided, as they may lead to reasoning biases arising from the absence of structured knowledge anchors (e.g., critical indicators from financial statements). Furthermore, the reasoning space of LLMs should be constrained within domain-specific knowledge frameworks to prevent irrelevant or invalid deductions.

#### 8.2.2. Dynamic Demand-Responsive Scenarios

For scenarios characterized by rapidly shifting demands and user preference variability, such as *itinerary planning and multimodal interaction in personal assistant services*, a dynamic adaptation mechanism based on prompt engineering is recommended. By dynamically associating fragmented knowledge units (e.g., user behavior history and real-time traffic updates) with semantic templates and employing heuristic rules for search-space pruning (e.g., prioritizing locally updated information within the past 24 hours), the system can balance contextual adaptability with response speed. Model fine-tuning or reinforcement learning (RLHF/DPO)-based strategy updates should be avoided due to their lengthy iterative cycles and computational overhead, which cannot meet real-time responsiveness requirements, such as millisecond-grade reaction times for last-minute destination changes. Lightweight caching architectures should be implemented within the retrieval system, prioritizing frequently accessed knowledge fragments, such as operating hours of popular tourist attractions, to achieve an equilibrium between dynamism and stability.

#### 8.2.3. Deterministic Decision-Making Scenarios

In scenarios requiring a single, reliable conclusion, such as *clinical diagnosis generation in the healthcare domain*, a multi-level deterministic assurance system should be established. Time-validation layers can filter outdated knowledge (e.g., therapies no longer approved), while field-sensitive retrieval modules trigger predefined decision rules conforming to up-to-date clinical guidelines (e.g., those codified within the latest version of the International Classification of Diseases [ICD]). Knowledge graph path constraints should restrict the reasoning process to validated causal links within medical logic (e.g., linking symptom patterns to laboratory test results within corroborated diagnostic pathways), thereby minimizing the likelihood of deviations from standard protocols. Probabilistic exploration strategies that generate alternative hypotheses (e.g., speculative differential diagnoses for atypical pneumonia) should be strictly disallowed to avoid clinical misjudgments. Additionally, delegating decision-making authority to external classification models must be avoided to maintain end-to-end explainability and a clear causal link in the decision-making pipeline.

#### 8.2.4. Time-Sensitive Scenarios

In tasks highly sensitive to response delays, such as *real-time risk warnings and trading decisions in the financial sector*, heuristic rules should be employed to prioritize indexing of frequently queried knowledge units (e.g., volatility indices and liquidity indicators) at the top of the search hierarchy. Directed retrieval expansion strategies that preload potentially associated information (e.g., contractual clauses of derivative instruments tied to underlying assets) can further reduce latency in multi-turn interactions. Monte Carlo Tree Search (MCTS) and other sample-based algorithms are ill-suited for such scenarios due to the excessive computational complexity caused by branch expansion, rendering them infeasible within tight time constraints (e.g., milliseconds). Similarly, the invocation of complex mathematical solvers (e.g., numerical solutions for stochastic differential equations) can introduce uncontrollable delays and should be replaced with lightweight rule-based mechanisms (e.g., threshold-triggering mechanisms based on historical volatility ranges).

#### 8.2.5. Risk-Sensitive Scenarios

For scenarios with minimal tolerance for errors, such as *contract clause generation and citation of judicial interpretations in the legal sector*, a dual-layer defensive mechanism must be employed. A pre-action review layer should validate the compliance of generated content with statutory standards (e.g., ensuring consistency between liability clauses and Article 577 of the Civil Code), while a reliability validation layer performs cross-referencing validation across multiple sources (e.g., aligning Supreme Court precedents with regional court guidelines) to resolve potential conflicts. Retrieval systems must include version control modules to track and update legal references (e.g., automatically flagging repealed local statutes). Unconstrained reinforcement learning-based text generation methods must be avoided, as their exploratory nature risks violating the normative requirements of legal documents (e.g., generating presumptive liability terms unsupported by judicial interpretations). All decision-making actions must pass through deterministic rule engines to filter inadmissible outputs, and the system should never execute decision actions autonomously, such as generating legally binding arbitration notices without oversight.

#### 8.2.6. Complex Path Exploration Scenarios

In exploration tasks involving multiple possible trajectories, such as *differential diagnosis and therapeutic pathway optimization in medicine*, weighted ranking search algorithms should balance search depth and breadth. Knowledge graph topology can guide prioritization (e.g., standard treatment procedures for acute coronary syndrome), while Monte Carlo Tree Search can extend exploration into uncommon differential paths (e.g., rare genetic metabolic disorders). Dynamic pruning threshold functions should be designed (e.g., adjusting the scope of differential diagnosis based on patient history) to eliminate low-confidence hypotheses in real time, thereby controlling computational scale. Brute-force searching of all potential paths (e.g., concurrently testing hundreds of pathogens for nonspecific symptoms) should be avoided to prevent exponential computational scaling. Careful handling of specific token triggers during retrieval (e.g., avoiding spurious associations between ”fever” and unrelated oncological hyperthermia research) is critical to maintaining logical coherence in diagnostic reasoning.

### 8.3. Opportunity Points

Based on the Do’s and Don’ts of current technologies analyzed in the previous section, there remain numerous directions with substantial academic value and application potential that have yet to be fully explored. This section systematically discusses several promising opportunity points across three dimensions: *data and indexing*, *models and methodologies*, and *application services*.

#### 8.3.1. Data and Indexing

##### Cold-Hot Tiered Indexing and Dynamic Context Management

The challenge of managing massive and highly heterogeneous data resources lies in devising an effective *cold-hot tiered indexing* mechanism that prioritizes data according to their frequency of use and importance. Such a mechanism not only demands classification of data based on timeliness and access frequency but also requires integration with dynamic context management. This allows the system to intelligently retrieve the most relevant data according to the immediate context.

Moreover, a dynamically updated indexing mechanism can mitigate the loss of data timeliness, which often leads to deteriorated inference accuracy. By ensuring access to the most recent and task-appropriate data, this approach reduces redundancy and incorrect retrievals associated with static indexing. When combined with automated task scheduling and resource allocation strategies, fine-grained real-time inference support can be achieved, significantly enhancing the system’s overall efficiency.

##### Cross-Institution Knowledge Base Construction

The construction of cross-institution or cross-domain knowledge bases offers new opportunities for advancing RAG+Reasoning research. At the core of large-scale cross-institutional knowledge bases lies the optimization of data integration and sharing mechanisms. This entails addressing challenges such as data security and privacy while adopting standardized data interfaces or leveraging federated learning paradigms to enable multidimensional data integration.

Through semantic alignment across multiple sources, entity resolution, and concept abstraction, cross-institutional knowledge can be transformed into authoritative and richly contextualized knowledge bases. These enhanced repositories provide robust contextual support for reasoning tasks and can deliver deeper insights in areas such as healthcare, finance, and urban management.

##### Fine-Grained Layering and Confidence Grading

In scenarios where retrieval and reasoning operate synchronously, the *interpretability* and *reliability* of generated outcomes are paramount. Fine-grained layering of data and indices, along with confidence grading of retrieval results, enables the system to selectively use the most trustworthy and relevant subsets of data during different stages of reasoning. This approach fosters transparency and traceability in final decisions or generative outputs.

For instance, in medical diagnosis scenarios, confidence grading can initiate additional verification or expert review in high-risk cases. In the legal domain, confidence layering systematically presents key evidence and identifies sources of uncertainty, reducing reasoning vulnerabilities and minimizing the risk of erroneous conclusions caused by information ambiguity.

#### 8.3.2. Models and Methodologies

##### Event-Driven Active Retrieval

Traditional retrieval mechanisms are predominantly passive. However, *event-driven active retrieval* presents a promising exploration avenue. By monitoring critical events, such as the injection of new data, user interactions, or changes in external sensors, event-triggered retrieval and reasoning processes can be initiated to capture and respond to potential risks and opportunities in real time. Integrating methodologies such as sequence-based event detection or multitask-learning-based intent recognition can facilitate automatic determination of *when* and *how* to trigger retrieval actions. Iteratively optimizing these processes contributes to a more efficient and continuous reasoning loop.

##### Spatiotemporal-Aware Retrieval and Association

Many applications, such as natural disaster monitoring, traffic flow prediction, and inventory management in retail, exhibit strong dependencies on temporal and spatial dimensions. By incorporating spatiotemporal-aware algorithms, retrieval processes can prioritize or emphasize crucial documents according to constraints tied to time and space. This not only enhances timeliness but also improves the purposefulness and accuracy of reasoning.

Furthermore, modeling the evolution of events within spatiotemporal dimensions—when combined with semantic indexing and vector-based retrieval mechanisms in RAG—can enable more precise characterization and utilization of complex spatiotemporal dynamics during reasoning.

##### Multimodal Fusion in Retrieval and Reasoning

Multimodal data (e.g., text, images, audio, video, and sensor data) collectively constitute a richer contextual environment, offering critical cues for reasoning tasks. However, existing studies are often limited to the retrieval of single or a few data modalities. Advancing research on multimodal fusion and reasoning mechanisms under the RAG+Reasoning framework has the potential to greatly enhance the system’s capacity for addressing complex queries.

The research focus lies in constructing cross-modal representation learning and alignment methods, enabling unified representations of the same entities or events across different modalities. During retrieval, confidence scores for each modality can be integrated into a comprehensive ranking process, culminating in multimodal-informed joint decision-making during reasoning. This approach not only improves contextual understanding in complex tasks but also broadens the application scope of RAG technologies in scenarios such as expert systems and autonomous driving, where sensory integration and interpretation are critical.

##### Dynamic Risk Propagation Modeling and Management

The tight coupling of retrieval and reasoning with multi-stage decision-making inevitably introduces risk propagation issues. Misjudgments of high-risk or low-confidence documents during upstream retrieval are often inherited by downstream reasoning processes, amplifying uncertainties and increasing error margins. To address this, dynamic risk modeling should be embedded within retrieval workflows, enabling risk quantification, tracking, and management at multiple stages. When necessary, risk mitigation mechanisms or process rollbacks can be triggered, creating a closed-loop correction framework.

Incorporating strategies for analyzing and managing risk propagation is not only a technical challenge but also a matter of system deployment and standardization. In high-stakes domains such as healthcare and financial risk management, establishing comprehensive safety standards and compliance protocols will be crucial. These protocols should treat dynamic risk propagation management as a critical component of evaluating and iterating knowledge retrieval and reasoning systems.

#### 8.3.3. Application Services

##### Validation of Logical Chain Completeness

While RAG with Reasoning can provide partially interpretable reasoning outputs, verifying the completeness of logical chains remains a challenge. Future research could integrate formal verification or symbolic reasoning techniques to ensure consistency and completeness across key reasoning nodes and intermediate conclusions. This would prevent logical gaps or illogical leaps in reasoning, offering robust regulatory support for high-stakes industries such as law and finance.

##### Intervenable Generation During Reasoning

Contemporary Agentic RAG often operate as ”black boxes,” rendering external interventions nearly impossible during generative reasoning tasks. However, providing mechanisms for human intervention—such as through visualization or interactive interfaces—could enable experts or users to perform manual corrections, initialize prior knowledge, or modify interim assumptions during the reasoning process. This would substantially enhance the system’s flexibility and safety.

Specifically, intervenable generation allows not only post hoc error corrections but also proactive identification and rectification of potential risks or biases at earlier stages. Interactive interpretable reasoning platforms or visualization tools grounded in knowledge graphs could empower users to scrutinize and influence reasoning workflows, thereby enhancing confidence and control in decision-making processes across diverse domains.

##### Risk Decision Interception Firewalls

In closed-loop automated tasks such as algorithmic trading or medical diagnostic decision-making, erroneous reasoning outputs can lead to catastrophic outcomes. To mitigate such risks, the system architecture should incorporate *risk decision interception firewalls*, which perform multidimensional validations at critical reasoning nodes or prior to outputting decisions. When confidence levels or high-risk indicators breach thresholds, these firewalls can block decision outputs or escalate them for stricter human review.

This mechanism serves as a “final line of defense” for RAG+Reasoning systems, ensuring decision security in large-scale automated information networks. It also provides a robust foundation for compliance and regulatory auditing, enabling safer deployment in critical applications.

##### Edge-Cloud Collaborative Retrieval and Reasoning

With the rapid development of IoT and 5G technologies, many scenarios demand on-site data collection and preliminary processing on edge devices, followed by high-level retrieval and reasoning tasks on cloud platforms. Efficiently partitioning tasks, allocating resources, and maintaining consistency between indexes and models across the edge-cloud continuum represent critical research directions.

Leveraging techniques such as lightweight model compression, distributed index synchronization, and communication optimization can ensure fast reasoning while maximizing resource utilization. Edge-cloud collaborative solutions are particularly impactful for real-time industrial monitoring and smart city applications, reducing network latency and bandwidth bottlenecks while ensuring accurate and timely inference outputs.

In summary, RAG+Reasoning systems present many untapped opportunities across various dimensions. Further research and practical validation could greatly improve their use in complex, high-risk scenarios while fueling new growth in GenAI.

## 9. Future Trends

In this chapter, we summarize four major trends in technological advancements based on current research, aiming to elucidate and guide the potential future directions of RAG.

### 9.1. The Integration of RAG and Graph

Recent developments have witnessed a growing synergy between RAG systems and graph-based approaches. The intrinsic benefits of graph structures, such as explicit logical relationships and knowledge indexing, have enabled new paradigms for addressing challenges in global reasoning, dynamic data management, and personalized services within RAG systems.

Knowledge Organization.

Graph-structured knowledge organization frameworks offer a powerful alternative to traditional vector-based retrieval methods, excelling in modeling complex relationships and supporting global reasoning. For example, GraphRAG (Edge et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib19)) combines hierarchical graph indexing with community detection to extract entity relationship networks from text corpora, enabling large-scale thematic analysis through hierarchical summaries. Building on this, PIKE (Wang et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)) introduces a multi-level heterogeneous knowledge graph that organizes documents, semantic segments, and refined knowledge units into a three-layer hierarchy, improving extraction accuracy and multi-hop reasoning via atomized knowledge construction and task decomposition. For dynamic personalization, EMG-RAG (Wang et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib90)) features a three-layer Editable Memory Graph architecture that structures memory data by ontology classification, subclass, and entity relationships, using reinforcement learning to enable real-time updates and multidimensional queries. Together, these advances leverage graph topologies to address the limitations of conventional RAG systems—such as one-dimensional representation and weak contextual links—enabling multilevel reasoning from local fact retrieval to global thematic summarization and forming a foundation for interpretable, adaptive RAG systems.

Symbolic Reasoning.
Graph-structured symbolic reasoning methods leverage the multi-hop reasoning power of Knowledge Graphs (KG) to better manage complex semantic and logical relationships. Frameworks like HippoRAG2 and the Think-on-Graph (ToG) (Ma et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) series exemplify this. HippoRAG2 (Gutiérrez et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib29)) builds open knowledge graphs and uses personalized PageRank with a dense-sparse coding approach inspired by brain memory, boosting performance in factual memory, semantic understanding, and multi-hop reasoning. Likewise, ToG-2 combines iterative retrieval of knowledge graphs and documents, using relationship discovery, entity pruning, and context-driven graph searches to integrate fine-grained information from unstructured text, enhancing implicit relationship detection.

Task Planning.
Graph-based task planning in RAG systems enhances complex problem-solving by overcoming the limitations of traditional linear workflows, which struggle with multi-step or multimodal reasoning. These approaches build dynamic knowledge graphs, like Mind Maps, to explicitly model logical dependencies and context. For instance, the Agentic Reasoning (Wu et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) transforms reasoning chains into graph structures for entity extraction, relation identification, and community clustering, enabling dynamic path tracking and optimized retrieval, excelling in tasks like doctoral-level GPQA (Rein et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib68)). Collaborative frameworks such as Co-STORM extend this to multi-agent scenarios, representing queries, tool calls, and knowledge integration as traversable graph nodes to support task decomposition and adaptive reasoning.

Tool Usage and Management.
Graph-enhanced approaches to tool management overcome limitations of traditional dependency modeling by effectively capturing complex relationships like parameter passing, functional collaboration, and resource management. Graph RAG-Tool Fusion (Lumer et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib58)) models tools as graph nodes within a dual-layer architecture of core system APIs and domain-specific tools, encoding direct and indirect dependencies as edges. It uses a two-stage retrieval process: vector-based tool retrieval followed by a graph-based depth-first search to assemble dependency-compliant toolsets.

### 9.2. Multi-Model Collaboration

Multi-model collaboration has emerged as a pivotal strategy for enhancing task complexity handling and domain adaptability in RAG systems (Chen et al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib14)). By integrating the strengths of different models, this approach achieves optimized performance. For example, the CR-Planner (Li et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) combines general-purpose generation models (e.g., GPT-4) with domain-specific critic models (e.g., Llama-3-8B). This hybrid system dynamically orchestrates subgoal planning and execution evaluation, utilizing MCTS to generate high-quality training data. Similarly, UAR (Cheng et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))employs intent-aware and knowledge-requirement classifiers to dynamically trigger retrieval, decoupling lightweight classification tasks from resource-intensive decoding operations of LLMs. Furthermore, Adaptive-RAG  (Jeong et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) deploys small-complexity classifiers to route queries into different levels of processing strategies, balancing response speed for simple queries with deep reasoning for complex ones. These strategies form a closed ”generation-evaluation”loop, leveraging complementary strengths across models to achieve improved accuracy and computational efficiency.

### 9.3. Multi-Modal Collaboration

The breakthrough in Chain-of-Thought (CoT) capabilities of language models has catalyzed the transition of multimodal reasoning from perceptual-level integration to cognitive-level reasoning, promoting Multimodal Collaborative Reasoning as a key trend (Bi et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib5)) By deeply integrating the logical reasoning capabilities of language models with the spatial-semantic representation of multimodal data, it significantly enhances information synthesis in complex scenarios (Abootorabi et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib3)). For instance, in the medical domain, multimodal RAG systems such as MedCoT (Liu et al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib57)) utilize hierarchical expert systems to integrate CT imaging and pathology reports, enabling knowledge graph validation of diagnostic hypotheses and reducing misdiagnosis risks. Future research will likely focus on robust cross-modal knowledge alignment, progressive knowledge distillation, and adaptive reasoning frameworks.

### 9.4. Customized Reinforcement Learning

The application of reinforcement learning (RL) in RAG systems has become instrumental in improving module coordination and enhancing overall efficiency. Recent studies focus on designing reward mechanisms tailored to the specific needs of RAG systems. Frameworks such as RAG-Gym (Xiong et al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)) and DeepRAG (Guan et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) model reasoning processes using Markov Decision Processes and introduce fine-grained process supervision mechanisms. Additionally, ReARTeR (Lee et al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) and SmartRAG (Gao et al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) incorporate trust-aware reward strategies and end-to-end policy optimization to achieve superior accuracy and robustness. Opportunities remain for further exploring automated reward modeling with LLMs to facilitate fine-grained supervision.

## 10. Conclusion

This paper has systematically reviewed the synergistic integration of Retrieval-Augmented Generation (RAG) and reasoning, providing a formal definition of reasoning within the RAG framework as a structured, multi-step, goal-driven process that dynamically combines parametric and retrieved knowledge to address complex problems.

We presented a comprehensive taxonomy covering the purposes, collaboration paradigms, and implementation methods underlying RAG+Reasoning systems. The synergy enables more precise retrieval informed by logical analysis and enhances reasoning with contextually relevant, up-to-date knowledge beyond parametric limitations.

While the enhanced reasoning capabilities allow tackling complex knowledge-intensive tasks such as deep research, expert-level problem solving, and domain-specific decision support, practical challenges remain. These include computational and token costs that grow non-linearly, risks of overthinking leading to inefficiency and error propagation, and the lack of evaluation frameworks that effectively assess intermediate reasoning quality alongside final results.

To bridge the gap from theory to real-world application, we proposed practical design guidelines tailored to diverse domains like finance, healthcare, law, and personal assistants, emphasizing adaptability to heterogeneous, dynamic knowledge sources and strict requirements for output reliability and traceability.

Finally, we identified promising directions for future research, including graph-structured knowledge integration, multimodal and multi-model collaborative reasoning architectures, and advanced reinforcement learning techniques for optimizing retrieval-reasoning workflows.

Overall, this work establishes both a theoretical foundation and practical roadmap to drive the development of next-generation RAG+Reasoning systems capable of robust, transparent, and efficient cognition, paving the way for impactful applications across academia and industry.

## References

* (1)
* Abdallah et al. (2025)

  Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, and Adam Jatowt. 2025.
  Rankify: A comprehensive python toolkit for retrieval, re-ranking, and retrieval-augmented generation.
  *arXiv preprint arXiv:2502.02464* (2025).
* Abootorabi et al. (2025)

  Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. 2025.
  Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation.
  *arXiv preprint arXiv:2502.08826* (2025).
* Asai et al. (2023)

  Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
  Self-rag: Learning to retrieve, generate, and critique through self-reflection. In *The Twelfth International Conference on Learning Representations*.
* Bi et al. (2025b)

  Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, et al. 2025b.
  Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1).
  *arXiv preprint arXiv:2504.03151* (2025).
* Bi et al. (2025a)

  Yuxi Bi, Yunfan Gao, and Haofen Wang. 2025a.
  StePO-Rec: Towards Personalized Outfit Styling Assistant via Knowledge-Guided Multi-Step Reasoning.
  *arXiv preprint arXiv:2504.09915* (2025).
* Chen et al. (2025b)

  Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, Jeff Z Pan, et al. 2025b.
  Learning to Reason with Search for LLMs via Reinforcement Learning.
  *arXiv preprint arXiv:2503.19470* (2025).
* Chen et al. (2025f)

  Peter Baile Chen, Yi Zhang, Michael Cafarella, and Dan Roth. 2025f.
  Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method.
  *arXiv preprint arXiv:2501.18539* (2025).
* Chen et al. (2025c)

  Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. 2025c.
  Towards reasoning era: A survey of long chain-of-thought for reasoning large language models.
  *arXiv preprint arXiv:2503.09567* (2025).
* Chen et al. (2023)

  Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023.
  Theoremqa: A theorem-driven question answering dataset.
  *arXiv preprint arXiv:2305.12524* (2023).
* Chen et al. (2024)

  Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024.
  Do not think that much for 2+ 3=? on the overthinking of o1-like llms.
  *arXiv preprint arXiv:2412.21187* (2024).
* Chen et al. (2025d)

  Yixiang Chen, Penglei Sun, Xiang Li, and Xiaowen Chu. 2025d.
  MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation.
  *arXiv preprint arXiv:2504.07724* (2025).
* Chen et al. (2025e)

  Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. 2025e.
  Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning.
  *arXiv preprint arXiv:2501.15228* (2025).
* Chen et al. (2025a)

  Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, and Philip S Yu. 2025a.
  Harnessing Multiple Large Language Models: A Survey on LLM Ensemble.
  *arXiv preprint arXiv:2502.18036* (2025).
* Cheng et al. (2024)

  Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, and Xipeng Qiu. 2024.
  Unified active retrieval for retrieval augmented generation.
  *arXiv preprint arXiv:2406.12534* (2024).
* Cuadron et al. (2025)

  Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. 2025.
  The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks.
  *arXiv preprint arXiv:2502.08235* (2025).
* Dao and Le (2025)

  Alan Dao and Thinh Le. 2025.
  ReZero: Enhancing LLM search ability by trying one-more-time.
  arXiv:2504.11001 [cs.CL]
  <https://arxiv.org/abs/2504.11001>
* Dettmers et al. (2023)

  Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
  Qlora: Efficient finetuning of quantized llms.
  *Advances in neural information processing systems* 36 (2023), 10088–10115.
* Edge et al. (2024)

  Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024.
  From local to global: A graph rag approach to query-focused summarization.
  *arXiv preprint arXiv:2404.16130* (2024).
* Fan et al. (2025)

  Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. 2025.
  Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?
  *arXiv preprint arXiv:2504.06514* (2025).
* Gao et al. (2024a)

  Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. 2024a.
  SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback.
  *arXiv preprint arXiv:2410.18141* (2024).
* Gao et al. (2023)

  Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.
  Retrieval-augmented generation for large language models: A survey.
  *arXiv preprint arXiv:2312.10997* (2023).
* Gao et al. (2024b)

  Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024b.
  Modular rag: Transforming rag systems into lego-like reconfigurable frameworks.
  *arXiv preprint arXiv:2407.21059* (2024).
* Gao et al. (2025)

  Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, and S Kevin Zhou. 2025.
  FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs.
  *arXiv preprint arXiv:2501.09957* (2025).
* Guan et al. (2025)

  Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025.
  DeepRAG: Thinking to Retrieval Step by Step for Large Language Models.
  *arXiv preprint arXiv:2502.01142* (2025).
* Guo et al. (2025a)

  Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025a.
  Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
  *arXiv preprint arXiv:2501.12948* (2025).
* Guo et al. (2025b)

  Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025b.
  Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
  *arXiv preprint arXiv:2501.12948* (2025).
* Guo et al. (2024)

  Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024.
  Lightrag: Simple and fast retrieval-augmented generation.
  (2024).
* Gutiérrez et al. (2025)

  Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025.
  From RAG to Memory: Non-Parametric Continual Learning for Large Language Models.
  *arXiv preprint arXiv:2502.14802* (2025).
* He et al. (2024)

  Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024.
  Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.
  *arXiv preprint arXiv:2402.14008* (2024).
* He et al. (2025)

  Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, et al. 2025.
  Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?
  *arXiv preprint arXiv:2502.19361* (2025).
* Ho et al. (2020)

  Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
  Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.
  *arXiv preprint arXiv:2011.01060* (2020).
* Hong et al. (2025)

  Yubin Hong, Chaofan Li, Jingyi Zhang, and Yingxia Shao. 2025.
  FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG.
  *arXiv preprint arXiv:2504.07103* (2025).
* Hongjin et al. (2024)

  SU Hongjin, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Liu Haisu, Quan Shi, Zachary S Siegel, Michael Tang, et al. 2024.
  BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In *The Thirteenth International Conference on Learning Representations*.
* Hsu et al. (2024)

  Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. 2024.
  Grounding by trying: Llms with reinforcement learning-enhanced retrieval.
  *arXiv preprint arXiv:2410.23214* (2024).
* Hu (2025)

  Jian Hu. 2025.
  REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models.
  *arXiv preprint arXiv:2501.03262* (2025).
* Hu et al. (2025)

  Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. 2025.
  MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search.
  *arXiv preprint arXiv:2503.20757* (2025).
* Huot et al. (2024)

  Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, and Mirella Lapata. 2024.
  Agents’ Room: Narrative Generation through Multi-step Collaboration.
  *arXiv preprint arXiv:2410.02603* (2024).
* Islam et al. (2024)

  Shayekh Bin Islam, Md Asib Rahman, KSM Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan Parvez. 2024.
  Open-rag: Enhanced retrieval-augmented reasoning with open-source large language models.
  *arXiv preprint arXiv:2410.01782* (2024).
* Jaech et al. (2024)

  Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024.
  Openai o1 system card.
  *arXiv preprint arXiv:2412.16720* (2024).
* Jain et al. (2024)

  Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024.
  Livecodebench: Holistic and contamination free evaluation of large language models for code.
  *arXiv preprint arXiv:2403.07974* (2024).
* Jeong et al. (2024)

  Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024.
  Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity.
  *arXiv preprint arXiv:2403.14403* (2024).
* Jiang (2025)

  Pengcheng Jiang. 2025.
  DeepRetrieval: Powerful Query Generation for Information Retrieval with Reinforcement Learning.
  *arXiv preprint arXiv:2503.00223* (2025).
* Jiang et al. (2024a)

  Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J Semnani, and Monica S Lam. 2024a.
  Into the unknown unknowns: Engaged human learning through participation in language model agent conversations.
  *arXiv preprint arXiv:2408.15232* (2024).
* Jiang et al. (2024b)

  Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J Semnani, and Monica S Lam. 2024b.
  Into the unknown unknowns: Engaged human learning through participation in language model agent conversations.
  *arXiv preprint arXiv:2408.15232* (2024).
* Jiang et al. (2023)

  Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.
  Active retrieval augmented generation. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 7969–7992.
* Joshi et al. (2024)

  Ashutosh Joshi, Sheikh Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, and Juhi Naik. 2024.
  REAPER: Reasoning based retrieval planning for complex RAG systems. In *Proceedings of the 33rd ACM International Conference on Information and Knowledge Management*. 4621–4628.
* Kwiatkowski et al. (2019)

  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.
  Natural questions: a benchmark for question answering research.
  *Transactions of the Association for Computational Linguistics* 7 (2019), 453–466.
* Lee et al. (2024)

  Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024.
  PlanRAG: A plan-then-retrieval augmented generation for generative large language models as decision makers. In *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*. 6537–6555.
* Lee et al. (2025)

  Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi Li. 2025.
  ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation.
  *arXiv preprint arXiv:2503.21729* (2025).
* Li et al. (2024c)

  Jinzheng Li, Jingshu Zhang, Hongguang Li, and Yiqing Shen. 2024c.
  An Agent Framework for Real-Time Financial Information Searching with Large Language Models.
  *arXiv preprint arXiv:2502.15684* (2024).
* Li et al. (2025a)

  Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a.
  Search-o1: Agentic search-enhanced large reasoning models.
  *arXiv preprint arXiv:2501.05366* (2025).
* Li et al. (2024a)

  Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. 2024a.
  Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks.
  *arXiv preprint arXiv:2410.01428* (2024).
* Li et al. (2024b)

  Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. 2024b.
  Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks.
  *arXiv preprint arXiv:2410.01428* (2024).
* Li et al. (2025b)

  Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and Le Sun. 2025b.
  Deepsolution: Boosting complex engineering solution design via tree-based exploration and bi-point thinking.
  *arXiv preprint arXiv:2502.20730* (2025).
* Lightman et al. (2023)

  Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.
  Let’s verify step by step. In *The Twelfth International Conference on Learning Representations*.
* Liu et al. (2024)

  Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, and Zuozhu Liu. 2024.
  Medcot: Medical chain of thought via hierarchical expert.
  *arXiv preprint arXiv:2412.13736* (2024).
* Lumer et al. (2025)

  Elias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason, James A Burke, and Vamse Kumar Subbiah. 2025.
  Graph RAG-Tool Fusion.
  *arXiv preprint arXiv:2502.07223* (2025).
* Luo et al. (2025)

  Haoran Luo, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan, et al. 2025.
  KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search.
  *arXiv preprint arXiv:2501.18922* (2025).
* Lyu et al. (2025)

  Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2025.
  Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models.
  *ACM Transactions on Information Systems* 43, 2 (2025), 1–32.
* Ma et al. (2024)

  Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2024.
  Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation.
  *arXiv preprint arXiv:2407.10805* (2024).
* Ma et al. (2023)

  Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.
  Query rewriting in retrieval-augmented large language models. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 5303–5315.
* Mialon et al. (2023)

  Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023.
  Gaia: a benchmark for general ai assistants. In *The Twelfth International Conference on Learning Representations*.
* Muennighoff et al. (2025)

  Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025.
  s1: Simple test-time scaling.
  *arXiv preprint arXiv:2501.19393* (2025).
* Patil et al. (2024)

  Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2024.
  Gorilla: Large language model connected with massive apis.
  *Advances in Neural Information Processing Systems* 37 (2024), 126544–126565.
* Petroni et al. (2020)

  Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020.
  KILT: a benchmark for knowledge intensive language tasks.
  *arXiv preprint arXiv:2009.02252* (2020).
* Pezeshkpour and Hruschka (2025)

  Pouya Pezeshkpour and Estevam Hruschka. 2025.
  Insight-RAG: Enhancing LLMs with Insight-Driven Augmentation.
  *arXiv preprint arXiv:2504.00187* (2025).
* Rein et al. (2024)

  David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2024.
  Gpqa: A graduate-level google-proof q&a benchmark. In *First Conference on Language Modeling*.
* Shao et al. (2023)

  Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.
  Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.
  *arXiv preprint arXiv:2305.15294* (2023).
* Shao et al. (2024)

  Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. 2024.
  Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
  *arXiv preprint arXiv:2402.03300* (2024).
* Shi et al. (2024a)

  Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024a.
  Can Language Models Solve Olympiad Programming?
  *arXiv preprint arXiv:2404.10952* (2024).
* Shi et al. (2024b)

  Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024b.
  Can Language Models Solve Olympiad Programming?
  *arXiv preprint arXiv:2404.10952* (2024).
* Song et al. (2025)

  Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025.
  R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning.
  *arXiv preprint arXiv:2503.05592* (2025).
* Srinivas and Runkana (2025)

  Sakhinana Sagar Srinivas and Venkataramana Runkana. 2025.
  Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding.
  *arXiv preprint arXiv:2504.01281* (2025).
* Sui et al. (2025)

  Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. 2025.
  Stop overthinking: A survey on efficient reasoning for large language models.
  *arXiv preprint arXiv:2503.16419* (2025).
* Sun et al. (2025)

  Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and Han Li. 2025.
  ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding.
  *arXiv preprint arXiv:2501.07861* (2025).
* Talmor and Berant (2018)

  Alon Talmor and Jonathan Berant. 2018.
  The web as a knowledge-base for answering complex questions.
  *arXiv preprint arXiv:1803.06643* (2018).
* Tran et al. (2024)

  Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. 2024.
  RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models.
  *arXiv preprint arXiv:2412.02830* (2024).
* Trivedi et al. (2022a)

  Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a.
  Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.
  *arXiv preprint arXiv:2212.10509* (2022).
* Trivedi et al. (2022b)

  Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b.
  MuSiQue: Multihop Questions via Single-hop Question Composition.
  *Transactions of the Association for Computational Linguistics* 10 (2022), 539–554.
* Vu et al. (2023)

  Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023.
  Freshllms: Refreshing large language models with search engine augmentation.
  *arXiv preprint arXiv:2310.03214* (2023).
* Wang et al. (2025c)

  Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, and Dong Yu. 2025c.
  Don’t Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls.
  *arXiv preprint arXiv:2502.11183* (2025).
* Wang et al. (2025b)

  Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025b.
  PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation.
  *arXiv preprint arXiv:2501.11551* (2025).
* Wang et al. (2025a)

  Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025a.
  Chain-of-Retrieval Augmented Generation.
  *arXiv preprint arXiv:2501.14342* (2025).
* Wang et al. (2024e)

  Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, et al. 2024e.
  Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation.
  *arXiv preprint arXiv:2410.08821* (2024).
* Wang et al. (2024b)

  Siqi Wang, Chao Liang, Yunfan Gao, Yang Liu, Jing Li, and Haofen Wang. 2024b.
  Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT. In *Proceedings of the 32nd ACM International Conference on Multimedia*. 4757–4765.
* Wang et al. (2024c)

  Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, and Zhicheng Dou. 2024c.
  Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation.
  *arXiv preprint arXiv:2406.05654* (2024).
* Wang et al. (2023)

  Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. 2023.
  Cmb: A comprehensive medical benchmark in chinese.
  *arXiv preprint arXiv:2308.08833* (2023).
* Wang et al. (2024d)

  Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. 2024d.
  Searching for best practices in retrieval-augmented generation.
  *arXiv preprint arXiv:2407.01219* (2024).
* Wang et al. (2024a)

  Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, and Wei Shi. 2024a.
  Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs.
  *arXiv preprint arXiv:2409.19401* (2024).
* Wang et al. (2025d)

  Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linpeng Tang, Wentao Zhang, et al. 2025d.
  RARE: Retrieval-Augmented Reasoning Modeling.
  *arXiv preprint arXiv:2503.23513* (2025).
* Weng et al. (2024)

  Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2024.
  Cycleresearcher: Improving automated research via automated review.
  *arXiv preprint arXiv:2411.00816* (2024).
* Wu et al. (2025b)

  Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025b.
  Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research.
  *arXiv preprint arXiv:2502.04644* (2025).
* Wu et al. (2025a)

  Wenjie Wu, Yongcheng Jing, Yingjie Wang, Wenbin Hu, and Dacheng Tao. 2025a.
  Graph-augmented reasoning: Evolving step-by-step knowledge graph retrieval for llm reasoning.
  *arXiv preprint arXiv:2503.01642* (2025).
* Xi et al. (2025)

  Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, and Huajun Chen. 2025.
  OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking.
  *arXiv preprint arXiv:2501.09751* (2025).
* Xiao et al. (2025)

  Liang Xiao, Wen Dai, Shuai Chen, Bin Qin, Chongyang Shi, Haopeng Jing, and Tianyu Guo. 2025.
  Retrieval-Augmented Generation by Evidence Retroactivity in LLMs.
  *arXiv preprint arXiv:2501.05475* (2025).
* Xiong et al. (2025b)

  Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. 2025b.
  Rag-gym: Optimizing reasoning and search agents with process supervision.
  *arXiv preprint arXiv:2502.13957* (2025).
* Xiong et al. (2025c)

  Guanming Xiong, Haochen Li, and Wen Zhao. 2025c.
  MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering.
  *arXiv preprint arXiv:2502.13428* (2025).
* Xiong et al. (2025a)

  Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, and Jürgen Schmidhuber. 2025a.
  Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models.
  *arXiv preprint arXiv:2503.08275* (2025).
* Xu et al. (2025)

  Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. 2025.
  Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models.
  *arXiv preprint arXiv:2501.09686* (2025).
* Xu et al. (2024)

  Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Chaojun Xiao, Zhiyuan Liu, Ge Yu, and Chenyan Xiong. 2024.
  ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents.
  *arXiv preprint arXiv:2402.13547* (2024).
* Yan et al. (2025)

  Ruiran Yan, Zheng Liu, and Defu Lian. 2025.
  O1 embedder: Let retrievers think before action.
  *arXiv preprint arXiv:2502.07555* (2025).
* Zhang et al. (2024)

  Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei Zhang. 2024.
  Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering.
  *arXiv preprint arXiv:2408.11875* (2024).
* Zhang et al. (2025)

  Zhuocheng Zhang, Yang Feng, and Min Zhang. 2025.
  LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers.
  *arXiv preprint arXiv:2502.18139* (2025).
* Zhao et al. (2024)

  Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah A Smith. 2024.
  Set the clock: Temporal alignment of pretrained language models.
  *arXiv preprint arXiv:2402.16797* (2024).
* Zhao et al. (2025)

  Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chunyan Miao. 2025.
  MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot.
  *arXiv preprint arXiv:2502.04413* (2025).
* Zheng et al. (2025)

  Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.
  DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments.
  *arXiv preprint arXiv:2504.03160* (2025).
* Zhong et al. (2025)

  Yijie Zhong, Feifan Wu, Mengying Guo, Xiaolian Zhang, Meng Wang, and Haofen Wang. 2025.
  Meta-PKE: Memory-Enhanced Task-Adaptive Personal Knowledge Extraction in Daily Life.
  *Information Processing & Management* 62, 4 (2025), 104097.
* Zhou et al. (2024)

  Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. 2024.
  Metacognitive retrieval-augmented large language models. In *Proceedings of the ACM Web Conference 2024*. 1453–1463.
* Zhu et al. (2025b)

  Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, and Weinan Zhang. 2025b.
  Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning.
  *arXiv preprint arXiv:2502.14361* (2025).
* Zhu et al. (2025a)

  Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. 2025a.
  Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering.
  *arXiv preprint arXiv:2502.14245* (2025).

## Appendix

### Agentic RAG Symbol Reference System

The following table presents a complete symbol reference system with formally defined mathematical notations for all core concepts.

Table 3. Basic states and system components

| Symbol | Type | Definition & Description |
| --- | --- | --- |
| St=(Ht,Ct)subscript𝑆𝑡subscript𝐻𝑡subscript𝐶𝑡S\_{t}=(H\_{t},C\_{t})italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = ( italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) | Composite state | Complete system state at timestep t𝑡titalic\_t, containing historical information and context vectors |
| Htsubscript𝐻𝑡H\_{t}italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | Vector/Set | Historical information aggregation |
| Ctsubscript𝐶𝑡C\_{t}italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | Vector | Contextual embedding vectors |
| qtsubscript𝑞𝑡q\_{t}italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | Vector | Vector representation of current query at step t𝑡titalic\_t |
| 𝒦tsubscript𝒦𝑡\mathcal{K}\_{t}caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | Set | Dynamic knowledge base (Initialized as 𝒦0=∅subscript𝒦0\mathcal{K}\_{0}=\emptysetcaligraphic\_K start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT = ∅) |

Table 4. Action space and policy definitions

| Symbol | Type | Definition & Description |
| --- | --- | --- |
| 𝒜𝒜\mathcal{A}caligraphic\_A | Set | Action space, e.g., 𝒜={Retrieve,Generate,Verify,Terminate}𝒜RetrieveGenerateVerifyTerminate\mathcal{A}=\{\text{Retrieve},\text{Generate},\text{Verify},\text{Terminate}\}caligraphic\_A = { Retrieve , Generate , Verify , Terminate } |
| atsubscript𝑎𝑡a\_{t}italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | Scalar | Selected action at timestep t𝑡titalic\_t (at∈𝒜subscript𝑎𝑡𝒜a\_{t}\in\mathcal{A}italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ∈ caligraphic\_A) |
| π⁢(St;Θ)𝜋  subscript𝑆𝑡Θ\pi(S\_{t};\Theta)italic\_π ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ; roman\_Θ ) | Function | Policy function with parameters ΘΘ\Thetaroman\_Θ, mapping states to action probability distributions (π:𝒮→Δ⁢(𝒜):𝜋→𝒮Δ𝒜\pi:\mathcal{S}\to\Delta(\mathcal{A})italic\_π : caligraphic\_S → roman\_Δ ( caligraphic\_A )) |

Table 5. State transition mechanisms

| Symbol | Type | Definition & Description |
| --- | --- | --- |
| δ𝛿\deltaitalic\_δ | Function | State transition function, update rule St+1=δ⁢(St,⋅)subscript𝑆𝑡1𝛿subscript𝑆𝑡⋅S\_{t+1}=\delta(S\_{t},\cdot)italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_δ ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , ⋅ ) |
| 𝒯asubscript𝒯𝑎\mathcal{T}\_{a}caligraphic\_T start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT | Function | Low-level state transition operation for action a𝑎aitalic\_a (e.g., 𝒯Retrievesubscript𝒯Retrieve\mathcal{T}\_{\text{Retrieve}}caligraphic\_T start\_POSTSUBSCRIPT Retrieve end\_POSTSUBSCRIPT denotes retrieval) |
| ℛℛ\mathcal{R}caligraphic\_R | Function | Retrieval function, ℛ⁢(St)ℛsubscript𝑆𝑡\mathcal{R}(S\_{t})caligraphic\_R ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) returns retrieval results |
| ∘\circ∘ | Operator | Function composition operator (e.g., f∘g⁢(x)=f⁢(g⁢(x))𝑓𝑔𝑥𝑓𝑔𝑥f\circ g(x)=f(g(x))italic\_f ∘ italic\_g ( italic\_x ) = italic\_f ( italic\_g ( italic\_x ) )) |

Table 6. Feedback and optimization components

| Symbol | Type | Definition & Description |
| --- | --- | --- |
| R⁢(St,at,St+1)𝑅subscript𝑆𝑡subscript𝑎𝑡subscript𝑆𝑡1R(S\_{t},a\_{t},S\_{t+1})italic\_R ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ) | Function | Reward function, outputs reward value rtsubscript𝑟𝑡r\_{t}italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT |
| 𝕀⁢(⋅)𝕀⋅\mathbb{I}(\cdot)blackboard\_I ( ⋅ ) | Function | Indicator function (returns 1 if condition holds, else 0) |
| ∇θJ⁢(θ)subscript∇𝜃𝐽𝜃\nabla\_{\theta}J(\theta)∇ start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT italic\_J ( italic\_θ ) | Operator | Policy gradient for optimizing policy parameters ΘΘ\Thetaroman\_Θ |
| γ𝛾\gammaitalic\_γ | Scalar | Discount factor for cumulative reward calculation |

Table 7. Submodule-specific symbols

| Symbol | Type | Definition & Description |
| --- | --- | --- |
| ΨΨ\Psiroman\_Ψ | Function | Reasoning function, generates intermediate reasoning results |
| ΓΓ\Gammaroman\_Γ | Function | Decision function, produces final outputs (e.g., answers) |
| ψ⁢(⋅)𝜓⋅\psi(\cdot)italic\_ψ ( ⋅ ) | Function | Branch selector for reflective reasoning path selection |
| ϕ⁢(⋅)italic-ϕ⋅\phi(\cdot)italic\_ϕ ( ⋅ ) | Function | Confidence mapping function (evaluations to scalar confidence) |
| τ𝜏\tauitalic\_τ | Scalar | Decision threshold for triggering specific operations (e.g., verification/termination) |

### Symbol Design Hierarchy

* •

  Base states/actions: Standard font (St,at
  subscript𝑆𝑡subscript𝑎𝑡S\_{t},a\_{t}italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT)
* •

  Sets/spaces: Calligraphic font (𝒜,𝒦t
  𝒜subscript𝒦𝑡\mathcal{A},\mathcal{K}\_{t}caligraphic\_A , caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT)
* •

  Core mechanism functions: Uppercase Greek (Ψ,Γ
  ΨΓ\Psi,\Gammaroman\_Ψ , roman\_Γ)
* •

  Operational functions: Calligraphic font (ℛ,𝒯a
  ℛsubscript𝒯𝑎\mathcal{R},\mathcal{T}\_{a}caligraphic\_R , caligraphic\_T start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT)
* •

  Auxiliary functions: Lowercase Greek (δ,ϕ
  𝛿italic-ϕ\delta,\phiitalic\_δ , italic\_ϕ) or blackboard bold (𝕀𝕀\mathbb{I}blackboard\_I)

### Annotation Guidelines

* •

  Symbol disambiguation:

  + –

    ℛℛ\mathcal{R}caligraphic\_R strictly denotes retrieval function (vs. reward R𝑅Ritalic\_R)
  + –

    δ𝛿\deltaitalic\_δ exclusively represents state transitions (vs. branch selector ψ𝜓\psiitalic\_ψ)
* •

  Dynamic extensions:

  + –

    Action space 𝒜𝒜\mathcal{A}caligraphic\_A and knowledge base 𝒦tsubscript𝒦𝑡\mathcal{K}\_{t}caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT support incremental updates:
    𝒦t+1=𝒦t⊕Retrieve⁢(qt)subscript𝒦𝑡1direct-sumsubscript𝒦𝑡Retrievesubscript𝑞𝑡\mathcal{K}\_{t+1}=\mathcal{K}\_{t}\oplus\text{Retrieve}(q\_{t})caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ⊕ Retrieve ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT )

Generated on Thu Apr 24 12:39:21 2025 by [LaTeXML![Mascot Sammy](data:image/png;base64...)](http://dlmf.nist.gov/LaTeXML/)