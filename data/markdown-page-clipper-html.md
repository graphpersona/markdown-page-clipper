---
title: Synergizing RAG and Reasoning: A Systematic Review
source: https://arxiv.org/html/2504.15909v2
clipped: 2025-05-12T08:07:33.342Z
domain: arxiv.org
author: ,
lang: en
words: 25188
---

# Synergizing RAG and Reasoning: A Systematic Review

**Source:** [https://arxiv.org/html/2504.15909v2](https://arxiv.org/html/2504.15909v2)

**Clipped:** 2025-05-12T08:07:33.342Z

\\useforestlibrary

edges \\useunder\\ul

Yunfan Gao Shanghai Research Institute for Intelligent Autonomous Systems, Tongji UniversityChina [gaoyunfan1602@gmail.com](mailto:gaoyunfan1602@gmail.com) Yun Xiong Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan UniversityChina [yunx@fudan.edu.cn](mailto:yunx@fudan.edu.cn) ,Â  Yijie Zhong College of Design and Innovation, Tongji UniversityChina [dun.haski@gmail.com](mailto:dun.haski@gmail.com) ,Â  Yuxi Bi College of Design and Innovation, Tongji UniversityChina [yuxibi@gmail.com](mailto:yuxibi@gmail.com) ,Â  Ming Xue Percena AIChina [mxue@percena.co](mailto:mxue@percena.co) Â andÂ  Haofen Wang College of Design and Innovation, Tongji UniversityChina [carter.whfcarter@gmail.com](mailto:carter.whfcarter@gmail.com)

###### Abstract.

Recent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining â€reasoningâ€ within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.

â€ â€ copyright: none

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/timeline.png)

Figure 1. Timeline of studies on RAG-reasoning synergy. From a technical perspective, the approaches can be categorized into Prompt-Based, Tuning-Based, and RL-Based methods. A notable trend is the increasing use of Reinforcement Learning to enhance RAG systems, particularly following the prosperity of test-time scaling. Meanwhile, Prompt-Based and Tuning-Based methods continue to evolve in parallel, demonstrating that there are multiple pathways to integrating reasoning capabilities into RAG systems.

## 1\. Introduction

Recent breakthroughs in Large Language Models (LLMs) like OpenAI O1Â (Jaech etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib40)) and DeepSeek-R1Â (Guo etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib26)) have shifted the paradigm from â€pre-training scalingâ€ to â€test-time scalingâ€Â (Muennighoff etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib64)). Unlike traditional language models that improve via corpus accumulation during pre-training, these models enhance performance in complex tasksâ€”such as mathematical derivation and code generationÂ (He etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib30))â€”through post-training innovations during the inference phase (e.g., Long-CoT thinkingÂ (Chen etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib9))). This shift has led to the emergence of â€Large Reasoning Modelsâ€ (LRMs)Â (Xu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib100)) with advanced internal reasoning abilities.

These advancements have not only boosted basic model capabilities but also opened new avenues for application technologies like Retrieval-Augmented Generation (RAG)Â (Gao etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib22)). Serving as a key link between language models and external knowledge, RAG overcomes traditional LLMsâ€™ limits in knowledge freshness, domain specificity, and factual accuracy by retrieving real-time non-parametric information and integrating it into the context. This enhances information processing and reduces hallucination risks in knowledge-intensive tasks.

Technological evolution is advancing RAG architectures through innovations like query rewritingÂ (Ma etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib62)), re-rankingÂ (Abdallah etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib2)), and hybrid retrievalÂ (Wang etÂ al., [2024d](https://arxiv.org/html/2504.15909v2#bib.bib89)), creating an Advanced RAG paradigm focused on pre-retrieval optimization and post-retrieval refinement. Modular RAGÂ (Gao etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib23)) further breaks down these systems into component-based, service-oriented architectures, using orchestration to tackle practical challenges.

Despite improvements in query intent recognition and knowledge use, challenges of RAG remain in demanding tasks like deep research and complex decision-making. Key issues include: 1) difficulty capturing intent from ambiguous queries; 2) poor logical coherence in multi-hop reasoning; 3) efficiency limits of traditional retrieval in open domains; and 4) degraded generation quality from noisy retrieved data.

Models like DeepSeek-R1, with strong reasoning capabilities, inspire new directions for RAG systems. As shown in FigureÂ [1](https://arxiv.org/html/2504.15909v2#S0.F1 "Figure 1 â€£ Synergizing RAG and Reasoning: A Systematic Review"), recent research explores integrating formal reasoning frameworks with knowledge retrieval. This approach optimizes retrieval through logic-driven query reformulation and uses reasoning to analyze and validate retrieved knowledge, creating cognitive synergy between retrieval and generation. This paradigm aims to overcome conventional limitations, enabling intelligent systems with rigorous logic and reliable knowledge use. From a trend perspective, an increasing number of methods combine reasoning and retrieval abilities through reinforcement learning (RL), marking a new direction in the LRM era. Meanwhile, prompt-based approaches continue to rapidly evolve, with researchers aiming to achieve results through workflow design while keeping model parameters frozen. Notably, sole reliance on tuning methods is steadily decreasing, suggesting limited improvements from additional fine-tuning at this stage.

Traditional RAG is limited by its unidirectional flow (retrieval â†’ generation). Integrating reasoning capabilities grants the system greater autonomy, unlocking new possibilities. As shown in FigureÂ [2](https://arxiv.org/html/2504.15909v2#S1.F2 "Figure 2 â€£ 1. Introduction â€£ Synergizing RAG and Reasoning: A Systematic Review"), this integration is poised to drive major breakthroughs, enabling practical use in complex real-world scenarios.

1) From Ambiguous Semantic Matching to Logic-Driven Targeted Retrieval. Traditional RAG relies on semantic similarity for retrieval; however, it is sensitive to phrasing variations. Advanced reasoning allows deep logical analysis of queries (e.g., causal links, conditional constraints) to dynamically refine retrieval strategiesÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)). For example, to answer â€How to reduce postoperative infection risks in diabetes patients?â€, the system prioritizes retrieving â€blood glucose control thresholdsâ€ and â€antibiotic usage guidelinesâ€ over simply matching â€diabetes postoperative careâ€. This approach supports multi-hop retrieval by breaking down complex queries into sequential sub-queries while preserving cross-document coherence through reasoning chains.

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/advantage.png)

Figure 2. Advantages of Combining RAG with Reasoning

2) From Simple Information Aggregation to Logically Coherent Context Construction. Current RAG systems input all retrieved document chunks into context directly, often causing fragmented or contradictory information that confuses LLMs. Reasoning-enhanced systems integrate evidence chains by logically verifying and inferring causality in retrieved content, filtering conflicts and forming coherent explanationsÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)). They also use dynamic knowledge completion to detect missing logical links, prompting iterative retrieval or inference to fill gapsÂ (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)).

3) From Simple and Single-Turn QA to Systemic Decision Support. Traditional RAG performs well in factual QAÂ (Petroni etÂ al., [2020](https://arxiv.org/html/2504.15909v2#bib.bib66)) but struggles with multi-step and complex decision-making. Reasoning-integrated systems produce structured reasoning output, enhancing multi-objective optimization to balance retrieval breadth and solution feasibility under various constraints. For example, multiple constraints under different conditions in engineering construction plansÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)), and the formulation of diagnosis and treatment plans for various diseases in the medical fieldÂ (Zhao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)).

4) From Indiscriminate Retrieval to Intelligent Resource Allocation. Traditional RAG retrieves documents for all queries, regardless of complexity. Reasoning-enhanced systems use on-demand retrieval, handling simple queries with direct generation and complex ones with multi-round retrieval to reduce latencyÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)). Dynamic retrieval pruning uses pre-reasoning predictions to target key information, minimizing unnecessary document and graph traversalÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)).

5) From Passive Knowledge Tool to Proactive Cognitive Assistant. Advancing beyond reactive knowledge retrieval, reasoning-enhanced systems can proactively serve users by asking clarifying questions and anticipating implicit needs. This shift enables human-like assistants that integrate memory, reasoning, and decision-making, proving especially valuable for complex tasks such as deep researchÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)), business analyticsÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)), personal assistantÂ (Zhong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib108)) and urban planningÂ (Wang etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib86)).

However, the synergistic pathway between RAG and reasoning requires more than simply replacing conventional generative LLMs with LRM modules. It necessitates deep integration of technological evolution insights from LRM - achieved through reconstructing knowledge retrieval mechanisms and strengthening reasoning-generation collaborative linkages - to enable system-level enhancement of cognitive capabilities within the RAG architecture.

Therefore, this paper aims to address the pivotal and forward-looking research question of â€how RAG systems can synergize with reasoning capabilitiesâ€. We systematically review current studies after 2024 while establishing explicit definitions for reasoning within RAG contexts. Building on this foundation, we provide an in-depth taxonomy and analysis of the objectives, typical patterns, and implementations underlying RAG-reasoning integration, clarifying key technological trajectories and critical breakthroughs.

As RAG technology enters its next developmental phase, downstream task complexity has escalated significantly - particularly evident in emerging challenges like Deep ResearchÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)). These advanced applications not only demand enhanced reasoning capacities but also drive RAGâ€™s expansion into multimodal, cross-domain, and dynamic environments. However, while the integration of reasoning capabilities demonstrably improves complex task performance, existing research frequently overlooks associated computational overheads and potential risks. Through systematic examination of these operational constraints and analysis of industry applications, we propose practical guidelines for multiple real-world scenarios with diverse requirements.

Finally, we outline future research directions grounded in current technological evolution, including: 1) RAG-graph architecture integration, 2) coordinated multimodal reasoning frameworks, 3) hybrid model collaboration, and 4) RL optimization specifically designed for RAG systems. This work establishes both theoretical foundations and practical roadmaps for subsequent research in this evolving field.

The contributions of this paper can be summarized as follows:

-   â€¢
    
    Pioneering Review. This work represents the first comprehensive survey focusing on the integration of RAG with reasoning, offering novel insights and forward-looking guidance for advancing this emerging research frontier.
    
-   â€¢
    
    Systematic Taxonomy. We present a multi-dimensional framework to systematically examine the objectives, paradigms, and methodologies for combining RAG with reasoning capabilities, establishing clear classification criteria across technical dimensions.
    
-   â€¢
    
    Practical Guidance. Beyond theoretical exploration, we critically discuss the additional cost and potential risks associated with the introduction of reasoning, accompanied by an actionable Practical Guide for real-world scenarios.
    
-   â€¢

## 2\. Overview

This chapter establishes a conceptual framework for the paper along two key dimensions. First, it formally defines â€reasoningâ€ and distinguishes it from â€inference.â€ Second, it organizes a taxonomy of synergy mechanisms between â€RAG and Reasoning.â€ To construct a clear cognitive pathway, we address three progressive research questions:

-   â€¢
    
    Why synergize RAG and reasoning?
    
-   â€¢
    
    What are their typical collaboration paradigms?
    
-   â€¢
    
    How can this integration be realized?
    

### 2.1. Definition

The definition of reasoning in modern AI systems remains an evolving construct, particularly within the context of LRMs exemplified by DeepSeek R1 and OpenAI O1. Here, under the scope of LLMs, we formalize reasoning as a structured, multi-step process that dynamically decomposes complex problems, generates intermediate hypotheses, and iteratively refines solutions through logical and evidence-based transformations. Mathematically, let a reasoning process â„›â„›\\mathcal{R}caligraphic\_R be defined as a tuple âŸ¨ğ’¦p,ğ’¦r,ğ’®t,Î¦âŸ©subscriptğ’¦ğ‘subscriptğ’¦ğ‘Ÿsubscriptğ’®ğ‘¡Î¦\\langle\\mathcal{K}\_{p},\\mathcal{K}\_{r},\\mathcal{S}\_{t},\\Phi\\rangleâŸ¨ caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT , caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT , caligraphic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , roman\_Î¦ âŸ©, where ğ’¦psubscriptğ’¦ğ‘\\mathcal{K}\_{p}caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT denotes parametric knowledge embeddings, ğ’¦rsubscriptğ’¦ğ‘Ÿ\\mathcal{K}\_{r}caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT represents retrieved contextual knowledge, ğ’®t\={s0,s1,â€¦,sn}subscriptğ’®ğ‘¡subscriptğ‘ 0subscriptğ‘ 1â€¦subscriptğ‘ ğ‘›\\mathcal{S}\_{t}=\\{s\_{0},s\_{1},\\ldots,s\_{n}\\}caligraphic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = { italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦ , italic\_s start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT } constitutes the evolving state sequence with s0subscriptğ‘ 0s\_{0}italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT as the initial query and snsubscriptğ‘ ğ‘›s\_{n}italic\_s start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT as the final response, and Î¦:ğ’®iÃ—ğ’¦pÃ—ğ’¦râ†’ğ’®i+1:Î¦â†’subscriptğ’®ğ‘–subscriptğ’¦ğ‘subscriptğ’¦ğ‘Ÿsubscriptğ’®ğ‘–1\\Phi:\\mathcal{S}\_{i}\\times\\mathcal{K}\_{p}\\times\\mathcal{K}\_{r}\\rightarrow% \\mathcal{S}\_{i+1}roman\_Î¦ : caligraphic\_S start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT Ã— caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT Ã— caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT â†’ caligraphic\_S start\_POSTSUBSCRIPT italic\_i + 1 end\_POSTSUBSCRIPT defines the state transition function.

The reasoning process exhibits three defining characteristics. First, it is inherently multi-step, systematically decomposing complex problems into intermediate cognitive states (e.g., sub-question generation or temporary conclusions) rather than pursuing direct input-output mapping. Second, it generates novel knowledge or facts â€” synthesizing implicit relationships, deriving latent constraints, or reformulating problems in ways not explicitly present in the initial input or parametric memory (e.g., transforming â€Is A greater than B?â€ into comparative subquestions about A and Bâ€™s attributes). Crucially, these representations are not merely retrieved but dynamically constructed through the reasoning trajectory. Third, the process is teleological â€” its architecture and termination conditions are explicitly optimized for complex problem resolution, where complexity is measured by the necessity of state transitions or the insufficiency of direct retrieval from either parametric (ğ’¦psubscriptğ’¦ğ‘\\mathcal{K}\_{p}caligraphic\_K start\_POSTSUBSCRIPT italic\_p end\_POSTSUBSCRIPT) or external (ğ’¦rsubscriptğ’¦ğ‘Ÿ\\mathcal{K}\_{r}caligraphic\_K start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT) knowledge sources. This stands in stark contrast to atomic inference, which lacks such deliberate state construction and goal-aware iteration.

The distinction between reasoning and inference manifests most saliently in their computational signatures. While inference â„â„\\mathcal{I}caligraphic\_I constitutes a single-step conditional probability computation Pâ¢(y|x)\=âˆt\=1TPâ¢(yt|x,y<t)ğ‘ƒconditionalğ‘¦ğ‘¥superscriptsubscriptproductğ‘¡1ğ‘‡ğ‘ƒconditionalsubscriptğ‘¦ğ‘¡ğ‘¥subscriptğ‘¦absentğ‘¡P(y|x)=\\prod\_{t=1}^{T}P(y\_{t}|x,y\_{<t})italic\_P ( italic\_y | italic\_x ) = âˆ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT italic\_P ( italic\_y start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_x , italic\_y start\_POSTSUBSCRIPT < italic\_t end\_POSTSUBSCRIPT ), reasoning â„›â„›\\mathcal{R}caligraphic\_R implements a meta-process coordinating multiple inference calls through explicit state management â„›â¢(x)\=Î¦1âˆ˜Î¦2âˆ˜â‹¯âˆ˜Î¦nâ¢(x)â„›ğ‘¥subscriptÎ¦1subscriptÎ¦2â‹¯subscriptÎ¦ğ‘›ğ‘¥\\mathcal{R}(x)=\\Phi\_{1}\\circ\\Phi\_{2}\\circ\\cdots\\circ\\Phi\_{n}(x)caligraphic\_R ( italic\_x ) = roman\_Î¦ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT âˆ˜ roman\_Î¦ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT âˆ˜ â‹¯ âˆ˜ roman\_Î¦ start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ( italic\_x ). This multi-phase architecture enables systematic error correction through backtracking mechanisms and dynamic retrieval refinement â€” features fundamentally absent in conventional inference pipelines. The operational boundary emerges when state transitions involve explicit symbolic manipulation (equation restructuring in mathematical reasoning) or knowledge graph traversal (temporal reasoning over retrieved events), distinguishing true reasoning from mere multi-step inference.

{forest}

for tree= forked edges, growâ€™=0, draw, rounded corners, node options=align=center, calign=edge midpoint, font=, \[The Synergy of RAG and Reasoning, fill=black!10,rotate=90,text width=6cm,font=\[Purpose Â§[3](https://arxiv.org/html/2504.15909v2#S3 "3. The purpose of the synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=red!20,font=\[ Reasoning-Augmented Retrieval  
(Better Retrieval) Â§[3.1](https://arxiv.org/html/2504.15909v2#S3.SS1 "3.1. Reasoning-Augmented Retrieval â€£ 3. The purpose of the synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=3.5cm \[ ARMÂ (Chen etÂ al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)); AdaptiveRAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)); FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)); LevelRAGÂ (Zhang etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104)); OmniThinkÂ (Xi etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95)); PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)); SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)); UARÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)); O1-EmbedeerÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); ITER-RETGENÂ (Shao etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)); HiRAGÂ (Zhang etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103)); MetaRAGÂ (Zhou etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109)); MMOA-RAGÂ (Chen etÂ al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13)); ReZeroÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)); DeepResearcherÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)); ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)); FRAGÂ (Gao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)); PORAGÂ (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)); Insight-RAGÂ (Pezeshkpour and Hruschka, [2025](https://arxiv.org/html/2504.15909v2#bib.bib67)); ChainRAGÂ (Zhu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib111)); FG-RAGÂ (Hong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)); MCTS-RAGÂ (Hu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37)); REAPERÂ (Joshi etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47)); DeepNoteÂ (Wang etÂ al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)), text width=10.0cm, node options=align=left \] \] \[ Retrieval-Augmented Reasoning  
(Better Reasoning) Â§[3.2](https://arxiv.org/html/2504.15909v2#S3.SS2 "3.2. Retrieval-Augmented Reasoning â€£ 3. The purpose of the synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=3.5cm \[ ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)); AgenticReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)); CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)); CR-plannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)); DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)); DeepsolutionÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)); KBQA-O1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)); OpenRAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)); PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)); R1-SeacherÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)); RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)); ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)); ReSearchÂ (Chen etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7)); MedRAGÂ (Zhao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)); RAREÂ (Tran etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78)); RARE(Wang etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib91)); RetroRAGÂ (Xiao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96)); KG-RARÂ (Wu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94)); RetrievalPRMÂ (Zhu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110)); MRD-RAGÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)); Search-O1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)); StePO-RecÂ (Bi etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)), text width=10.0cm, node options=align=left \] \] \] \[Paradigms Â§[4](https://arxiv.org/html/2504.15909v2#S4 "4. Patterns of synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=orange!20,font=\[Pre-defined Workflow Â§Â [4.1](https://arxiv.org/html/2504.15909v2#S4.SS1 "4.1. Pre-defined workflow â€£ 4. Patterns of synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm \[Reasoning in Pre-Retrieval,text width=2.1cm \[ LeReTÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)); O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); planRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)); UARÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)); MMOA-RAGÂ (Chen etÂ al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13)); MedRAGÂ (Zhao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)); FRAGÂ (Gao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)); Insight-RAGÂ (Pezeshkpour and Hruschka, [2025](https://arxiv.org/html/2504.15909v2#bib.bib67)); ChainRAGÂ (Zhu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib111)); RetrievalPRMÂ (Zhu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110)); REAPERÂ (Joshi etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47)); AdaptiveRAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)), text width=8.2cm, node options=align=left \] \] \[Reasoning in Post-Retrieval,text width=2.1cm \[ ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)); ARMÂ (Chen etÂ al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)); MRD-RAGÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)); FG-RAGÂ (Hong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)); ToG2Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)), text width=8.2cm, node options=align=left \] \] \[Hybrid Reasoning,text width=2.1cm \[ IR-COTÂ (Trivedi etÂ al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79)); FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)); ITER-RETGENÂ (Shao etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)); LevelRAGÂ (Zhang etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104)); HiRAGÂ (Zhang etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103)); MetaRAGÂ (Zhou etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109)); RetroRAGÂ (Xiao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96)); KG-RARÂ (Wu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94)); DeepNoteÂ (Wang etÂ al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)), text width=8.2cm, node options=align=left \] \] \] \[Dynamic Workflow Â§[4.2](https://arxiv.org/html/2504.15909v2#S4.SS2 "4.2. Dynamic RAG Workflow â€£ 4. Patterns of synergy â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm \[Proactivity-Driven Reasoning,text width=2.2cm \[ AgenticReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)); DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)); CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)); Co-STORMÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)); PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)); Search-O1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)); R1-SearcherÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)), text width=8.1cm, node options=align=left \] \] \[Reflection-Driven Reasoning,text width=2.2cm \[ FlareÂ (Jiang etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib46)); OpenRAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)); WriteHereÂ (Xiong etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)); ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)); Self-RAGÂ (Asai etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)), text width=8.1cm, node options=align=left \] \] \[Feedback-Driven Reasoning,text width=2.2cm \[ SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)); CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)); MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)); DeepSolutionÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)); RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)); ReZeroÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)); ReSearchÂ (Chen etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7)); RAREÂ (Tran etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78)); DeepResearcherÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)); PORAGÂ (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)); MCTS-RAGÂ (Hu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37)); KBQA-O1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)), text width=8.1cm, node options=align=left \] \] \] \] \[Implementation Â§[5](https://arxiv.org/html/2504.15909v2#S5 "5. Implementation and Optimization â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.5cm, for tree=fill=blue!20,font=\[Resoning Method Â§[5.1](https://arxiv.org/html/2504.15909v2#S5.SS1 "5.1. Reasoning Process â€£ 5. Implementation and Optimization â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm, \[LLM/CoT, text width=2.0cm \[ ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)); AdaptiveRAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)); O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); DeepNoteÂ (Wang etÂ al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)); HiRAGÂ (Zhang etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103)); MetaRAGÂ (Zhou etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109)); MMOA-RAGÂ (Chen etÂ al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13)); RetroRAGÂ (Xiao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96)); PORAGÂ (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)); KG-RARÂ (Wu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94)); MRD-RAGÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)); PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)), text width=8.2cm, node options=align=left \] \] \[Special Token Prediction, text width=2.0cm \[ Self-RAGÂ (Asai etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)); SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)); OpenRAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)); R1-SearcherÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)); ReZeroÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)); ReSearchÂ (Chen etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7)); ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)); Search-O1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)), text width=8.2cm, node options=align=left \] \] \[Search-Driven Reasoning, text width=2.0cm \[ OminiThinkÂ (Xi etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95)); DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)); CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)); DeepSolutionÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)); ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)); KBQA-O1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)); WriteHereÂ (Xiong etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)); RAREÂ (Tran etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78)); MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)); StePO-RecÂ (Bi etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)), text width=8.2cm, node options=align=left \] \] \[Reasoning on Graph, text width=2.0cm \[ FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)); ToG2Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)); LighRAGÂ (Guo etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib28)); FRAGÂ (Gao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)); FG-RAGÂ (Hong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)); MedRAGÂ (Zhao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)), text width=8.2cm, node options=align=left \] \] \[External Solver,text width=2.0cm \[ ARMÂ (Chen etÂ al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)), text width=8.2cm, node options=align=left \] \] \] \[Optimization Â§[5.2](https://arxiv.org/html/2504.15909v2#S5.SS2 "5.2. Reasoning Optimization â€£ 5. Implementation and Optimization â€£ Synergizing RAG and Reasoning: A Systematic Review"), text width=2.8cm, for tree=fill=blue!20 \[Prompt-Based, text width=2.0cm \[ Co-STORMÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)); Agentic ReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)); FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)); PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)); HiRAGÂ (Zhang etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib103)); MetaRAGÂ (Zhou etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib109)); MedRAGÂ (Zhao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib106)); RAREÂ (Tran etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib78)); ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)); RetroRAGÂ (Xiao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib96)); FRAGÂ (Gao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)); KG-RARÂ (Wu etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib94)); MRD-RAGÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)); FG-RAGÂ (Hong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)); MCTS-RAGÂ (Hu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37)); DeepSolutionÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)); StePO-RecÂ (Bi etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)), text width=8.2cm, node options=align=left \] \] \[Tuning-Based, text width=2.0cm \[ KBQA-Q1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)); O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)); DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)); CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)); MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)); RetrievalPRMÂ (Zhu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110)); REAPERÂ (Joshi etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib47)); RARE(Wang etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib91)); UARÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)), text width=8.2cm, node options=align=left \] \] \[RL-Based, text width=2.0cm \[PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)); LeReTÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)); RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)); ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)); SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)); CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)); DeepRetrievalÂ (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43)); DeepNoteÂ (Wang etÂ al., [2024e](https://arxiv.org/html/2504.15909v2#bib.bib85)); MMOA-RAGÂ (Chen etÂ al., [2025e](https://arxiv.org/html/2504.15909v2#bib.bib13)); ReZeroÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)); ReSearchÂ (Chen etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib7)); DeepResearcherÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)); PORAGÂ (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)); R1-SearchÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)), text width=8.2cm, node options=align=left \] \] \] \] \]

Figure 3. A structured taxonomy of synthesizing RAG and Reasoning.

### 2.2. Taxonomy

Integrating RAG with reasoning marks a paradigm shift in tackling complex knowledge-intensive tasks. This work develops a hierarchical taxonomy (FigureÂ [3](https://arxiv.org/html/2504.15909v2#S2.F3 "Figure 3 â€£ 2.1. Definition â€£ 2. Overview â€£ Synergizing RAG and Reasoning: A Systematic Review")) based on three key questions: why reasoning is needed with RAG (Purpose), how they structurally interact (Paradigm), and what methods enable effective integration (Implementation). This framework guides readers through the technical innovations in later chapters, providing a clear conceptual path without premature technical details, and highlighting the fieldâ€™s evolutionary logic, avoiding delving prematurely into specific technical details.

#### 2.2.1. Synergy Purpose

Integrating reasoning with RAG addresses the limitations of traditional RAG systems, which struggle with multi-step logic, contextual adaptation, and implicit knowledge synthesis due to reliance on superficial semantic matching and fixed knowledge limits. Adding reasoning enables dynamic retrieval planning, logical verification of evidence, and insight generation beyond retrieved data through abductive or counterfactual reasoning. At the same time, the introduction of external knowledge retrieval also helps alleviate reasoning interruptions caused by the knowledge limitations of LRM and reduces the likelihood of hallucinations. This integration occurs in two main ways: Reasoning-Augmented Retrieval, where inference drives context-aware information gathering; Retrieval-Augmented Reasoning, where external knowledge supports and expands the modelâ€™s deductive abilities.

#### 2.2.2. Synergy Paradigms

Building upon the above necessity, our taxonomy categorizes RAG+Reasoning systems along the axis of procedural dynamism. Pre-defined workflows employ fixed templates that systematically alternate between retrieval and reasoning phases, with intervention points predetermined at _pre-retrieval reasoning_ (e.g., query decomposition), _post-retrieval reasoning_ (e.g., evidence synthesis), or _hybrid stages_. While offering operational transparency, these architectures exhibit limited adaptability to emergent task complexities. In contrast, dynamic workflows implement state-contingent reasoning processes where retrieval actions are conditionally triggered through continuous system introspection. This paradigm further branches into _Proactivity-Driven_ strategies (self-initiated knowledge requests), _Reflection-driven_ mechanisms (error-corrective retrieval based on intermediate result analysis), and _Feedback-driven_ approaches (environmental reward signals or external model evaluations). The progression from static to dynamic architectures reflects the fieldâ€™s maturation toward human-like contextual adaptation in open-world problem-solving.

#### 2.2.3. Synergy Implementation

Operationalizing these synergies requires innovations across reasoning and retrieval strategies. Foundational reasoning architectures span LLM-Based like COT, search-based hypothesis generation (tree search, Monte Carlo methods), symbolic solver integration, and graph-structured multi-hop inference. These capabilities are further enhanced through three principal augmentation strategies: prompt-based techniques that utilize natural language templates and special token (e.g., Â¡PlanÂ¿, Â¡VerifyÂ¿) to steer model behavior, tuning-based methods that inject domain-specific knowledge or distill reasoning capability, and RL-based frameworks that optimize retrieval-reasoning policies through outcome reward models (ORM) or process reward models (PRM). The alignment between these methodologies and the proposed taxonomy is criticalâ€”static workflows predominantly rely on predictable prompt-guided reasoning chains, whereas dynamic systems increasingly integrate search-based exploration or solver-augmented strategies to navigate evolving state spaces.

Overall, this tripartite taxonomyâ€”motivational drivers, architectural paradigms, and implementation methodologiesâ€”establishes a unified lens for analyzing RAG+Reasoning systems. Subsequent chapters will elaborate on each stratum, progressively revealing how these conceptual distinctions translate into technical innovations that push the boundaries of machine intelligence.

## 3\. The purpose of the synergy

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/purpose.png)

Figure 4. The purpose of the synergy between RAG and reasoning

The integration of RAG and reasoning marks a crucial advancement in enhancing LLMsâ€™ problem-solving abilities. Their true potential lies not in isolated use but in their synergy, which overcomes key limitations in retrieval and reasoning. This section explains the main motivations for combining RAG with reasoning, emphasizing two primary benefits: (1) enhancing retrieval accuracy and flexibility through reasoning, and (2) reinforcing complex reasoning by using context-rich retrieved knowledge. FigureÂ [4](https://arxiv.org/html/2504.15909v2#S3.F4 "Figure 4 â€£ 3. The purpose of the synergy â€£ Synergizing RAG and Reasoning: A Systematic Review") illustrates these collaborative aims and the limitations they address.

The first key benefit is Reasoning-Augmented Retrieval, where reasoning improves the retrieval process. Traditional RAG systems struggle with query formulation, relevance assessment, and iterative refinementâ€”tasks needing logical and contextual analysis. Reasoning enables adaptive retrieval through dynamic query expansion, ambiguity resolution, and multi-hop evidence aggregation, overcoming the limits of keyword- or embedding-based methods and aligning retrieval with the taskâ€™s reasoning demands.

The second benefit is Retrieval-Augmented Reasoning, where external knowledge supplements the limitations of purely parametric LLM reasoning. Even advanced models face hallucination, knowledge gaps, and compositional challenges alone. Retrieval grounds reasoning in up-to-date, domain-specific, or rare information absent from model weights, crucial for explainability, multi-step deduction, and integrating diverse sources.

Together, combining RAG and reasoning fills fundamental gaps in both techniques. By enhancing retrieval via reasoning and strengthening reasoning through retrieval, it broadens LLMsâ€™ capacity to address complex real-world problems.

### 3.1. Reasoning-Augmented Retrieval

Reasoning-Augmented Retrieval (RAR) represents a significant advancement in information retrieval by integrating multi-step reasoning to dynamically enhance retrieval quality. Unlike traditional methods that depend on static semantic matching, RAR creates a cognitive feedback loop mimicking human iterative reasoning, surpassing the limitations of simple â€query-documentâ€ interactions.

RARâ€™s effectiveness stems from several key features. It often uses on-demand retrieval, where reasoningâ€”evaluating intent clarity, knowledge state, and temporal factorsâ€”guides adaptive search initiation, reducing redundancies present in fixed triggers (e.g., UARâ€™s classifierÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))). It improves semantic alignment by inferring implicit query logic such as business rules or entity relationships to generate precise retrieval requests aligned with data schemas (e.g., PlanRAGâ€™s plan-retrieval loopsÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49))). RAR also applies multi-step iterative refinement, using intermediate reasoning outputs (e.g., chain-of-thought, partial answersÂ (Trivedi etÂ al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79))) to recursively reformulate queries in a closed-loop system essential for resolving multi-hop dependenciesÂ (Shao etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)). Furthermore, it adapts to specific domains by tailoring retrieval to vertical contexts (e.g., financial or medical) and balances efficiency and precision through lightweight reasoning strategies (e.g., AdaptiveRAGâ€™s complexity-based selectionÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42))).

Traditional retrieval systems, effective for simple queries, struggle with complex information needs due to rigid designs favoring static matching over dynamic reasoning, limiting their adaptability to changing contexts and diverse data. RAR primarily addresses five core challenges inherent in these conventional methods.

#### 3.1.1. Semantic Disparities Between Queries and Documents

A key challenge lies in the mismatch between user queries and documentsâ€”whether due to differing expression styles (professional jargon vs. casual language) or implicit contextual gapsâ€”making direct semantic matching unreliable. Importantly, high similarity does not guarantee true relevance, as documents may share keywords or surface features without addressing the underlying intent or logic of the query. Retrieval models must therefore understand deeper semantics beyond superficial similarity.Domain adaptation further complicates this issue. To overcome these gaps, approaches such as reasoning-augmented embeddings (O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) enriches queries with inferred â€œthinkingâ€ text), feedback-driven rewriting (SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) dynamically refines queries based on retrieved results), and pre-planning (PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) extracts business rules to generate SQL queries aligned with database schemas) help better capture domain-specific semantics and ensure relevance beyond mere similarity.

#### 3.1.2. Inflexible Intent Disambiguation

Traditional RAG methods rely on fixed embedding similarity strategies , which fail to dynamically interpret the implicit intent behind complex queries (e.g., multi-hop reasoning or domain-specific requirements). User queries often exhibit semantic complexity that far exceeds their surface textâ€”for instance, a request to â€optimize supply chain costsâ€ may require correlating disparate database fields not explicitly mentioned. Static retrieval methods lack the adaptability to capture such dynamically evolving information needs. A critical limitation lies in intent dynamicity: as contextual understanding expands, traditional systems generate fixed retrieval results based solely on the initial query. Furthermore, semantic representation limitations of dense retrieval models (e.g., BERT-based models) hinder their ability to encode intricate semantic relationships (e.g., irony, metaphors), leading to misaligned results. Current approaches attempt to mitigate these issues through multi-step intent decomposition (e.g., LevelRAGâ€™s high-level searcher breaks complex queries into multi-hop sub-queriesÂ (Zhang etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104))) and dynamic query reformulation (e.g., LeReTâ€™s reinforcement learning generates diversified query candidatesÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35))), iteratively refining retrieval strategies to align with document content.

#### 3.1.3. Inefficient Coordination of Multi-Source Heterogeneous Data

Retrieval from diverse sourcesâ€”text, tables, graphs, web, and APIsâ€”often produces fragmented results due to a lack of global reasoning. The key challenge is modal heterogeneity: different retrieval techniques (dense retrieval for text, SQL for tables, GQL for graphs) operate independently without unified coordination. For example, experiments show standard RAG methods (like dense retrieval with query decomposition) yield only 32.7% perfect recall and 40.9% F1 on the OTT-QA dataset. These outcomes reveal the limitations of traditional approaches in aligning textual queries with structured tablesâ€”such as failing to link concepts like â€K-12 student free ratesâ€ in text to related â€education expenditureâ€ columns when not explicitly mentioned. Additionally, disconnected entity matching (e.g., relating â€company revenueâ€ in text to financial tables) worsens inefficiencies, as conventional methods depend on semantic similarity and overlook domain-specific relationships and exact-value matches. Advanced techniquesâ€”such as reasoning-driven alignment (ARMâ€™s N-gram constraints for cross-modal entity decodingÂ (Chen etÂ al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8))) and unified semantic spaces (LevelRAGâ€™s shared multi-modal representationsÂ (Zhang etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib104)))â€”enable more effective, integrated retrieval.

#### 3.1.4. Incompleteness and Incoherence in Complex Retrieval Tasks

Single-step retrieval systems fall short in complex multi-hop reasoning tasks, such as deducing entity chains or conducting decision analysis. Traditional static retrieval conflicts with multi-step cognitive needs, resulting in three main issues: 1) Path dependency, where later retrievals rely on information from earlier steps (e.g., finding â€the most populous county in Californiaâ€ before its education policies), but conventional systems lack state management; 2) Error propagation,early retrieval errors cause mistakes in intermediate results, which then affect the next round of retrieval; 3) Semantic inflexibility of fixed queries, which cannot adapt to dynamic concepts like entity aliases or relational predicates.

Advanced methods address these flaws through integrated strategies. PlanRAG uses iterative â€plan-retrospect-replanâ€ cycles to trigger sub-queries when gaps arise. Reinforcement learning in LeReT improves query generation via reward-driven path selection. Likewise, ITER-RETGEN rebuilds follow-up queries using intermediate answers (e.g., â€award recipientâ€™s heightâ€) to resolve multi-hop dependencies.

#### 3.1.5. Trade-offs Between Retrieval Efficiency and Precision

Complex scenarios face a tension between exhaustive retrieval, which is computationally costly, and restricted retrieval, which risks information loss. Expanding retrieval blindly inflates costs (e.g., LLM API calls) without ensuring relevance. Simple queries suffer from unnecessary multi-step retrieval, wasting resources, while complex queries face quality risks if retrieval is too limited. Adaptive approaches like complexity-aware routing (Adaptive-RAGâ€™s lightweight classifier allocates retrieval budgetsÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42))) and cost-sensitive training (SmartRAGâ€™s reinforcement learning balances quality and stepsÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21))) dynamically manage this trade-off.

In summary, Reasoning-Augmented Retrieval overcomes traditional RAGâ€™s limitations in dynamic triggering, semantic alignment, multi-hop support, domain adaptation, and efficiency trade-offs by deeply integrating reasoning into the retrieval process. Its key innovation is a bidirectional enhancement between reasoning and retrievalâ€”reasoning refines retrieval strategies, while retrieval supports iterative reasoningâ€”jointly boosting accuracy and efficiency in complex information tasks.

### 3.2. Retrieval-Augmented Reasoning

Retrieval-Augmented Reasoning (ReAR) combines external knowledge retrieval with inherent model reasoning to overcome failures from knowledge gaps or logical discontinuities in complex tasks. Unlike traditional RAG methods that retrieve information once, ReAR uses an iterative, context-sensitive retrieval that continuously provides relevant data to support multi-step reasoning. This approach is crucial for tasks needing strict logic, such as mathematical proofs, where intermediate steps require specific theorems or lemmas. By making retrieval an adaptive, ongoing process rather than a one-time step, ReAR strengthens each reasoning stage with accurate, current information, improving the overall inferenceâ€™s reliability and robustness.

ReARâ€™s core feature is dynamic knowledge supplementation, generating retrieval queries in real-time based on the evolving reasoning context. This overcomes the limits of single-round retrieval by enabling knowledge refinement at each step, as seen in process supervision frameworks like RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)). ReAR also improves reasoning paths using methods like search space compressionâ€”for example, MCTS-guided heuristics in KBQAâ€”and structured feedback from diverse sources like knowledge graphsÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)). These techniques maintain logical consistency while reducing irrelevant or conflicting information. Importantly, ReAR adapts well across domains, supporting precise knowledge retrieval and tool use for specialized tasks such as industrial problem-solving in PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)) or scientific reasoningÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)).

By integrating retrieval as an active part of the reasoning loop, ReAR addresses LLMsâ€™ temporal and depth constraints, ensuring adherence to domain-specific and time-sensitive requirements. This close coupling turns external knowledge into an on-demand resource, creating a closed-loop system that enhances the modelâ€™s ability to handle complex, knowledge-intensive problems. Specifically, ReAR seeks to address the following limitations and challenges:

#### 3.2.1. Knowledge Gap in Multi-step Reasoning

In long-range reasoning, missing intermediate knowledge often breaks logical chains, especially in industrial and scientific contexts requiring multi-source data integration (e.g., text, tables, time-series). Static retrieval methods worsen this by not adapting to the reasoning processâ€™s changing needs. ReAR techniques address this with chained retrieval, as in CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)), which breaks multi-hop questions into sequential sub-queries (e.g., retrieving â€event causesâ€ then their â€impactsâ€), systematically linking knowledge. Reasoning-state-aware retrieval, used in FLAREÂ (Jiang etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib46)), predicts future information needs by generating interim prompts (e.g., â€the next step requires discussion of â€¦â€), enabling dynamic query construction that preserves coherence. Together, these approaches resolve the conflict between discrete retrieval and continuous reasoning.

#### 3.2.2. Reasoning Discontinuity Caused by Domain Knowledge Boundaries

Reasoning discontinuity arises from LLMsâ€™ limited knowledge, struggling with specialized domains (e.g., semiconductor design in PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83))) and real-time data (e.g., medical parameters in Agentic ReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93))). End-to-end models often produce factual errors, while traditional RAG methods fail to retrieve deep professional knowledge due to coarse retrieval, especially with complex data like tables,charts and images.

ReAR addresses this with two complementary solutions: knowledge atomization and structural organization, as in PIKEâ€™s decomposition of documents into fine-grained units and multi-layer knowledge graphs for semantic and logical retrieval; and dynamic tool integration, as in Agentic Reasoningâ€™s real-time data acquisition via code execution and API calls to compute critical indicators (e.g., medical FiO2). These innovations overcome the challenges of specialized knowledge depth and timely information relevance that limit conventional methods.

#### 3.2.3. Search Space Explosion and Local Optima Traps

The main challenge in multi-step reasoning is the exponential growth of the search space, where methods like Chain-of-Thought (CoT) often yield suboptimal or inconsistent results due to unconstrained hypotheses. Traditional approaches like CoT and Tree-of-Thought (ToT) lack external knowledge constraints, causing invalid assumptions, while purely symbolic reasoning falls short in open-domain tasks. To address this, two strategies are used: knowledge base-anchored heuristic search (KBQA-O1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59))), which limits reasoning actions to subgraphs in knowledge graphs, and a retrieval-verification mechanism (Search-o1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52))) that prunes unsupported reasoning paths using evidence from the knowledge base. Together, these reduce the search space and preserve reasoning coherence.

#### 3.2.4. Dynamic Knowledge Requirements in Multi-Step Reasoning

Complex multi-step reasoning tasks face the challenge of continuously changing knowledge requirements. This is evident in cases like multi-hop reasoning and engineering planning, where each stage generates new sub-problems (e.g., moving from â€architectural designâ€ to â€material cost estimationâ€). Static knowledge bases or one-time retrieval methods cannot meet this evolving demand. This manifests in two ways: initial knowledge may miss later needs, causing gaps; and fixed knowledge sets may include irrelevant information, reducing reasoning accuracy. To address this, new retrieval-augmented reasoning approaches introduce dynamic solutions: process supervision (e.g., reward models in RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97))) detects knowledge gaps in real time, atomic decision-making (e.g., step decomposition in DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25))) triggers retrieval as needed, and tree-like expansions (e.g., multi-path retrieval in DeepSolutionÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55))) enable parallel exploration. By integrating knowledge retrieval within reasoning, these methods let the system identify, supplement, and verify knowledge dynamicallyâ€”much like a human expertâ€”greatly enhancing the reliability and completeness of complex reasoning.

#### 3.2.5. Insufficient Depth and Breadth of Reasoning

This issue is prominent in expert tasks like medical diagnosis, legal analysis, and research report generation. LLMsâ€™ static knowledge often fails to capture the evolving scope of domain knowledge, resulting in shallow reasoning that misses multi-level, cross-domain connections. For example, when assessing â€Company A is affected by economic recession,â€ traditional methods rely on superficial statistical patterns and cannot systematically follow the deeper logical chain from â€Company A â†’ industry supply chain â†’ macroeconomic policy â†’ international political landscape,â€ leading to reasoning that lacks causal depth.

To overcome this, recent advances use structured, retrieval-enhanced frameworks. ToG2.0Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) models Knowledge Graph relational paths as retrieval guidance vectors, enabling targeted queries along entity paths, surpassing the limits of keyword-based retrieval. This approach complements CR-Plannerâ€™sÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) iterative expansion, which triggers retrieval of specialized knowledge (e.g., textbook proofs of algorithm complexity) at critical reasoning points, ensuring accurate domain knowledge integration via multi-round validation. Addressing cross-domain knowledge linkage, CO-STORMÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) employs a multi-agent system whose host module generates cross-modal retrieval commands by analyzing potential semantics in uncited documents.

## 4\. Patterns of synergy

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/pattern.png)

Figure 5. Patterns of Synergy between RAG and Reasoning

SectionÂ [3](https://arxiv.org/html/2504.15909v2#S3 "3. The purpose of the synergy â€£ Synergizing RAG and Reasoning: A Systematic Review") detailed the need and motivation for integrating RAG with reasoning. Building on this, this section presents two core implementation patterns for RAG-reasoning synergy (FigureÂ [5](https://arxiv.org/html/2504.15909v2#S4.F5 "Figure 5 â€£ 4. Patterns of synergy â€£ Synergizing RAG and Reasoning: A Systematic Review")): (1) the Pre-defined Workflow, which uses logical architectures with preset rules for coordination, and (2) Dynamic Workflow, which relies on context-aware, adaptive coordination via real-time decision engines. These patterns illustrate current frameworks combining knowledge retrieval and multi-step reasoning from deterministic and flexible perspectives.

### 4.1. Pre-defined workflow

Pre-defined workflow is a multi-step reasoning approach with a fixed architecture and sequential execution, emphasizing process clarity and operational determinism. It consists of predefined iterative stages, each with strict input-output rules and no dynamic changes based on intermediate results. This modular design ensures controllability and structured reasoning for complex tasks. All steps are executed regardless of intermediate outcomes, guaranteeing repeatability and stability while avoiding uncertainties from dynamic decisions. Although it sacrifices adaptability, this approach offers procedural predictability and is well-suited for scenarios demanding clear reasoning paths, albeit with possible computational redundancy due to lack of real-time adjustments.

Mathematically, the pre-defined RAG workflow can be formalized as a deterministic multi-step operational chain. Given an input query Qğ‘„Qitalic\_Q and a predefined sequence of Nğ‘Nitalic\_N reasoning steps and the final decision output Dğ·Ditalic\_D, the complete workflow is expressed as:

(1)

D\=fNâˆ˜â‹¯âˆ˜f2âˆ˜f1â¢(Q)ğ·subscriptğ‘“ğ‘â‹¯subscriptğ‘“2subscriptğ‘“1ğ‘„D=f\_{N}\\circ\\cdots\\circ f\_{2}\\circ f\_{1}(Q)italic\_D = italic\_f start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT âˆ˜ â‹¯ âˆ˜ italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT âˆ˜ italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ( italic\_Q )

where each fiâˆˆ{Î¨,R,Î“}subscriptğ‘“ğ‘–Î¨ğ‘…Î“f\_{i}\\in\\{\\Psi,R,\\Gamma\\}italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT âˆˆ { roman\_Î¨ , italic\_R , roman\_Î“ } denotes strictly defined functions for reasoning (Î¨Î¨\\Psiroman\_Î¨), retrieval (Rğ‘…Ritalic\_R), or decision-making (Î“Î“\\Gammaroman\_Î“), with âˆ˜\\circâˆ˜ representing function composition. This formulation adheres to the fixed mapping sequence Qâ†¦Î¨â¢(Q)â†¦Râ¢(Î¨â¢(Q))â†¦Î“â¢(Râ¢(Î¨â¢(Q)))maps-toğ‘„Î¨ğ‘„maps-toğ‘…Î¨ğ‘„maps-toÎ“ğ‘…Î¨ğ‘„Q\\mapsto\\Psi(Q)\\mapsto R(\\Psi(Q))\\mapsto\\Gamma(R(\\Psi(Q)))italic\_Q â†¦ roman\_Î¨ ( italic\_Q ) â†¦ italic\_R ( roman\_Î¨ ( italic\_Q ) ) â†¦ roman\_Î“ ( italic\_R ( roman\_Î¨ ( italic\_Q ) ) ), exhibiting Markovian properties where ft+1subscriptğ‘“ğ‘¡1f\_{t+1}italic\_f start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT depends solely on ftsubscriptğ‘“ğ‘¡f\_{t}italic\_f start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPTâ€™s output while remaining independent of historical states {f<t}subscriptğ‘“absentğ‘¡\\{f\_{<t}\\}{ italic\_f start\_POSTSUBSCRIPT < italic\_t end\_POSTSUBSCRIPT }. The chained composition guarantees process closure and reproducibility, though constrained by the static combinatorial nature of {fi}i\=1Nsuperscriptsubscriptsubscriptğ‘“ğ‘–ğ‘–1ğ‘\\{f\_{i}\\}\_{i=1}^{N}{ italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT } start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT.

In the pre-defined pipeline, based on the position where reasoning is introduced, it can be further divided into Pre-Retrieval, Post-Retrieval, and Hybrid.

#### 4.1.1. Pre-Retrieval Reasoning

For pre-retrieval methods, the sequence is explicitly defined as

(2)

D\=Î“âˆ˜â„›âˆ˜Î¨â¢(Q)ğ·Î“â„›Î¨ğ‘„D=\\Gamma\\circ\\mathcal{R}\\circ\\Psi(Q)italic\_D = roman\_Î“ âˆ˜ caligraphic\_R âˆ˜ roman\_Î¨ ( italic\_Q )

where Î¨Î¨\\Psiroman\_Î¨ denotes a reasoning operator that systematically transforms or enriches the query prior to retrieval. This paradigm enhances retrieval precision by resolving ambiguities, inferring implicit intents, or optimizing query representations. Current research identifies four principal methodological categories for designing Î¨Î¨\\Psiroman\_Î¨:

Query Optimization focuses on generating and selecting query variants to maximize retrieval relevance. Mathematically, this is formalized as Candidates\=Generateâ¢(Q,C)CandidatesGenerateğ‘„ğ¶\\text{Candidates}=\\text{Generate}(Q,C)Candidates = Generate ( italic\_Q , italic\_C ), Î¨Optimizeâ¢(Q,C)\=argâ¡maxcandidateâˆˆCandidatesâ¡Scoreâ¢(candidate)subscriptÎ¨Optimizeğ‘„ğ¶subscriptcandidateCandidatesScorecandidate\\quad\\Psi\_{\\text{Optimize}}(Q,C)=\\arg\\max\_{\\text{candidate}\\in\\text{Candidates% }}\\text{Score}(\\text{candidate})roman\_Î¨ start\_POSTSUBSCRIPT Optimize end\_POSTSUBSCRIPT ( italic\_Q , italic\_C ) = roman\_arg roman\_max start\_POSTSUBSCRIPT candidate âˆˆ Candidates end\_POSTSUBSCRIPT Score ( candidate ), where (Generate) produces candidate queries and (argâ¡max\\arg\\maxroman\_arg roman\_max) selects optimal variants based on contrastive training or reinforcement learning. Representative implementations, such as LeReTÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)), leverage iterative sampling and optimization to balance query diversity and specificity.

Attribute Judgment employs classification mechanisms to dynamically regulate retrieval triggers. This is modeled as Î¨Classifyâ¢(Q)\=Classifyâ¢(Q)subscriptÎ¨Classifyğ‘„Classifyğ‘„\\Psi\_{\\text{Classify}}(Q)=\\text{Classify}(Q)roman\_Î¨ start\_POSTSUBSCRIPT Classify end\_POSTSUBSCRIPT ( italic\_Q ) = Classify ( italic\_Q ), where Classify evaluates query attributes (e.g., temporal sensitivity, intent complexity) against predefined criteria. Frameworks like UARÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15)) and AdaptiveRAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) exemplify this approach by integrating multi-stage classifiers to minimize unnecessary retrievals.

Plan Generation decomposes complex queries into structured sub-task sequences to guide retrieval direction. Formulated as Î¨Planâ¢(Q)\=Planâ¢(Q)subscriptÎ¨Planğ‘„Planğ‘„\\Psi\_{\\text{Plan}}(Q)=\\text{Plan}(Q)roman\_Î¨ start\_POSTSUBSCRIPT Plan end\_POSTSUBSCRIPT ( italic\_Q ) = Plan ( italic\_Q ), the operator Plan generates hierarchical task decompositions, as seen in PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) , which utilizes chain-of-thought reasoning to align retrieval targets with multi-step problem-solving requirements.

Semantic Enhancement enriches query representations using domain-specific or task-aware embeddings. Expressed as Î¨Enhanceâ¢(Q)\=Encodeâ¢(Q,ğ’¦)subscriptÎ¨Enhanceğ‘„Encodeğ‘„ğ’¦\\Psi\_{\\text{Enhance}}(Q)=\\text{Encode}(Q,\\mathcal{K})roman\_Î¨ start\_POSTSUBSCRIPT Enhance end\_POSTSUBSCRIPT ( italic\_Q ) = Encode ( italic\_Q , caligraphic\_K ), where ğ’¦ğ’¦\\mathcal{K}caligraphic\_K denotes auxiliary knowledge (e.g., reasoning trajectories), methods like O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) integrate latent reasoning patterns into query embeddings to improve retrieval robustness.

Collectively, these methodologies demonstrate that pre-retrieval reasoning serves as a systematic interface to mitigate semantic gaps between raw queries and knowledge bases, establishing a critical component for precision-driven RAG architectures.

#### 4.1.2. Post-Retrieval Reasoning

In pre-defined RAG systems with multi-step reasoning pipelines, the post-retrieval reasoning paradigm represents a critical advancement where cognitive processing occurs after information retrieval from external sources. This approach addresses inherent limitations in conventional RAG, particularly in managing knowledge conflicts, mitigating information insufficiency, and enhancing logical consistency across complex reasoning tasks. Mathematically, this process can be formalized as a deterministic function composition:

(3)

D\=Î“âˆ˜Î¨âˆ˜â„›â¢(Q)ğ·Î“Î¨â„›ğ‘„D=\\Gamma\\circ\\Psi\\circ\\mathcal{R}(Q)italic\_D = roman\_Î“ âˆ˜ roman\_Î¨ âˆ˜ caligraphic\_R ( italic\_Q )

â„›â„›\\mathcal{R}caligraphic\_R denotes the retrieval operator, Î¨Î¨\\Psiroman\_Î¨ implements the reasoning transformation, and Î“Î“\\Gammaroman\_Î“ represents the final decision function.

The core characteristic of Post-Retrieval Reasoning lies in its execution of the reasoning process after retrieval, with the reasoning target being the retrieved content. ToG2.0Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) proposes an iterative multi-step reasoning framework that alternates between graph retrieval and context retrieval, integrating the reasoning judgment of LLMs to progressively expand entities and prune irrelevant information, ultimately generating accurate answers. This approach dynamically addresses the issue of insufficient information through iterative refinement while establishing a dual-evidence verification mechanism via knowledge graph relation pruning and entity-guided context retrieval. Its graph-structured reasoning module transforms the connectivity validation of triple paths into a constraint satisfaction problem, effectively mitigating logical inconsistencies between text fragments and thereby significantly improving the quality of complex question answering.

ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)), on the other hand, employs a predefined three-stage process (Self-Inquiry â†’ Knowledge Assimilation â†’ Thought Accommodation) to structurally comprehend and calibrate retrieved knowledge, resolving conflicts between parametric memory and external knowledge. During the Knowledge Assimilation stage, ActiveRAG enhances the corrective effect of external knowledge on the internal representations of LLMs through multi-instruction fine-tuning strategies (e.g., counterfactual comparison and anchor association), substantially reducing the likelihood of hallucination generation. ARMâ€™sÂ (Chen etÂ al., [2025f](https://arxiv.org/html/2504.15909v2#bib.bib8)) structural alignment and self-verification stages also demonstrate optimization for post-retrieval reasoning. By incorporating domain knowledge via mixed-integer programming (MIP) solvers, ARM ensures the rationality and coverage of retrieval results, providing a scalable optimization framework for multi-source data compatibility and thereby enabling globally optimal cross-modal retrieval.

#### 4.1.3. Hybrid Reasoning

The Hybrid pattern of pre-defined process forms a composite processing paradigm by integrating pre-retrieval reasoning with post-retrieval reasoning. The essence is formalized as a multi-round recursive iterative process, where each iteration cycle strictly comprises three phases: Retrieval, Generation, and Reasoning, executed as structured composite operations. Let the total number of iterations be Tğ‘‡Titalic\_T; the workflow is defined as:

(4)

QT\=(â—‹t\=1Tâ„›ğ“‰âˆ˜Î“tâˆ˜Î¨t)(Q0)Q\_{T}=\\left(\\bigcirc\_{t=1}^{T}\\mathcal{R\_{t}}\\circ\\Gamma\_{t}\\circ\\Psi\_{t}% \\right)(Q\_{0})italic\_Q start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT = ( â—‹ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT caligraphic\_R start\_POSTSUBSCRIPT caligraphic\_t end\_POSTSUBSCRIPT âˆ˜ roman\_Î“ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âˆ˜ roman\_Î¨ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ( italic\_Q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT )

Here, each iterative unit is indexed by tğ‘¡titalic\_t. The process terminates when a predefined condition ğ’¯â¢(Qt,Dt,Ct)ğ’¯subscriptğ‘„ğ‘¡subscriptğ·ğ‘¡subscriptğ¶ğ‘¡\\mathcal{T}(Q\_{t},D\_{t},C\_{t})caligraphic\_T ( italic\_Q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) is met, yielding the final response Î“finalâ¢(CT)subscriptÎ“finalsubscriptğ¶ğ‘‡\\Gamma\_{\\text{final}}(C\_{T})roman\_Î“ start\_POSTSUBSCRIPT final end\_POSTSUBSCRIPT ( italic\_C start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT ). This recursive mechanism enables dynamic synergy between knowledge acquisition and semantic inference, overcoming the linear limitations of single-cycle retrieval-generation frameworks.

IR-CoTÂ (Trivedi etÂ al., [2022a](https://arxiv.org/html/2504.15909v2#bib.bib79)) leverages chain-of-thought reasoning to iteratively construct intermediate logic chains, enabling multi-hop retrieval guided by progressively refined contextual cues. FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) introduces a dual-phase architecture that first generates structured search graphs to model temporal and entity dependencies, followed by dynamic query rewriting to optimize financial data retrieval. LevelRAG employs hierarchical validation mechanisms, aggregating multi-granular retrieval results and triggering supplementary retrievals based on context completeness assessments. ITER-RETGENÂ (Shao etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib69)) utilizes generation-enhanced feedback loops to iteratively refine query representations, enhancing semantic alignment between retrieval and generation phases.

These approaches share a common foundation in structured recursion while diverging in operational mechanisms. By enforcing deterministic iteration cycles, they balance controlled workflow execution with adaptive semantic exploration, addressing challenges such as multi-step reasoning, temporal coherence, and cross-domain knowledge synthesis. The hybrid paradigmâ€™s strength lies in its capacity to decompose complex queries into iterative retrieval-generation units, systematically bridging knowledge gaps while maintaining interpretability and robustness in open-domain problem-solving scenarios.

### 4.2. Dynamic RAG Workflow

The RAG with dynamic workflow represents an autonomous reasoning architecture centered around LLMs, characterized by the integration of non-deterministic operational workflows and real-time decision-making capabilities. Unlike pre-defined pipelines, this architecture enables continuous monitoring of reasoning states to dynamically trigger retrieval, generation, or verification operations. The LLM actively evaluates contextual demands during reasoning processes, autonomously determining optimal moments for invoking external tools or resources through a hybrid feedback coordination mechanism. By eliminating fixed iterative units and pre-determined tool-calling sequences, the framework achieves dynamic evolution of execution pathways, demonstrating superior adaptability in complex cognitive tasks through real-time adjustment of computational workflows based on intermediate reasoning outcomes.

This dynamic architecture manifests three principal characteristics: 1) Operator invocation is governed by the LLMâ€™s contextual state analysis, exemplified through special token prediction (e.g., â€˜\[Web-Search\]â€˜ or â€˜Â¡begin\_of\_queryÂ¿â€˜) to initiate external operations; 2) Reasoning trajectories exhibit high flexibility, allowing dynamic query reformulation and sub-problem generation to overcome limitations of static workflows; 3) Context-driven decision mechanisms prioritize real-time reasoning states over predefined rules, enhancing systemic responsiveness to emergent task complexities while improving precision.

Defining the reasoning state at time tğ‘¡titalic\_t as St\=(Ht,Ct)subscriptğ‘†ğ‘¡subscriptğ»ğ‘¡subscriptğ¶ğ‘¡S\_{t}=(H\_{t},C\_{t})italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = ( italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ), where Htsubscriptğ»ğ‘¡H\_{t}italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT denotes historical information aggregation and Ctsubscriptğ¶ğ‘¡C\_{t}italic\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT represents contextual embedding vectors, the decision process is modeled as a stochastic system:

(5)

at+1âˆ¼Ï€â¢(St;Î˜)similar-tosubscriptğ‘ğ‘¡1ğœ‹subscriptğ‘†ğ‘¡Î˜a\_{t+1}\\sim\\pi(S\_{t};\\Theta)italic\_a start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT âˆ¼ italic\_Ï€ ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ; roman\_Î˜ )

(6)

St+1\=Î´â¢(St,ğ’¯at+1â¢(St))subscriptğ‘†ğ‘¡1ğ›¿subscriptğ‘†ğ‘¡subscriptğ’¯subscriptğ‘ğ‘¡1subscriptğ‘†ğ‘¡S\_{t+1}=\\delta(S\_{t},\\mathcal{T}\_{a\_{t+1}}(S\_{t}))italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_Î´ ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_T start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) )

Here, Ï€:ğ’®â†’Î”â¢(ğ’œ):ğœ‹â†’ğ’®Î”ğ’œ\\pi:\\mathcal{S}\\to\\Delta(\\mathcal{A})italic\_Ï€ : caligraphic\_S â†’ roman\_Î” ( caligraphic\_A ) constitutes the policy function mapping states to probability distributions over action space ğ’œğ’œ\\mathcal{A}caligraphic\_A (retrieval, generation, verification, etc.), while ğ’¯asubscriptğ’¯ğ‘\\mathcal{T}\_{a}caligraphic\_T start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT denotes state transition functions corresponding to action ağ‘aitalic\_a. The non-Markovian nature of the system emerges from St+1subscriptğ‘†ğ‘¡1S\_{t+1}italic\_S start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPTâ€™s dependence on complete historical trajectories {Sâ‰¤t}subscriptğ‘†absentğ‘¡\\{S\_{\\leq t}\\}{ italic\_S start\_POSTSUBSCRIPT â‰¤ italic\_t end\_POSTSUBSCRIPT }, with dynamic adaptability ensured through extensible action spaces ğ’œğ’œ\\mathcal{A}caligraphic\_A and online optimization of policy parameters Î˜Î˜\\Thetaroman\_Î˜. This formulation enables context-sensitive state updates via Î´:ğ’®Ã—ğ’ªâ†’ğ’®:ğ›¿â†’ğ’®ğ’ªğ’®\\delta:\\mathcal{S}\\times\\mathcal{O}\\to\\mathcal{S}italic\_Î´ : caligraphic\_S Ã— caligraphic\_O â†’ caligraphic\_S, establishing a theoretical foundation for open-ended reasoning processes in complex problem domains.

Based on the mode of reasoning initiation, agentic RAG with dynamic workflows can be further categorized into three distinct types: Proactivity-driven, Reflection-driven, and Feedback-driven mechanisms. The LLM proactivity-driven approach is characterized by the modelâ€™s autonomous triggering of actions based on internal assessments, executing operations without external intervention through mechanisms analogous to human intuitive decision-makingâ€”for instance, when the model independently identifies insufficient evidentiary support in the current reasoning process, it proactively generates retrieval requests to supplement information. The reflection-driven mode emphasizes self-examination of the reasoning process, dynamically initiating subsequent operations through quantitative evaluation of intermediate result quality (e.g., triggering actions when the calculated reasoning support score of 0.7 exceeds a predefined threshold of 0.6), which simulates the self-optimization logic of expert systems, enabling the model to adjust reasoning pathways through introspection. The feedback-driven mechanism incorporates external intervention, employing independent models or rule-based systems to perform real-time scoring of intermediate states (e.g., an external reward model assigning a 2.5/5 score to reasoning steps) while providing corrective suggestions, operating similarly to a mentor-guided mode that continuously calibrates the reasoning workflow through external feedback signals.

#### 4.2.1. Proactivity-Driven Reasoning

The core innovation of Proactivity-driven Reasoning lies in enabling LLMs to fully govern the reasoning process through self-triggered prediction mechanisms. This active control manifests through three key mechanisms: (1) direct tool invocation via model-generated special tokens (e.g., \[Web-Search\]), without external intervention, (2) context-aware decision making based on real-time knowledge gaps or hypothesis verification requirements, and (3) Markov Decision Process (MDP)-based dynamic path optimization.

Formally, the reasoning process can be modeled as a state sequence S\={s0,s1,â€¦,st}ğ‘†subscriptğ‘ 0subscriptğ‘ 1â€¦subscriptğ‘ ğ‘¡S=\\{s\_{0},s\_{1},\\dots,s\_{t}\\}italic\_S = { italic\_s start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦ , italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT }, where each state stsubscriptğ‘ ğ‘¡s\_{t}italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT encapsulates the current reasoning context. At each step tğ‘¡titalic\_t, the LLM selects an action atâˆˆ{retrieve,generate,terminate}subscriptğ‘ğ‘¡retrievegenerateterminatea\_{t}\\in\\{\\text{retrieve},\\text{generate},\\text{terminate}\\}italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âˆˆ { retrieve , generate , terminate } based on stsubscriptğ‘ ğ‘¡s\_{t}italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, executes the corresponding operation (e.g., document retrieval or answer generation), and updates its state through transition function st+1\=Î´â¢(st,at,ot)subscriptğ‘ ğ‘¡1ğ›¿subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘œğ‘¡s\_{t+1}=\\delta(s\_{t},a\_{t},o\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_Î´ ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_o start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) where otsubscriptğ‘œğ‘¡o\_{t}italic\_o start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT represents action outcomes. This MDP framework enables dynamic path adjustment through real-time feedback until termination (aT\=terminatesubscriptğ‘ğ‘‡terminatea\_{T}=\\text{terminate}italic\_a start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT = terminate) and final answer generation.

Recent advancements demonstrate significant improvements over conventional RAG approaches. The Agentic Reasoning framework achieves granular control through dynamic tool invocation, eliminating predefined execution sequences. DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) optimizes cost-accuracy tradeoffs via MDP-based imitation learning, addressing the retrieval-generation disconnection in traditional systems. CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)) introduces hybrid-driven mechanisms combining LLM-initiated subqueries with external policy control, enhancing error tolerance for complex queries. Collectively, these approaches establish a paradigm shift from fixed pipelines to context-sensitive, self-optimizing reasoning architectures.

#### 4.2.2. Reflection-Driven Reasoning

The reflection-driven mechanism represents a dynamic reasoning framework that enables iterative self-evaluation and revision of intermediate outputs through model introspection. Common methods include: (1) a evaluation system combining explicit token prediction and implicit confidence scoring, (2) self-monitoring capabilities through grounding tokens for content-document consistency verification and utility tokens for answer effectiveness assessment, and (3) adaptive routing mechanisms that automatically select single-hop or multi-hop reasoning paths based on contextual complexity. The mathematical formalism of this process can be expressed as:

(7)

ğ’«\=â‹ƒt\=1T\[Gâ¢(ğ‚t)â†’Eâ¢(ğ‡t,ğ’Ÿ)â†’Ïˆâ¢(Ï•â¢(ğt),Ï„)\]ğ’«superscriptsubscriptğ‘¡1ğ‘‡delimited-\[\]â†’ğºsubscriptğ‚ğ‘¡ğ¸subscriptğ‡ğ‘¡ğ’Ÿâ†’ğœ“italic-Ï•subscriptğğ‘¡ğœ\\mathcal{P}=\\bigcup\_{t=1}^{T}\\left\[G(\\mathbf{C}\_{t})\\rightarrow E(\\mathbf{H}\_{% t},\\mathcal{D})\\rightarrow\\psi(\\phi(\\mathbf{e}\_{t}),\\tau)\\right\]caligraphic\_P = â‹ƒ start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT \[ italic\_G ( bold\_C start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) â†’ italic\_E ( bold\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_D ) â†’ italic\_Ïˆ ( italic\_Ï• ( bold\_e start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) , italic\_Ï„ ) \]

where GğºGitalic\_G denotes the generation function operating on current context ğœtsubscriptğœğ‘¡\\mathbf{c}\_{t}bold\_c start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, Eğ¸Eitalic\_E represents the evaluation function that assesses hidden states ğ¡tsubscriptğ¡ğ‘¡\\mathbf{h}\_{t}bold\_h start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT against external knowledge base ğ’Ÿğ’Ÿ\\mathcal{D}caligraphic\_D, Ï•italic-Ï•\\phiitalic\_Ï• serves as the confidence mapping function, Ï„ğœ\\tauitalic\_Ï„ is the decision threshold, and Ïˆğœ“\\psiitalic\_Ïˆ functions as the branch selector.

In practical implementations like Self-RAGÂ (Asai etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)), this framework generates candidate responses alongside reflection tokens, computes passage relevance scores (ISRELâˆˆ\[0,1\]ISREL01\\text{ISREL}\\in\[0,1\]ISREL âˆˆ \[ 0 , 1 \]) and factual support metrics (ISSUP), and employs weighted aggregation of token probabilities in Ï•italic-Ï•\\phiitalic\_Ï• to determine retrieval activation or generation revision through threshold-based Î´ğ›¿\\deltaitalic\_Î´ operations. Meanwhile, Open-RAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) incorporates hybrid threshold mechanisms and Mixture-of-Experts architecture to enforce counterfactual verification through non-retrieval confidence scoring (PrNoRTsubscriptPrNoRT\\text{Pr}\_{\\text{NoRT}}Pr start\_POSTSUBSCRIPT NoRT end\_POSTSUBSCRIPT), enabling dynamic expansion of complex reasoning capabilities while preserving base model efficiency. ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) utilizes knowledge-guided reasoning chains combined with external knowledge sources to perform reflection-driven reasoning. In each iteration, it adjusts the reasoning path through the â€Thought-Action-Observationâ€ paradigm, effectively preventing error propagation and improving answer accuracy.

The paradigmâ€™s innovation lies in reconstructing traditional sequential processes into conditional Markov decision processes, where state transition probabilities Pâ¢(st+1|st)ğ‘ƒconditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡P(s\_{t+1}|s\_{t})italic\_P ( italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) are dynamically determined by model self-evaluation outcomes. Compared to proactive LLM-driven methods (e.g., Toolformerâ€™s direct API invocation), the reflection-driven approach establishes closed-loop control through explicit evaluation stages (function Eğ¸Eitalic\_E), effectively mitigating hallucination risks while maintaining computational efficiency.

#### 4.2.3. Feedback-Driven Reasoning

The feedback-driven dynamic RAG system establishes closed-loop control over reasoning processes through external signals, formally modeled as a Partially Observable Markov Decision Process. The system state st\=(qt,ğ’¦t,Ht)subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ’¦ğ‘¡subscriptğ»ğ‘¡s\_{t}=(q\_{t},\\mathcal{K}\_{t},H\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) evolves through iterative interactions, comprising the current query representation qtsubscriptğ‘ğ‘¡q\_{t}italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, dynamic knowledge base ğ’¦tsubscriptğ’¦ğ‘¡\\mathcal{K}\_{t}caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, and historical trajectory â„‹tsubscriptâ„‹ğ‘¡\\mathcal{H}\_{t}caligraphic\_H start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT. Initialized with q0subscriptğ‘0q\_{0}italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT and ğ’¦0\=âˆ…subscriptğ’¦0\\mathcal{K}\_{0}=\\emptysetcaligraphic\_K start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT = âˆ…, the policy function Ï€â¢(at|st)ğœ‹conditionalsubscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡\\pi(a\_{t}|s\_{t})italic\_Ï€ ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) generates actions from the operational space ğ’œ\={Retrieve,Reason,Verify,Answer,âˆ…}ğ’œRetrieveReasonVerifyAnswer\\mathcal{A}=\\{\\text{Retrieve},\\text{Reason},\\text{Verify},\\text{Answer},\\emptyset\\}caligraphic\_A = { Retrieve , Reason , Verify , Answer , âˆ… }. State transitions follow st+1\=Î´â¢(st,at)subscriptğ‘ ğ‘¡1ğ›¿subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡s\_{t+1}=\\delta(s\_{t},a\_{t})italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = italic\_Î´ ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) with knowledge base updates

(8)

ğ’¦t+1\=ğ’¦tâŠ•Retrieveâ¢(qt)â‹…ğ•€â¢(at\=Retrieve)subscriptğ’¦ğ‘¡1direct-sumsubscriptğ’¦ğ‘¡â‹…Retrievesubscriptğ‘ğ‘¡ğ•€subscriptğ‘ğ‘¡Retrieve\\mathcal{K}\_{t+1}=\\mathcal{K}\_{t}\\oplus\\text{Retrieve}(q\_{t})\\cdot\\mathbb{I}(a% \_{t}=\\text{Retrieve})caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âŠ• Retrieve ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) â‹… blackboard\_I ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = Retrieve )

where âŠ•direct-sum\\oplusâŠ• denotes incremental updates and ğ•€ğ•€\\mathbb{I}blackboard\_I represents an indicator function. The reward function Râ¢(st,at,st+1)â†’rtâ†’ğ‘…subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡1subscriptğ‘Ÿğ‘¡R(s\_{t},a\_{t},s\_{t+1})\\rightarrow r\_{t}italic\_R ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ) â†’ italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT drives policy optimization through

(9)

Ï€t+1\=Î©â¢(Ï€t,âˆ‡Î¸ğ”¼aâˆ¼Ï€tâ¢\[Râ¢(st,a,st+1)\])subscriptğœ‹ğ‘¡1Î©subscriptğœ‹ğ‘¡subscriptâˆ‡ğœƒsubscriptğ”¼similar-toğ‘subscriptğœ‹ğ‘¡delimited-\[\]ğ‘…subscriptğ‘ ğ‘¡ğ‘subscriptğ‘ ğ‘¡1\\pi\_{t+1}=\\Omega(\\pi\_{t},\\nabla\_{\\theta}\\mathbb{E}\_{a\\sim\\pi\_{t}}\[R(s\_{t},a,s\_% {t+1})\])italic\_Ï€ start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = roman\_Î© ( italic\_Ï€ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , âˆ‡ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT blackboard\_E start\_POSTSUBSCRIPT italic\_a âˆ¼ italic\_Ï€ start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT \[ italic\_R ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a , italic\_s start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT ) \] )

forming an adaptive control loop. Three distinct feedback mechanisms emerge within this framework.

Explicit reward feedback employs specialized models Ï€rewardsubscriptğœ‹reward\\pi\_{\\text{reward}}italic\_Ï€ start\_POSTSUBSCRIPT reward end\_POSTSUBSCRIPT for quantitative evaluation, exemplified by RAG-Gymâ€™s process rewardsÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)). The reward function combines immediate and terminal rewards:

(10)

rt\=Î»1â¢Ï€rewardâ¢(st)+Î»2â¢ğ”¼st+kâ¢\[Î³kâ¢Rterminal\]subscriptğ‘Ÿğ‘¡subscriptğœ†1subscriptğœ‹rewardsubscriptğ‘ ğ‘¡subscriptğœ†2subscriptğ”¼subscriptğ‘ ğ‘¡ğ‘˜delimited-\[\]superscriptğ›¾ğ‘˜subscriptğ‘…terminalr\_{t}=\\lambda\_{1}\\pi\_{\\text{reward}}(s\_{t})+\\lambda\_{2}\\mathbb{E}\_{s\_{t+k}}\[% \\gamma^{k}R\_{\\text{terminal}}\]italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = italic\_Î» start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT italic\_Ï€ start\_POSTSUBSCRIPT reward end\_POSTSUBSCRIPT ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) + italic\_Î» start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT blackboard\_E start\_POSTSUBSCRIPT italic\_s start\_POSTSUBSCRIPT italic\_t + italic\_k end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT \[ italic\_Î³ start\_POSTSUPERSCRIPT italic\_k end\_POSTSUPERSCRIPT italic\_R start\_POSTSUBSCRIPT terminal end\_POSTSUBSCRIPT \]

with discount factor Î³ğ›¾\\gammaitalic\_Î³. SmartRAG extends this through policy gradient optimization

(11)

âˆ‡Î¸Jâ¢(Î¸)\=ğ”¼Ï„âˆ¼Ï€Î¸â¢\[âˆ‘t\=0Tâˆ‡Î¸logâ¡Ï€Î¸â¢(at|st)â¢A^t\]subscriptâˆ‡ğœƒğ½ğœƒsubscriptğ”¼similar-toğœsubscriptğœ‹ğœƒdelimited-\[\]superscriptsubscriptğ‘¡0ğ‘‡subscriptâˆ‡ğœƒsubscriptğœ‹ğœƒconditionalsubscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡subscript^ğ´ğ‘¡\\nabla\_{\\theta}J(\\theta)=\\mathbb{E}\_{\\tau\\sim\\pi\_{\\theta}}\[\\sum\_{t=0}^{T}% \\nabla\_{\\theta}\\log\\pi\_{\\theta}(a\_{t}|s\_{t})\\hat{A}\_{t}\]âˆ‡ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT italic\_J ( italic\_Î¸ ) = blackboard\_E start\_POSTSUBSCRIPT italic\_Ï„ âˆ¼ italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT \[ âˆ‘ start\_POSTSUBSCRIPT italic\_t = 0 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT âˆ‡ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) over^ start\_ARG italic\_A end\_ARG start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT \]

where the advantage function A^tsubscript^ğ´ğ‘¡\\hat{A}\_{t}over^ start\_ARG italic\_A end\_ARG start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT integrates temporal feedback.

Implicit environmental feedback derives from knowledge base validation, as implemented in KBQA-o1â€™s SPARQL verification and SolutionRAGâ€™s pruning mechanismsÂ (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)). This feedback is formalized as rt\=ğ•€(ğ’¦tâŠ§q0)â‹…cvalidâˆ’ğ•€(âŠ¥âˆˆğ’¦t)â‹…cinvalidr\_{t}=\\mathbb{I}(\\mathcal{K}\_{t}\\models q\_{0})\\cdot c\_{\\text{valid}}-\\mathbb{I% }(\\bot\\in\\mathcal{K}\_{t})\\cdot c\_{\\text{invalid}}italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = blackboard\_I ( caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âŠ§ italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ) â‹… italic\_c start\_POSTSUBSCRIPT valid end\_POSTSUBSCRIPT - blackboard\_I ( âŠ¥ âˆˆ caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) â‹… italic\_c start\_POSTSUBSCRIPT invalid end\_POSTSUBSCRIPT with validation function ğ•€â¢(â‹…)ğ•€â‹…\\mathbb{I}(\\cdot)blackboard\_I ( â‹… ) and penalty coefficients cğ‘citalic\_c. ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) introduces threshold-triggered correction: when rt<Ï„subscriptğ‘Ÿğ‘¡ğœr\_{t}<\\tauitalic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT < italic\_Ï„, it activates refinement loops ğ’¦t+1\=PEMâ¢(ğ’¦t,q0)âŠ•Retrieveâ¢(PRMâ¢(st))subscriptğ’¦ğ‘¡1direct-sumPEMsubscriptğ’¦ğ‘¡subscriptğ‘0RetrievePRMsubscriptğ‘ ğ‘¡\\mathcal{K}\_{t+1}=\\text{PEM}(\\mathcal{K}\_{t},q\_{0})\\oplus\\text{Retrieve}(\\text% {PRM}(s\_{t}))caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = PEM ( caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_q start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ) âŠ• Retrieve ( PRM ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) ).

Structured rule feedback encodes domain knowledge through differentiable scoring functions. MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)) implements depth-attenuated rewards

(12)

rt\=11+Î±â¢dtâ¢âˆ‘i\=1nLLMscoreâ¢(at(i))subscriptğ‘Ÿğ‘¡11ğ›¼subscriptğ‘‘ğ‘¡superscriptsubscriptğ‘–1ğ‘›subscriptLLMscoresuperscriptsubscriptğ‘ğ‘¡ğ‘–r\_{t}=\\frac{1}{1+\\alpha d\_{t}}\\sum\_{i=1}^{n}\\text{LLM}\_{\\text{score}}(a\_{t}^{(% i)})italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = divide start\_ARG 1 end\_ARG start\_ARG 1 + italic\_Î± italic\_d start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_ARG âˆ‘ start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT LLM start\_POSTSUBSCRIPT score end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT ( italic\_i ) end\_POSTSUPERSCRIPT )

with search depth dtsubscriptğ‘‘ğ‘¡d\_{t}italic\_d start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPTand decay coefficient Î±ğ›¼\\alphaitalic\_Î±. CR-Plannerâ€™s hierarchical critique combines subgoal and execution scores: rttotal\=Î²1â¢Ï€subâ¢(st)+Î²2â¢Ï€execâ¢(at|st)superscriptsubscriptğ‘Ÿğ‘¡totalsubscriptğ›½1subscriptğœ‹subsubscriptğ‘ ğ‘¡subscriptğ›½2subscriptğœ‹execconditionalsubscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡r\_{t}^{\\text{total}}=\\beta\_{1}\\pi\_{\\text{sub}}(s\_{t})+\\beta\_{2}\\pi\_{\\text{exec% }}(a\_{t}|s\_{t})italic\_r start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT total end\_POSTSUPERSCRIPT = italic\_Î² start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT italic\_Ï€ start\_POSTSUBSCRIPT sub end\_POSTSUBSCRIPT ( italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) + italic\_Î² start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT italic\_Ï€ start\_POSTSUBSCRIPT exec end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | italic\_s start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT ) through weighted fusion.

These feedback mechanisms interact through a unified strategy update framework, where external feedback-driven approaches achieve controllable optimization of the reasoning process through interpretable feedback signals while maintaining the generative capabilities of LLMs. Overall, the dynamic process of RAG, by endowing the model with autonomy in the reasoning process, not only enhances adaptability to complex tasks but also provides a new solution for efficient reasoning in resource-constrained environments.

## 5\. Implementation and Optimization

Building upon preceding sections, this section systematically analyzes the concrete implementation and optimization strategies for reasoning within the RAG paradigm. In contrast to existing surveys that predominantly focus on post-training methodologies or isolated LLM reasoning mechanisms, our analysis maintains a dedicated focus on the synergistic integration of RAG with reasoning examining their co-adaptive implementations through a structural lens.

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/implementation.png)

Figure 6. Implementation and optimization of the synergy between RAG and Reasoning

### 5.1. Reasoning Process

#### 5.1.1. LLM CoT

Integrating Chain-of-Thought (CoT) reasoning with LLMs is key to combining RAG with complex reasoning tasks. Research shows CoT enhances RAG systems by explicitly guiding multi-step reasoning and dynamically incorporating external knowledge. For example, ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)) uses a â€Self-Inquiry â†’ Knowledge Assimilation â†’ Thought Accommodationâ€ chain to align knowledge and reasoning: a knowledge assimilation agent merges external documents with LLM memory via operations like association and reflection, creating structured knowledge. Meanwhile, a reasoning adaptation agent refines inference chains from Self-Inquiry to ensure answers align with retrieved knowledge and address reasoning gaps. Similarly, Adaptive-RAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) alternates between CoT and retrieval, breaking down multi-hop reasoning into steps such as entity localization and document correlation, refining retrieval and generation based on prior results.

At the knowledge and reasoning level, O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) drives RAG through open-ended long-text reasoning, extending CoT beyond fixed triggers via coherent thought processes like problem decomposition. PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) explicitly uses CoT to produce executable multi-step plans, adjusting operations dynamically through a closed-loop â€plan-execute-feedbackâ€ cycle. Despite different implementations, these methods share two CoT strengths: breaking down complex problems into clear intermediate steps and guiding external knowledge selection through reasoning states. Studies show these approaches outperform traditional RAG in multi-hop QA and knowledge-intensive tasks by enhancing both LLMsâ€™ reasoning and adaptability to external knowledge.

#### 5.1.2. Special Token Prediction

Recent advances active RAG also highlight special token prediction as a key method for dynamically linking external knowledge retrieval with multi-step reasoningÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)). By embedding domain- or action-specific tokens (e.g., â€˜\[Web-search\]â€˜, â€˜\[Retrieve=Yes\]â€˜, â€˜Â¡begin\_of\_queryÂ¿â€˜) into LLM vocabularies, models can autonomously trigger tools or self-reflect during text generation. Frameworks like Self-RAGÂ (Asai etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)) and SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) use dedicated tokens (â€˜Retrieveâ€˜, â€˜ISRELâ€˜, â€˜\[RETRIEVE\]â€˜) to manage retrieval activation, relevance checks, and output verification, turning static reasoning chains into conditional workflows. The innovation lies in predicting these tokens within generated sequences, segmenting tasks into retrieval initiation, document evaluation, and knowledge grounding phases.

Hybrid models such as Open-RAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) combine token control with mixture-of-experts (MoE) routing, sparsely activating experts aligned with token-predicted reasoning. Unlike traditional chain-of-thought or search tree methods, special token prediction offers finer control and interpretability by encoding decision logic explicitly in token sequences while maintaining end-to-end training. This approach also overcomes latency and inflexibility of preset retrieval schedules by enabling context-aware, on-demand tool use. For example, R1-SearcherÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)) and Search-o1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) use token boundaries like â€˜Â¡end\_of\_queryÂ¿â€˜ to coordinate retrieval pauses and resume generation after knowledge integration.

Together, these systems show that token-level prediction not only bridges reasoning and retrieval but also creates a scalable framework for tool-enhanced language agents, preserving generative fluency while enabling systematic external knowledge integration and procedural reasoning.

#### 5.1.3. Search-Driven Reasoning

Recent advancements in search-driven reasoning have significantly improved RAG frameworks by employing structured search strategies for dynamic information exploration and multi-step reasoning with external knowledge. Current approaches mainly follow three paradigms: tree-based search, MCTS, and reinforcement learning-optimized policy networks.

Tree-based methods organize reasoning hierarchically through structured path exploration. For example,StePO-RecÂ (Bi etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib6)) uses a multi-step tree-structured reasoning method that iteratively retrieves different outfit matching knowledge and user preferences at each node, ultimately achieving generative recommendations for complementary items. OmniThinkÂ (Xi etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib95)) uses an information tree to expand topic analysis by generating subqueries that guide breadth-first or depth-first retrievals. DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) applies a binary tree search within a Markov decision process to explore parametric knowledge and retrieval paths in parallel, selecting optimal branches. DeepSolutionâ€™sÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)) bidirectional thinking tree alternates expanding solution and critique nodes with scoring for path pruning, aligning naturally with MCTS evaluation. These methods balance exploration efficiency with solution coverage through explicit tree structures.

MCTS enhances robustness by optimizing long-term decisions via simulation, evaluation, and backpropagation. CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) integrates MCTS with the UCB strategy to balance exploration and exploitation while estimating optimal subgoals through multi-step simulations. KBQA-O1Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59)) and MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98)) generate candidate actions using policy models and combine reward models to globally assess logical forms, reducing local optima. ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) innovatively merges MCTS with procedural reward models (PRMs), interleaving retrieval and reasoning steps, and filtering high-reward paths to form a closed-loop â€reason-retrieve-reasonâ€ cycle. These methods probabilistically explore paths and use reinforcement learning feedback to improve global reasoning for complex tasks.

Reinforcement learning-optimized policy networks adaptively refine search strategies. LeReTÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)) replaces fixed search algorithms with reinforcement learning (e.g., IPO) to dynamically optimize query generation based on rewards like retrieval accuracy, implicitly learning optimal search patterns without explicit tree or graph structures, thus offering greater flexibility and scalability.

In summary, search-driven reasoning unites inference and retrieval through structured strategies, combining multi-path exploration, dynamic evaluation, and adaptive optimization to deliver interpretable, efficient solutions for knowledge-intensive tasks. Future work may focus on hybrid paradigms (e.g., integrating MCTS and reinforcement learning) and lightweight algorithms to balance performance with computational efficiency.

#### 5.1.4. Reasoning on Graph

Graph-structured reasoning offers a novel approach for multi-hop inference in RAG systems by explicitly modeling knowledge interaction paths through topology. Current methods fall into two categories: query-flow-oriented search graphs (e.g. FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51))) and knowledge-association-based expansion graphs (ToG-2.0Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61))). FinSearch builds a directed acyclic graph (DAG) where nodes are atomic subqueries (e.g., stock prices, financial reports) and edges capture logical and temporal dependencies. A pre-planner breaks down queries into subquery sequences, using graph traversal to control information flow and dynamically adjust paths, such as backtracking when conflicts ariseâ€”substantially surpassing linear chain-of-thought methods in handling complex logic.

#### 5.1.5. External Solver

The integration of RAG and reasoning is also can be achieved by incorporating external solvers, where specialized solvers, such as the Alignment-Oriented LLM-based Retrieval Method (ARM), are employed to handle the reasoning component. The retrieval process for complex problems is formulated as a global optimization task, leveraging external solvers like mixed-integer programming (MIP) to achieve structural alignment and joint optimization of data objects. Specifically, ARM first decomposes user queries into keywords that match N-grams in the dataset through an information alignment module, generating an initial set of retrieval candidates via constrained decoding. Subsequently, in the structural alignment phase, the MIP solver performs global filtering on candidate objects based on a predefined objective function that maximizes both the relevance of retrieved objects to the query and their mutual compatibility. This ensures that the selected objects not only cover the requirements of the query but also form a coherent information chain through entity or inter-table linkages. Finally, the self-verification mechanism of the LLM, combined with a beam search-based aggregation strategy, dynamically refines and consolidates multiple candidate sets, ultimately producing a retrieval collection that satisfies both semantic matching and the structural organization of the data.

ToG-2.0 achieves multi-hop expansion by integrating knowledge graphs with documents, starting from an initial entity and iteratively extending relevant entities and relations (such as corporate ownership chains and technology dependency networks) via the Edge function. This process constructs structured triple paths while simultaneously retrieving and verifying document content. By tuning the width and depth parameters, the method emulates human reasoning: broadly exploring potential associations before deeply verifying high-confidence paths. FRAGÂ (Gao etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib24)) dynamically adjusts retrieval strategies by predicting the hop range of reasoning paths based solely on the query text, thereby enhancing retrieval quality without requiring additional fine-tuning or invocation of large language models, enabling flexible and efficient retrieval optimization. FG-RAGÂ (Hong etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib33)) further expands entity coverage in graph retrieval through context-aware entity expansion, providing richer background information. Combined with query-level fine-grained summary generation, FG-RAG transforms coarse-grained graph information into highly relevant detailed content, effectively improving the performance of query-focused summarization tasks.

Although differing in design from workflow-based methods, ToG-2.0 shares key advantages with other graph-structured approaches: explicitly modeling reasoning state dependencies, supporting dynamic path generation and optimization, and enabling closed-loop interaction between retrieval and reasoning. This effectively overcomes the limitations of traditional RAG in implicit relation inference and counterfactual analysis, thereby establishing an interpretable theoretical and practical framework for knowledge reasoning.

### 5.2. Reasoning Optimization

In the previous chapter, we focused on introducing several approaches to integrate reasoning with RAG. This chapter shifts attention to how to augment the reasoning capabilities, specifically including Prompt-Based, Tuning-Based, and RL-Based strategies.

#### 5.2.1. Prompt-Based

Prompt-Based optimization is a key approach to improving RAG and reasoning system performance by using carefully designed natural language prompts. These prompts break down complex reasoning tasks into manageable steps and guide LLMs to follow specific logical structures during generation. The main advantage is that control over reasoning flow is achieved solely through prompt design, without parameter fine-tuning or reinforcement learning, preserving the modelâ€™s generalization while enhancing task-specific results.

This approach has three main features. First, task structuring: prompts explicitly decompose and control reasoning chains via zero-shot or templated designs. Techniques like Co-STORMÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) and WriteHereÂ (Xiong etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99)) use role assignments, stage divisions, and operation-specific instructions to guide multi-step reasoningâ€”such as proposal generation, knowledge retrieval, refinement, and validationâ€”improving interpretability by representing intermediate steps clearly.

Second, result reliability is improved by standardizing outputs and reducing hallucinations. Strategies include requiring citation of retrieval results, enforcing specific output formats, and integrating reflection and calibration based on retrieved knowledge. Systems like FinSearchÂ (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51)) and ActiveRAGÂ (Xu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib101)) incorporate temporal weighting, deduplication, and domain rules through prompts, enhancing consistency and logical coherence, especially in complex domains.

Third, interactive adaptability allows dynamic prompt adjustments. Special tokens (e.g., `<Search>`, `[Web-search]`) enable models to trigger tools or revise queries in real time based on intermediate results. Methods such as Agentic ReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) and PlanRAGÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49)) use context-sensitive prompts and feedback loops to refine reasoning paths dynamically, maintaining coherence and accuracy in multi-hop tasks and outperforming traditional RAG methods in complex, evolving scenarios.

In summary, prompt-based optimization offers an efficient, flexible, and reliable approach to enhancing RAG+Reasoning by emphasizing task structuring, result standardization, and interactive adaptability. Its non-intrusive and broadly applicable design has established it as a mainstream strategy for optimizing LLM reasoning and serves as a foundation for future hybrid methods integrating fine-tuning and reinforcement learning. By systematically optimizing reasoning without altering model parameters through semantic structures, dynamic feedback, and symbolic constraints, this paradigm effectively manages macro-level controls like task decomposition and knowledge integration while addressing key challenges such as generation consistency, logical coherence, and external knowledge alignment. This makes prompt-based optimization a lightweight yet powerful solution for complex reasoning tasks.

Table 1. Comparison of RL-based RAG with Reasoning Methods

1ORM: Outcome-based Reward Model; PRM: Process-based Reward Model. 2Full: Full parameter tuning.

#### 5.2.2. Tuning-Based

The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-of-thought mechanism within LLMs. Current research mainly targets three goals: _retrieval pathway optimization_, _structured generation enhancement_, and _collaborative training with external modules_.

For retrieval pathway optimization, methods like CoRAGÂ (Wang etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib84)) and DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the modelâ€™s ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.

To improve structured generation, MCTS-KBQAÂ (Xiong etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib98))and Self-RAGÂ (Asai etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib4)) fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances self-supervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-EmbedderÂ (Yan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib102)) and Open-RAGÂ (Islam etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib39)) align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRAÂ (Dettmers etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib18)) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.

In collaborative optimization with external modules, AdaptiveRAGÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) and CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) apply parameter isolation to balance generality and adaptability. AdaptiveRAG fine-tunes a lightweight classifier to select retrieval strategies dynamically. CR-Planner introduces a Critic model trained with contrastive loss on MCTS trajectory data to assess the long-term value of reasoning actions, prioritizing efficient solutions in tasks like mathematical reasoning.

Together, these tuning strategies restructure the parameter space to internalize retrieval-reasoning interactions effectively, enhancing the modelâ€™s ability to solve complex problems while ensuring computational efficiency and broad applicability across domains.

#### 5.2.3. RL-Based

As shown in TableÂ [1](https://arxiv.org/html/2504.15909v2#S5.T1 "Table 1 â€£ 5.2.1. Prompt-Based â€£ 5.2. Reasoning Optimization â€£ 5. Implementation and Optimization â€£ Synergizing RAG and Reasoning: A Systematic Review"), Reinforcement learning (RL) has recently become pivotal for tackling long-chain reasoning in modern inference models and optimizing RAG combined with reasoning tasks. Central to these advances is the use of dynamic reward mechanisms that guide LLMs to balance knowledge retrieval and logical reasoning adaptively. RL optimization objectives generally fall into two categories: outcome-based reward modeling (ORM) and process-based reward modeling (PRM), with some hybrid approaches blending both to balance global goals and local optimizations.

The ORM paradigm focuses solely on the quality of the final output and its adherence to standards. For example, R1-SearcherÂ (Song etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib73)) employs a two-stage Reinforce++Â (Hu, [2025](https://arxiv.org/html/2504.15909v2#bib.bib36)) training where rewards in the first stage depend on correct retrieval calls and special token generation, while the second stage directly optimizes the F1 score of answers. This encourages the model to develop strategies maximizing knowledge integration, reducing hallucinations, and enhancing accuracy in multi-hop QA beyond traditional RAG methods. Similarly, KBQA-O1 Â (Luo etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib59))uses MCTS with a policy network for candidate reasoning paths and a reward model evaluating logical consistency, effectively balancing exploration and exploitation in knowledge base QA.

Conversely, PRM emphasizes detailed supervision of intermediate reasoning steps. LeReTÂ (Hsu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib35)) uses the Identity Policy Optimization (IPO) algorithm, optimizing query quality by rewarding average precision (AP) of retrieved documents, boosting retrieval recall and overall multi-hop task performance. ReARTeRÂ (Sun etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib76)) extends this with a step-level binary reward model, combining Monte Carlo scoring and temporal difference (TD) methods to evaluate reasoning paths proactively, reducing logical errors and redundant retrievals, and improving accuracy on benchmarks like HotpotQA.

Moreover, influenced by DeepSeek-R1, GRPOÂ (Shao etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib70)) is also gradually being applied in scenarios combining RAG and Reasoning. GRPO is a variant of the Proximal Policy Optimization (PPO) reinforcement learning algorithm that abandons the critic model and instead estimates the baseline from group scores, significantly reducing training resources. For example, ReZeroÂ (Dao and Le, [2025](https://arxiv.org/html/2504.15909v2#bib.bib17)) uses GRPO to introduce a â€retryâ€ mechanism for LLMs, incentivizing LLMs to keep trying after an initial search failure by rewarding retry search queries. This mechanism simulates the human strategy of â€if at first you donâ€™t succeed, try againâ€ in information retrieval. PORAGÂ (Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74)), based on GRPO, directly optimizes retrieval quality, contextual relevance, and generation coherence through a dual reward mechanism (retrieval fidelity and response quality).

Hybrid methods merge ORM and PRM to optimize both final outcomes and intermediate steps via composite rewards. SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) applies Proximal Policy Optimization (PPO), combining answer-level F1 rewards with penalties for excessive retrievals, balancing knowledge completeness and efficiency. RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97))advances this with multidimensional process rewards (sufficiency, utility, redundancy) and techniques like contrastive loss and Best-of-N sampling to promote efficient search decisions, even zero-shot. These hybrid strategies markedly lower retrieval costs while sustaining accuracy in complex tasks.

In addition, we can also observe that in current RL-based methods, academia focuses more on exploration with small-scale LLMs (Â¡8B), among which the Qwen and Llama series are the most widely used. Overall, RL provides a flexible, scalable framework for integrating RAG and reasoning. ORM guides the discovery of globally optimal strategies, PRM enhances reasoning robustness via local refinements, and their combination addresses modular system limits. Future work may explore collaborative rewards in multi-agent settings, offline RL based on world models, and hierarchical reward decomposition for open-domain applications.

## 6\. Downstream Tasks and Evaluation

While previous chapters focused on methodologies and advances in RAG combined with reasoning, this chapter shifts to tasks and evaluation. It provides a comprehensive overview and analysis of existing tasks, datasets, their current status, and emerging trends. By reviewing these resources, we highlight the landscapeâ€™s gaps and limitations in current evaluation methods. The chapter also explores key challenges in assessment frameworks, identifying shortcomings and suggesting potential improvements.

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/sankey_diagram.png)

Figure 7. The current downstream tasks and datasets related to the combination of RAG and Reasoning show that multi-hop question answering tasks still dominate. Correspondingly, HotpotQA, 2WikiMultihopQA, and MuSiQue remain the most commonly used evaluation datasets.

### 6.1. Knowledge-Intensive Tasks

In the evaluation for RAG systems, knowledge-intensive question answering (QA) remains the primary focus (FigureÂ [7](https://arxiv.org/html/2504.15909v2#S6.F7 "Figure 7 â€£ 6. Downstream Tasks and Evaluation â€£ Synergizing RAG and Reasoning: A Systematic Review")). As LLMs improve in semantic understanding and reasoning, benchmarks have expanded to cover tasks from simple fact retrieval to complex multi-step reasoning. However, evaluation methods specifically designed for RAG lag behind due to the dual challenge of assessing both retrieval-generation coherence and adaptability to dynamic knowledge bases. For example, multi-hop QA requires integrating dispersed knowledge through multi-stage retrieval while verifying logical consistency between answers and retrieval paths. This complexity increases dataset construction costs compared to purely generative tasks, keeping research centered on knowledge-intensive QA subcategories such as open-domain QA, knowledge-base QA, and multi-hop QA.

Commonly used datasets include Natural Questions (NQ)Â (Kwiatkowski etÂ al., [2019](https://arxiv.org/html/2504.15909v2#bib.bib48)) for single-hop factual queries, HotpotQA, 2WikiMultiHopQAÂ (Ho etÂ al., [2020](https://arxiv.org/html/2504.15909v2#bib.bib32)) and MusiqueÂ (Trivedi etÂ al., [2022b](https://arxiv.org/html/2504.15909v2#bib.bib80)) for multi-hop QA. These benchmarks are mostly based on Wikipedia and fail to reflect the RAG demands and corresponding complexity in real-world scenarios. Some efforts have pushed evaluation boundaries, like CRUD-RAGâ€™sÂ (Lyu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib60)) operational metrics and DomainRAGâ€™sÂ (Wang etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib87)) domain-specific evaluations, but high costs and metric-task interdependencies limit progress. As a result, knowledge-intensive QA remains central for testing RAG robustness and practicality, highlighting a critical bottleneck: the need for innovative frameworks that balance retrieval flexibility and controlled generation to support new developments like Agentic RAG .Overall, many evaluation benchmarks are lagging behind rapid RAG+Reasoning advances, especially as LLMs grow more powerful. Specifically, the current evaluation of RAG faces the following challenges.

##### Limited Challenge

With improving LLM capabilities, many knowledge-based questions are no longer difficult, as they can be answered without external retrieval. Current multi-hop reasoning datasets, often built from artificial templates, offer limited challenge. There is an urgent need for more complex datasets reflecting real-world scenarios and practical use.

##### Lack of Specificity

Existing evaluation tasks are still predominantly focused on factual assessment and knowledge retrieval, lacking evaluations that probe deeper analytical thinking. This constraint limits the ability to measure a modelâ€™s capacity for profound reasoning and cognitive depth.

##### Task Uniformity

The majority of benchmarks are overly dependent on QA tasks, focusing on reactive, question-and-answer-based interactions. There is a pressing need to introduce tasks aligned with real-world applications, such as active information retrieval tasks based on personal knowledge or proactive knowledge discovery.

##### Insufficient Dimensions

Evaluations are primarily end-to-end, focusing solely on final outcomes. However, with the introduction of reasoning processes, RAG+Reasoning systems have become iterative, multi-step frameworks. Current evaluations are unable to assess intermediate reasoning steps or retrieval chains effectively. The absence of step-by-step supervision data limits both research and training of related methods. Furthermore, current evaluation methodologies lack comprehensive assessments of system performance trade-offs, such as computational cost and efficiency, which are critical for practical deployment.

This emergent landscape necessitates the creation of a new generation of evaluation frameworks that can address these shortcomings. Such frameworks must not only ensure the adaptability of retrieval and the controllability of generation but also integrate intermediate reasoning evaluation and efficiency metrics, paving the way for the development of more robust and efficient RAG systems suited to diverse real-world applications.

### 6.2. New Tasks on RAG+Reasoning

Recently, combining RAG with reasoning has significantly improved modelsâ€™ ability to tackle more realistic and challenging tasks, raising the standards for evaluation methods. This subsection examines emerging tasks that assess their combined strengths, related tasks and datasets are shown in TableÂ [2](https://arxiv.org/html/2504.15909v2#S6.T2 "Table 2 â€£ 6.2. New Tasks on RAG+Reasoning â€£ 6. Downstream Tasks and Evaluation â€£ Synergizing RAG and Reasoning: A Systematic Review"). Here, â€emergingâ€ refers not to entirely new tasks but to those with unprecedented complexity and demands. These include Deep Research tasks requiring multi-layered information integration and reasoning; PhD (Expert)-Level Complex Reasoning tasks targeting advanced scenario reasoning; and critical; domain-specific decision support tasks like medical diagnosis and legal analysis. Such tasks demand not only external knowledge retrieval but also logical consistency, coherence, and depth in reasoning.

Table 2. Tasks and Datasets under the New Trend of RAG Combined with Reasoning

Task Type

Sub-Task

Dataset

Description

Scale

Construction By

Evaluation

Paper

Deep Research

Deep Research

Agentic Reasoning Deep ResearchÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93))

PHD-level dataset covering finance, medicine, and law.

15-30 domains

PhD Experts

Expert pass rate

(Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93))

Report Generation

WildSeekÂ (Jiang etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib45))

Info-seeking taskâ€“goal pairs for document generation.

100 samples

Rules/LLM/Manual

LLM

(Xiong etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99))

Report Generation

TELL ME A STORYÂ (Huot etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib38))

fiction writing evaluation dataset: detailed prompts and long-form narratives.

230 samples

Manual

LLM

(Xiong etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib99))

Peer Review

Review-5kÂ (Weng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92))

ICLR 2024 peer review dataset: paper metadata and structured reviewer feedback.

4,991 papers

OpenReview/arXiv

MSE/MAE/Acc

(Weng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92))

Report Generation

Research-14kÂ (Weng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92))

2022â€“2024 Accepted ML papers: outlines, full texts, and cited abstracts.

14,911 papers

Semantic Scholar + arXiv

Simulated review scores

(Weng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib92))

Report Generation

SolutionBenchÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55))

Engineering benchmark: constrained solutions across 8 real-world domains.

1,050 datapoints

Manual/LLM extraction

Analytical/ Technical scores

(Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55))

Mathematics & Reasoning

Math Reasoning

GPQAÂ (Rein etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib68))

PHD-level MCQs in physics, chemistry, and biology.

744 sets

PhD Experts

Accuracy

(Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93))

Math Reasoning

MATH500Â (Lightman etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib56))

500 math problems from the MATH test set.

500 problems

Public repos

Pass@K

(Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52))

Programming

LiveCodeBenchÂ (Jain etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib41))

Programming benchmark with easy, medium, and hard problems.

1,055 problems

Competition platforms

Pass@K

(Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52))

Programming

USACOÂ (Shi etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib71))

USA Computing Olympiad problems, testing algorithms and coding.

307 problems

USA Computing Olympiad

Pass@K

(Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53))

Math Reasoning

TheoremQA-MathÂ (Hongjin etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib34))

BRIGHT subset: theorem-based math problems.

206 problems

STEM datasets

Accuracy

(Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53))

Programming

GorillaÂ (Patil etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib65))

API-aware code generation from HuggingFace, Torch Hub, TensorFlow Hub docs.

1,600 APIs

Manual

AST matching

(Srinivas and Runkana, [2025](https://arxiv.org/html/2504.15909v2#bib.bib74))

Math Reasoning

OlympiadBenchÂ (He etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib30))

Olympiad-level math competition problems.

1,000 problems

Competitions

Accuracy/F1

(Zhu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib110))

Complex Reasoning

ComplexWebQAÂ (Talmor and Berant, [2018](https://arxiv.org/html/2504.15909v2#bib.bib77))

Multi-step reasoning over web queries with cross-document integration.

34,689 queries

Web snippets

Accuracy

(Hu etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib37))

Demanding Retrieval

Domain Retrieval

StackEcon & StackBioÂ (Hongjin etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib34))

Biology and economics StackExchange questions for complex retrieval.

206 queries

StackExchange

nDCG@K

(Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53))

Active Retrieval

AR-BenchÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))

Active retrieval benchmark with four sub-tasks.

8k/sub-task

Synthetic

Accuracy

(Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))

Real-time

TAQAÂ (Zhao etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib105))

QA dataset with time-evolving answers.

10K-100K rows

Human-curated

LLM

(Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))

Real-time

FreshQAÂ (Vu etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib81))

Dynamic fact QA benchmark with evolving answers

600 samples

Mixed sources

LLM

(Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))

Domain Retrieval

PubMedÂ (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43))

PICO-based medical search dataset linking reviews to PubMed.

21k+ samples

Systematic reviews

Recall@K

(Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43))

Domain Retrieval

Trial searchÂ (Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43))

PICO-based clinical trial search linked to ClinicalTrials.gov.

7k+ samples

Manually

Recall@K

(Jiang, [2025](https://arxiv.org/html/2504.15909v2#bib.bib43))

Domain Retrieval

FinSearchBench-24Â (Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51))

Financial retrieval benchmark covering stocks, rates, policy, trends.

1,500 queries

Manually

Accuracy

(Li etÂ al., [2024c](https://arxiv.org/html/2504.15909v2#bib.bib51))

Decision & QA

Business

DQAÂ (Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49))

Decision QA benchmark with business scenarios in enterprise settings.

301 pairs

video games

Accuracy

(Lee etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib49))

Medical

CMB-ClinÂ (Wang etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib88))

CMB subset for clinical diagnosis reasoning in Chinese medical cases.

74 cases

Textbooks/diagnostic materials

LLM/Expert

(Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12))

Medical

MM-CasesÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12))

Medicine cases generated by GPT-4o-mini, verified by doctors.

609 cases

LLM/doctor-reviewed

LLM/Expert

(Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12))

Medical

TCM-CasesÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12))

TCM patient cases generated by GPT-4o-mini, verified by doctors.

130 cases

LLM/doctor-reviewed

LLM/Expert

(Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12))

#### 6.2.1. Deep Research

From the perspective of integrating RAG and reasoning, Deep Research tasks exemplify complex downstream applications. They require models to handle open-ended retrieval, produce long-form, structured text, and synthesize multi-source information through deep reasoning. This section analyzes their key features, evaluation datasets, and metrics.

At the core of Deep Research tasks lies the mission of addressing complex informational queries. These tasks are distinguished by several key attributes:

First, dynamic interactivity is essential. Models engage in iterative dialogue to uncover latent user needs or â€unknown unknownsâ€. For example, the Co-StormÂ (Jiang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib44)) framework enables collaboration with multiple language model agents to explore information gradually, easing user cognitive load and capturing unmet needs more accurately.

Second, integrating information from multiple sources is crucial. Models must consolidate diverse data to provide comprehensive coverage. For instance, uses dynamic mind maps to structure knowledge and produce cohesive reports, ensuring accuracy and completeness.

Third, expert-level accuracy is required. Many tasks demand domain expertise, expecting models to perform like human specialists. The Agentic ReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) framework illustrates this with high-stakes scenarios like medical treatment design or legal analysis, where outputs are judged on correctness, depth, and coherence.

Fourth, multi-modal reasoning is often necessary. Deep Research tasks involve varied data typesâ€”text, code, knowledge graphsâ€”and dynamic tool use such as web searches or code execution to enhance reasoning.

Finally, handling multiple real-world constraints is vital. Tasks may require generating practical solutions under specific conditions, like designing hospitals in challenging environments with factors like heavy rainfall and seismic activity, as seen in the DeepSolution framework. This ensures outputs are feasible and relevant.

To ensure the diversity and complexity of Deep Research tasks, their evaluation relies on datasets drawn from multiple domains. A few notable examples include:

WildSeek DatasetÂ (Jiang etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib45)): This dataset is constructed from real-world user information-seeking scenarios and comprises 100 data points covering 24 fields, including economics, computer science, and law. Each data point is characterized by a topic, user goal, and domain label. For example: â€Domain: Economics; Topic: Development of a Shared Trading Currency; Goal: Investigate how a new shared currency could eliminate transaction costsâ€. WildSeek effectively evaluates modelsâ€™ competence in dynamic interaction and multi-source information integration.

GAIAÂ (Mialon etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib63)). The GAIA Benchmark, developed jointly by Meta AI, Hugging Face, and others, is a comprehensive evaluation framework designed to assess general AI assistantsâ€™ ability to handle real-world problems. It features 466 carefully crafted tasks spanning language reasoning, visual perception, multi-agent collaboration, and adaptability, focusing on key skills like reasoning, multimodal processing, web browsing, and tool use. GAIA measures performance across dimensions such as task execution, adaptability, collaboration, generalization, and real-world reasoning with metrics like completion rate, response quality, efficiency, and robustness. Unlike traditional benchmarks, it emphasizes robustness and reliability in everyday scenarios, supports zero-shot evaluation, prevents data contamination, and is widely used in research and industry to guide AI development.

SolutionBenchÂ (Li etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib55)): This dataset spans eight engineering domains, including environmental, mining, and transportation engineering. Each instance presents a complex engineering problem with specific constraints. For example: â€Design a safe and efficient hospital construction plan in a region with 3000mm annual rainfall, expansive soils, and frequent seismic activity.â€\* SolutionBench evaluates modelsâ€™ ability to address multi-constraint problems and integrate specialized knowledge effectively.

The current evaluation system for DeepResearch faces the dual challenges of scarce specialized testing tasks and the difficulty of assessing complex, lengthy reports: On one hand, existing benchmark tests only cover basic capabilities and lack systematic evaluation standards in specialized scenarios like business analysis and policy assessment; on the other hand, the multimodal integration, logical chain verification, and domain adaptability testing of long reports pose technical bottlenecks for traditional assessment methods, necessitating the development of new evaluation tools that integrate logic graphs, dynamic scenario simulation, and domain knowledge bases.

In the future, the evaluation system will evolve into a multidimensional framework, including the construction of a three-level indicator matrix covering basic capabilities, reasoning levels, and application value. Overcoming these evaluation bottlenecks requires both technological innovation and joint standard-building efforts. This concerns not only the reliability validation of intelligent research tools but also the reshaping of research evaluation paradigms and industrial application boundaries.

#### 6.2.2. PhD (Expert)-Level Complex Reasoning

The integration of RAG with advanced reasoning has become essential for tackling expert-level, complex cognitive tasks, particularly at the PhD level. These tasks, including competitive programming, theorem-driven proof reasoning, and cross-disciplinary knowledge retrieval, require multi-layered logical inference and precise coordination between dynamic retrieval and domain-specific knowledge. PhD-level reasoning differs from standard evaluations across three dimensions: knowledge intensity, procedural rigor, and domain specificity. Knowledge intensity demands dynamic access to deep, specialized knowledge, such as analyzing dynamic programming time complexity or applying algebraic topology theoremsâ€”needs that surpass general corpora and call for domain-specific knowledge graphs and retrieval methods. Procedural rigor involves mathematical precision in multi-step proofs, requiring logical consistency in symbolic manipulation, theorem use, and counterexample refutation, as seen in international math competitions. Domain specificity reflects tailored reasoning methods, e.g., handling synchronization in concurrent programming or employing tensor calculus in quantum field theory.

Evaluation systems for such tasks are inherently multi-layered and multimodal. The USACO BenchmarkÂ (Shi etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib72)) offers a graduated difficulty scale for programming reasoning, testing both correctness and algorithmic constraints like time complexity. TheoremQA-MathÂ (Chen etÂ al., [2023](https://arxiv.org/html/2504.15909v2#bib.bib10)) links formalized math problems to theorem libraries, demanding verifiable mappings between theorem applications and calculations. Cross-disciplinary datasets like StackBio and StackEconÂ (Li etÂ al., [2024b](https://arxiv.org/html/2504.15909v2#bib.bib54)) assess modelsâ€™ ability to extract critical knowledge from dense, domain-rich documents, serving as strong tests for domain-oriented retrieval accuracy.

Modern evaluation surpasses traditional end-to-end tests by combining process and outcome validation. Frameworks like CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) use dual modelsâ€”a Sub-Goal Critic to score reasoning chains and an Execution Critic to evaluate retrievalâ€”allowing fine-grained step monitoring. For example, in dynamic programming, key steps like formulating state transitions and retrieving boundary conditions receive targeted feedback. Similarly, Search-O1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)) quantifies knowledge completeness by tracking uncertainty indicators (e.g., tentative language), measuring confidence and accuracy. Outcome validation maintains strict correctness benchmarks in programming and combines metrics like F1 scores with expert review in open-domain scientific QA to ensure precise understanding of domain-specific terms.

### 6.3. Challenges and Future Directions

#### 6.3.1. Complex Domain Tasks

Recent advances in RAG have provided novel solutions for more complex tasks in professional domains. These downstream tasks transcend the limitations of traditional question-answering models that rely solely on simple retrieval-generation patterns, involving challenges such as real-time information acquisition, integration of domain expertise, and dynamic decision-making support. The nature of these tasks can be characterized along three interrelated dimensions: (1) temporal dynamics, emphasizing the rapid changes in data and reasoning environment; (2) domain specificity, focusing on deep integration of industry knowledge and structured data; and (3) reasoning chain complexity, reflecting requirements for multi-stage reasoning and fine-grained decomposition of queries.

To rigorously evaluate such systems, innovative benchmarking approaches have been proposed. The FinSearchBench-24 dataset, for example, encompasses five months of market data variations, integrating multi-variable interactions across stock, policy, and industrial sectors, and includes over 1,500 multiple-choice questions, thereby surpassing the constraints of traditional static benchmarks. The evaluation adopts a hierarchical and quantitative methodology: the foundational level measures model accuracy and response latency; the intermediate layer assesses the temporal sensitivity of information relevance and the contribution of retrieval mechanisms to reasoning outcomes; and the advanced layer employs ablation studies to highlight performance variances under dynamic temporal decay. This multifaceted evaluation not only differentiates surface-level retrieval capabilities but also rigorously measures the synergy between reasoning quality and temporal context, furnishing theoretical and practical foundations for long-term stability and predictive accuracy in complex domain systems.

Experimental findings further reveal that establishing long-term evaluation protocols with temporal weighting functions is indispensable for adapting to realistic dynamic environments. Nonlinear declines in decision accuracy, observed when extending relevance windows from 72 to 168 hours, emphasize the importance of factoring temporal decay into assessment frameworks. Future work should extend these evaluation protocols to high-stakes domains such as medical diagnostics and legal consultation, where the standardization of interpretability metrics will critically support the evolution of RAG+ reasoning systems toward robust and trustworthy decision-assistance platforms.

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/cost.png)

Figure 8. From LLM to RAG and then to RAG+Reasoning, performance improvement comes with additional cost.

#### 6.3.2. Decision Support and Active Retrieval

The expansion of RAG+Reasoning frameworks into specialized tasks has fostered two complementary research paradigms: decision optimization and active retrieval. In the decision optimization category, systems must leverage heterogeneous structured data, rule bases, and objective functions to formulate optimal strategies. Representative systems like PlanRAG formalize Decision Question Answering (Decision QA) tasks targeting enterprise-level scenarios including supply chain optimization, industrial resource allocation, and market price regulation. These tasks require planning multimodal reasoning paths where models iteratively retrieve data from relational and graph databases, integrate intricate business rules, and iteratively refine decision-making paths through replanning mechanisms. To evaluate such capabilities, the Decision QA (DQA) benchmark creates dual database versions (MySQL and Neo4j) derived from economic systems in strategy games, assessing cross-structured generalization. The evaluation consists of a three-tier framework: the core tier measures answer accuracy; the intermediate layer diagnoses error types to identify system bottlenecks; and the foundational tier focuses on retrieval efficiency and the impact of replanning frequency. This structured evaluation framework not only tracks performance but also offers actionable insights for system refinement.

Conversely, the active retrieval evaluation addresses the challenge of dynamically determining when and how to invoke retrieval under complex multimodal contexts. Unlike rigid traditional RAG systems, UAR applies lightweight classifiers for fast, accurate triggers, improving performance in time-sensitive or creative tasks. Tested on AR-Bench, it combines binary trigger accuracy with GPT assessments, exact matches, and human reviews, boosting adaptability across diverse contexts.

Emerging trends in these evaluation paradigms indicate a shift from static, rule-based frameworks to dynamic system simulations, as exemplified by DQAâ€™s use of game engine-generated datasets to simulate realistic environments. Similarly, active retrieval tasks progress from simple retrieval trigger decisions toward collaborative multi-criteria decision-making. Evaluation methodologies are concurrently evolving from singular performance metrics to multidimensional matrices comprising core effectiveness, diagnostic error distributions, and economic cost measures.

## 7\. Cost and Risk

Integrating reasoning into RAG systems is neither effortless nor purely beneficial. Recent trends have exaggerated its advantages while downplaying the costs and risks. This trade-off between performance and cost is crucial. This section examines the expenses and misuse risks linked to adding reasoning to RAG systems. As shown in FigureÂ [8](https://arxiv.org/html/2504.15909v2#S6.F8 "Figure 8 â€£ 6.3.1. Complex Domain Tasks â€£ 6.3. Challenges and Future Directions â€£ 6. Downstream Tasks and Evaluation â€£ Synergizing RAG and Reasoning: A Systematic Review"), the cost of moving from LLM to RAG, then to RAG + Reasoning, incurs an inevitable â€invisible taxâ€. Though often hidden by performance gains, this cost is vital in assessing these methodsâ€™ overall practicality and efficiency.

The shift from LLM to RAG moves from simplicity to enhanced knowledge handling by incorporating external information. A basic LLM provides direct, efficient answers with low latency and token use but is limited to pre-trained knowledge, restricting complex or up-to-date queries. RAG overcomes this by adding a vector database for external retrieval, vastly expanding response scope and reliability. However, this requires substantial data processing, storage, and introduces higher latency and token costs due to data chunking, encoding, indexing, and retrieval overhead.

Advancing from RAG to RAG + Reasoning adds multi-step reasoning capabilities, enabling complex task handling, autonomous decisions, and more context-aware responses through intricate reasoning. This comes at the expense of increased delays, token consumption, processing demands, and greater complexity in system integration and maintenance. The reasoning layerâ€™s autonomy also brings opaqueness, unpredictability, and heightened security and reliability risks. These challenges highlight the necessity of carefully balancing effectiveness against costs when adopting RAG + Reasoning in real-world applications.

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/cost_diagram.png)

Figure 9. Cost quadrant diagram of retrieval and reasoning requirements

### 7.1. Cost Trade-off in RAG+Reasoning

FigureÂ [9](https://arxiv.org/html/2504.15909v2#S7.F9 "Figure 9 â€£ 7. Cost and Risk â€£ Synergizing RAG and Reasoning: A Systematic Review") illustrates typical works combining RAG and Reasoning, showing retrieval and reasoning demands alongside token consumption. While integrating dynamic knowledge retrieval with multi-step reasoning greatly improves accuracy in more complex tasks, the resulting systemic costs are often underestimated in research and practice. These costs grow non-linearly, causing serious efficiency bottlenecks in real-world use. The tradeoff between effectiveness and efficiency stems from RAG+Reasoningâ€™s architecture: multi-stage task decoupling, dynamic path planning, and intermediate state preservation. These features improve reasoning quality but trigger cascading increases in computational resources, token usage, and reduced retrieval efficiency. This section explores these implicit tradeoffs from the angles of resource use, token consumption, and retrieval efficiency.

#### 7.1.1. Non-Linear Growth of Computational Resources

The RAG+Reasoning framework separates retrieval and reasoning into multiple stages, causing computational demands to grow non-linearly. Dynamic chain-of-reasoning methods execute multiple LLM generations and retrievals per inference, resulting in complexity far exceeding baseline models. Fixed-length reasoning chains trigger repeated retrieval and generation calls, increasing resource needs with task complexity. More advanced techniques like MCTS-guided methods add rounds of candidate path generation and evaluation, further multiplying runtime and memory usage on GPUs compared to linear methods. Even simpler multi-step planning tasks incur much higher overhead than single-stage retrieval models due to extra graph construction and analysis. While this resource intensity improves inference accuracy, it poses serious scalability challenges under limited resources as computational costs grow superlinearly with model size, retrieval chain length, and task complexity.

#### 7.1.2. Implicit Token Inflation

Multi-step reasoning frameworks inherently cause significant token inflation through iterative intermediate processes like thought chains, retrieved documents, and verification feedback. Active learning setups consolidate multiple intermediate resultsâ€”retrieved documents, counterfactuals, multi-round validationsâ€”leading to token usage well beyond typical limits. Chain-based retrieval also generates token bloat due to exhaustive candidate path exploration. Iterative reasoning path selection, expansion, and evaluation add heavy token overhead in tasks needing deep reasoning chains involving extensive sequence generation and evaluation. Token usage grows exponentially with task complexity and increases further when intermediate reasoning favors depth or breadth. This inflation raises API costs and memory demands, especially in long-text generation like Deep Research Â (Zheng etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib107)).

#### 7.1.3. Marginal Decline in Retrieval Efficiency

Dynamic retrieval improves knowledge precision but suffers diminishing efficiency as task complexity increases. Adaptive methods reduce retrievals for simple tasks but still require multiple iterations for complex ones, adding significant overhead compared to standard RAG. The tradeoff between retrieval quality and frequency further limits efficiency. High-accuracy retrieval methods incur heavy computational and time costs, negating their efficiency benefits. Even advanced retrieval-trigger optimizations canâ€™t fully remove this overhead due to extra training and deployment costsÂ (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)). This natural efficiency ceiling highlights ongoing challenges in balancing retrieval accuracy and resource use, especially in large, complex tasks.

#### 7.1.4. Toward a Cost Model Framework

Against this backdrop, the development of fine-grained cost models becomes a necessary precondition for balancing effectiveness and efficiency. Existing evaluation metrics, which often rely on single-task performance indicators (such as Exact Match or F1) or coarse-grained runtime statistics, lack the comprehensiveness to jointly model computational resources, token flow, and retrieval overhead. Consequently, they fail to quantify the true tradeoffs in reasoning mechanisms. For instance, while multi-hop reasoning may improve task accuracy, these improvements are frequently offset by exponential growth in token consumption and latency relative to baseline methods. A fine-grained cost model would enable researchers and practitioners to more accurately evaluate the real benefits of reasoning-centric frameworks while addressing the underexplored interplay between computational cost and task performance.

### 7.2. Potential Risk of Over-Thinking

In the process of developing deep thinking models, â€overthinkingâ€ poses a key risk to system efficiency and reliabilityÂ (Fan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib20); Sui etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib75); Chen etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib11); He etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib31); Wang etÂ al., [2025c](https://arxiv.org/html/2504.15909v2#bib.bib82); Cuadron etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib16)), and this issue is further amplified after combining with RAG. It appears as redundant reasoning steps, excessive validation of known conclusions, or unnecessarily broad retrieval scopes, wasting computational resources, increasing error propagation, and degrading performance. For example, in financial risk assessment, an LLM with RAG might retrieve multiple similar market reports and repeatedly verify the same economic indicators rather than focusing on core risks, leading to delayed decisions. This stems from an imbalance between reasoning and retrieval: after accessing external knowledge, the model can enter a â€self-validation loop,â€ repeatedly parsing overlapping or contradictory documents. The generation module, seeking reliability, may trigger further retrievals, creating a feedback loop that worsens inefficiency. This issue is critical in real-time systems like medical diagnosis, where over-retrieval of irrelevant literature can delay urgent decisions.

Case studies show the impact of overthinkingÂ (Sui etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib75)). In legal document interpretation, early reasoning errors can amplify through the retrieval-generation loop, causing retrieval along incorrect paths and yielding illogical conclusions. This error propagation is evident in systems like the Search-o1Â (Li etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib52)), where flawed information extraction misguides subsequent reasoning. In industrial equipment manual interpretation, overextended reasoning with highly similar documents risks obscuring critical parameter differences, increasing procedural errors. These examples illustrate that overthinking not only hampers knowledge integration but also creates safety hazards in practical applications.

To mitigate these risks, researchers propose multiple optimization frameworks. ReaRAGÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) limits reasoning chain length and incorporates self-reflection to prune invalid branches. A simple and effective way is to use a two-stage filtering process, first narrowing documents by metadata, then validating fragment relevance, reducing redundant informationâ€”for instance, retrieving only relevant legal clauses rather than entire regulatory texts. The DeepSeek R1Â (Guo etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib27)) applies reinforcement learning with distillation to penalize redundant steps, cutting repeated formula validation in math proofs by over 40%. These approaches transform open-ended reasoning into controlled, goal-directed processes, using methods like attention weight analysis to measure information gain or confidence functions to evaluate reasoning paths.

Current research balances constraints with model creativity. Knowledge graph-guided reasoning is tested in clinical trials to prioritize key medical features over exhaustive literature retrievalÂ (Chen etÂ al., [2025d](https://arxiv.org/html/2504.15909v2#bib.bib12)). Causal reasoning models aim to break error chains; for example, in financial forecasting, causal graphs restrict reasoning to logically relevant macroeconomic links. Adaptive stopping strategies adjust reasoning depth in customer serviceâ€”simple queries use preset templates, complex issues activate multi-hop reasoning. These advances reshape retrieval-augmented reasoning, with the core challenge being to develop evaluation frameworks that avoid both â€cognitive stagnationâ€ from excessive constraints and â€cognitive overloadâ€ from insufficient control.

Future progress will integrate cognitive science with computational modeling. By mimicking human â€intuition-verificationâ€ decision-making, LLMs could switch seamlessly between rapid response and deep reasoning. In high-risk fields like industrial fault diagnosis, such hybrid models can quickly propose contingency plans after initial retrieval while verifying their validity through deeper analysis. This layered approach reduces overthinking risks and offers a safe, controllable path for applying LLMs in critical industries.

## 8\. Practical Guide

![Refer to caption](https://arxiv.org/html/2504.15909v2/extracted/6386523/Images/practical_guide.png)

Figure 10. Practical guide to synergizing RAG and Reasoning

The combination of RAG and Reasoning is not a one-size-fits-all solution; it requires careful evaluation of each scenarioâ€™s unique needs. As a rapidly evolving and relatively new field, practical applications are still limited, making best practices hard to define. This chapter abstracts and summarizes the key traits of typical RAG+Reasoning application domains and offers practical guidelines for system design based on these features. It provides recommendations on leveraging RAGâ€™s strengths with Reasoning, highlighting priorities, pitfalls to avoid, and current opportunities (FigureÂ [10](https://arxiv.org/html/2504.15909v2#S8.F10 "Figure 10 â€£ 8. Practical Guide â€£ Synergizing RAG and Reasoning: A Systematic Review")). The goal is to promote wider adoption and effective use of this technology in diverse, complex real-world settings.

### 8.1. Domain characteristics

As illustrated in the left part of FigureÂ [10](https://arxiv.org/html/2504.15909v2#S8.F10 "Figure 10 â€£ 8. Practical Guide â€£ Synergizing RAG and Reasoning: A Systematic Review"), we develop a seven-dimensional feature system based on the three core stages of RAGâ€”query, retrieval, and generationâ€”to systematically analyze challenges and adaptation needs across various industries. The query stage emphasizes the complexity of intent understanding and the demand for advanced reasoning, recognizing that industries differ in query abstraction and specificity; some require quickly capturing implicit, deep intentions, while others need complex reasoning. Effective preservation of original semantic meaning during understanding and reasoning is key to improving RAG performance. Retrieval focuses on the systemâ€™s adaptability to diverse and dynamic knowledge sources, which vary from rich multi-domain data to rapidly updating information; frequent updates and fragmented knowledge present challenges that demand effective integration to ensure consistent support for generation. The generation stage requires high-quality outputs, with strict control over hallucinationsâ€”especially critical in sensitive fields like healthcare and lawâ€”along with varying latency requirements for real-time or delayed responses. Explainability and traceability at this stage are essential for system credibility and serve as key evaluation metrics. This comprehensive framework reveals technical bottlenecks and guides improvements, and is applied to analyze four representative domains: finance, healthcare, law, and personal assistants.

#### 8.1.1. Finance

In the finance domain, user queries typically focus on structured needs like investment decisions and risk forecasting. While intent understanding is moderately complex, the system must perform advanced reasoning amid rapidly changing market conditions, relying heavily on external knowledge and frequent updates. For example, portfolio return forecasting integrates time series analysis, policy interpretation, and cross-market reasoning. Retrieval demands handling diverse data sourcesâ€”real-time market data, annual reports, and regulatory filingsâ€”with update cycles often measured in minutes. During generation, strict latency and hallucination control are crucial, as outputs must include decision-making suggestions with full data traceability. Investment research reports, for instance, require annotated key indicators, their data sources, and computation logic to ensure transparency and regulatory compliance. High latency control and robust traceability are essential to maintain transparency and adherence to financial regulations.

#### 8.1.2. Healthcare

Healthcare queries involve complex medical semantic parsing, often with ambiguous terms or incomplete symptoms. For example, â€persistent chest pain with shortness of breathâ€ requires multi-hop reasoning across cardiology, pulmonology, and emergency medicine. Retrieval must integrate electronic health records, medical imaging, and up-to-date clinical guidelines. In generation, hallucination tolerance is minimalâ€”errors in drug dosages or protocols risk malpractice. Therefore, accuracy, timeliness, and explainability are paramount, with every decision step traceable and verifiable.

#### 8.1.3. Legal Services

Legal consultations often require interpreting statutes and citing cases, balancing precise legal terms with natural language nuances. Retrieval depends on structured, infrequently updated sources like case law databases and local regulations. Generation demands accuracyâ€”for instance, drafting contract clauses must precisely cite specific statutes (e.g., Article 472 of the Civil Code) down to the paragraph level for traceability. Explainability is essential, with traceability usually above 95%, and probabilistic language avoided to comply with strict judicial documentation standards.

#### 8.1.4. Personal Assistants

This domain features diverse, dynamic user needs, including schedule management, real-time navigation, and open-domain conversations. Accurate intent disambiguation through contextual awareness is crucial. Retrieval integrates fragmented sources like user behavior logs, geolocation, and social media. Generation latency varies: weather updates require sub-second responses, while travel planning can tolerate 5+ seconds. Hallucination tolerance depends on contextâ€”creative outputs are acceptable for recipes but not for flight information, which demands full accuracy. This necessitates adaptive verification in the RAG system. Though intent complexity is lower than in healthcare or legal fields, the domainâ€™s interaction diversity requires heavy reliance on external knowledge and dynamic balancing of latency and accuracy.

### 8.2. Doâ€™s and Donâ€™ts

Building on aforementioned domain characteristics, we further identify six common scenarios, and derive technical adaptation principles for each. This section outlines key optimization strategies (Doâ€™s) and prohibitions (Donâ€™ts) , to guide the co-design of RAG and reasoning.

#### 8.2.1. Structured Reasoning Scenarios

For scenarios requiring multi-step logical decomposition and structured knowledge dependency, such as _portfolio return prediction_, Chain-of-Thought (CoT) task decomposition and knowledge graph (KG)-driven graph reasoning approaches should be employed. Complex problems should be broken into verifiable sub-tasks, such as coupling market trend analysis with policy impact assessment, while leveraging knowledge graph constraints to ensure logical completeness and auditability. It is essential to incorporate a temporal validation layer to cross-check the consistency of timestamp-sensitive information (e.g., real-time market data or emergent regulatory policies) within a dynamic knowledge base. Approaches that exclude retrieval-based verification of salient features must be avoided, as they may lead to reasoning biases arising from the absence of structured knowledge anchors (e.g., critical indicators from financial statements). Furthermore, the reasoning space of LLMs should be constrained within domain-specific knowledge frameworks to prevent irrelevant or invalid deductions.

#### 8.2.2. Dynamic Demand-Responsive Scenarios

For scenarios characterized by rapidly shifting demands and user preference variability, such as _itinerary planning and multimodal interaction in personal assistant services_, a dynamic adaptation mechanism based on prompt engineering is recommended. By dynamically associating fragmented knowledge units (e.g., user behavior history and real-time traffic updates) with semantic templates and employing heuristic rules for search-space pruning (e.g., prioritizing locally updated information within the past 24 hours), the system can balance contextual adaptability with response speed. Model fine-tuning or reinforcement learning (RLHF/DPO)-based strategy updates should be avoided due to their lengthy iterative cycles and computational overhead, which cannot meet real-time responsiveness requirements, such as millisecond-grade reaction times for last-minute destination changes. Lightweight caching architectures should be implemented within the retrieval system, prioritizing frequently accessed knowledge fragments, such as operating hours of popular tourist attractions, to achieve an equilibrium between dynamism and stability.

#### 8.2.3. Deterministic Decision-Making Scenarios

In scenarios requiring a single, reliable conclusion, such as _clinical diagnosis generation in the healthcare domain_, a multi-level deterministic assurance system should be established. Time-validation layers can filter outdated knowledge (e.g., therapies no longer approved), while field-sensitive retrieval modules trigger predefined decision rules conforming to up-to-date clinical guidelines (e.g., those codified within the latest version of the International Classification of Diseases \[ICD\]). Knowledge graph path constraints should restrict the reasoning process to validated causal links within medical logic (e.g., linking symptom patterns to laboratory test results within corroborated diagnostic pathways), thereby minimizing the likelihood of deviations from standard protocols. Probabilistic exploration strategies that generate alternative hypotheses (e.g., speculative differential diagnoses for atypical pneumonia) should be strictly disallowed to avoid clinical misjudgments. Additionally, delegating decision-making authority to external classification models must be avoided to maintain end-to-end explainability and a clear causal link in the decision-making pipeline.

#### 8.2.4. Time-Sensitive Scenarios

In tasks highly sensitive to response delays, such as _real-time risk warnings and trading decisions in the financial sector_, heuristic rules should be employed to prioritize indexing of frequently queried knowledge units (e.g., volatility indices and liquidity indicators) at the top of the search hierarchy. Directed retrieval expansion strategies that preload potentially associated information (e.g., contractual clauses of derivative instruments tied to underlying assets) can further reduce latency in multi-turn interactions. Monte Carlo Tree Search (MCTS) and other sample-based algorithms are ill-suited for such scenarios due to the excessive computational complexity caused by branch expansion, rendering them infeasible within tight time constraints (e.g., milliseconds). Similarly, the invocation of complex mathematical solvers (e.g., numerical solutions for stochastic differential equations) can introduce uncontrollable delays and should be replaced with lightweight rule-based mechanisms (e.g., threshold-triggering mechanisms based on historical volatility ranges).

#### 8.2.5. Risk-Sensitive Scenarios

For scenarios with minimal tolerance for errors, such as _contract clause generation and citation of judicial interpretations in the legal sector_, a dual-layer defensive mechanism must be employed. A pre-action review layer should validate the compliance of generated content with statutory standards (e.g., ensuring consistency between liability clauses and Article 577 of the Civil Code), while a reliability validation layer performs cross-referencing validation across multiple sources (e.g., aligning Supreme Court precedents with regional court guidelines) to resolve potential conflicts. Retrieval systems must include version control modules to track and update legal references (e.g., automatically flagging repealed local statutes). Unconstrained reinforcement learning-based text generation methods must be avoided, as their exploratory nature risks violating the normative requirements of legal documents (e.g., generating presumptive liability terms unsupported by judicial interpretations). All decision-making actions must pass through deterministic rule engines to filter inadmissible outputs, and the system should never execute decision actions autonomously, such as generating legally binding arbitration notices without oversight.

#### 8.2.6. Complex Path Exploration Scenarios

In exploration tasks involving multiple possible trajectories, such as _differential diagnosis and therapeutic pathway optimization in medicine_, weighted ranking search algorithms should balance search depth and breadth. Knowledge graph topology can guide prioritization (e.g., standard treatment procedures for acute coronary syndrome), while Monte Carlo Tree Search can extend exploration into uncommon differential paths (e.g., rare genetic metabolic disorders). Dynamic pruning threshold functions should be designed (e.g., adjusting the scope of differential diagnosis based on patient history) to eliminate low-confidence hypotheses in real time, thereby controlling computational scale. Brute-force searching of all potential paths (e.g., concurrently testing hundreds of pathogens for nonspecific symptoms) should be avoided to prevent exponential computational scaling. Careful handling of specific token triggers during retrieval (e.g., avoiding spurious associations between â€feverâ€ and unrelated oncological hyperthermia research) is critical to maintaining logical coherence in diagnostic reasoning.

### 8.3. Opportunity Points

Based on the Doâ€™s and Donâ€™ts of current technologies analyzed in the previous section, there remain numerous directions with substantial academic value and application potential that have yet to be fully explored. This section systematically discusses several promising opportunity points across three dimensions: _data and indexing_, _models and methodologies_, and _application services_.

#### 8.3.1. Data and Indexing

##### Cold-Hot Tiered Indexing and Dynamic Context Management

The challenge of managing massive and highly heterogeneous data resources lies in devising an effective _cold-hot tiered indexing_ mechanism that prioritizes data according to their frequency of use and importance. Such a mechanism not only demands classification of data based on timeliness and access frequency but also requires integration with dynamic context management. This allows the system to intelligently retrieve the most relevant data according to the immediate context.

Moreover, a dynamically updated indexing mechanism can mitigate the loss of data timeliness, which often leads to deteriorated inference accuracy. By ensuring access to the most recent and task-appropriate data, this approach reduces redundancy and incorrect retrievals associated with static indexing. When combined with automated task scheduling and resource allocation strategies, fine-grained real-time inference support can be achieved, significantly enhancing the systemâ€™s overall efficiency.

##### Cross-Institution Knowledge Base Construction

The construction of cross-institution or cross-domain knowledge bases offers new opportunities for advancing RAG+Reasoning research. At the core of large-scale cross-institutional knowledge bases lies the optimization of data integration and sharing mechanisms. This entails addressing challenges such as data security and privacy while adopting standardized data interfaces or leveraging federated learning paradigms to enable multidimensional data integration.

Through semantic alignment across multiple sources, entity resolution, and concept abstraction, cross-institutional knowledge can be transformed into authoritative and richly contextualized knowledge bases. These enhanced repositories provide robust contextual support for reasoning tasks and can deliver deeper insights in areas such as healthcare, finance, and urban management.

##### Fine-Grained Layering and Confidence Grading

In scenarios where retrieval and reasoning operate synchronously, the _interpretability_ and _reliability_ of generated outcomes are paramount. Fine-grained layering of data and indices, along with confidence grading of retrieval results, enables the system to selectively use the most trustworthy and relevant subsets of data during different stages of reasoning. This approach fosters transparency and traceability in final decisions or generative outputs.

For instance, in medical diagnosis scenarios, confidence grading can initiate additional verification or expert review in high-risk cases. In the legal domain, confidence layering systematically presents key evidence and identifies sources of uncertainty, reducing reasoning vulnerabilities and minimizing the risk of erroneous conclusions caused by information ambiguity.

#### 8.3.2. Models and Methodologies

##### Event-Driven Active Retrieval

Traditional retrieval mechanisms are predominantly passive. However, _event-driven active retrieval_ presents a promising exploration avenue. By monitoring critical events, such as the injection of new data, user interactions, or changes in external sensors, event-triggered retrieval and reasoning processes can be initiated to capture and respond to potential risks and opportunities in real time. Integrating methodologies such as sequence-based event detection or multitask-learning-based intent recognition can facilitate automatic determination of _when_ and _how_ to trigger retrieval actions. Iteratively optimizing these processes contributes to a more efficient and continuous reasoning loop.

##### Spatiotemporal-Aware Retrieval and Association

Many applications, such as natural disaster monitoring, traffic flow prediction, and inventory management in retail, exhibit strong dependencies on temporal and spatial dimensions. By incorporating spatiotemporal-aware algorithms, retrieval processes can prioritize or emphasize crucial documents according to constraints tied to time and space. This not only enhances timeliness but also improves the purposefulness and accuracy of reasoning.

Furthermore, modeling the evolution of events within spatiotemporal dimensionsâ€”when combined with semantic indexing and vector-based retrieval mechanisms in RAGâ€”can enable more precise characterization and utilization of complex spatiotemporal dynamics during reasoning.

##### Multimodal Fusion in Retrieval and Reasoning

Multimodal data (e.g., text, images, audio, video, and sensor data) collectively constitute a richer contextual environment, offering critical cues for reasoning tasks. However, existing studies are often limited to the retrieval of single or a few data modalities. Advancing research on multimodal fusion and reasoning mechanisms under the RAG+Reasoning framework has the potential to greatly enhance the systemâ€™s capacity for addressing complex queries.

The research focus lies in constructing cross-modal representation learning and alignment methods, enabling unified representations of the same entities or events across different modalities. During retrieval, confidence scores for each modality can be integrated into a comprehensive ranking process, culminating in multimodal-informed joint decision-making during reasoning. This approach not only improves contextual understanding in complex tasks but also broadens the application scope of RAG technologies in scenarios such as expert systems and autonomous driving, where sensory integration and interpretation are critical.

##### Dynamic Risk Propagation Modeling and Management

The tight coupling of retrieval and reasoning with multi-stage decision-making inevitably introduces risk propagation issues. Misjudgments of high-risk or low-confidence documents during upstream retrieval are often inherited by downstream reasoning processes, amplifying uncertainties and increasing error margins. To address this, dynamic risk modeling should be embedded within retrieval workflows, enabling risk quantification, tracking, and management at multiple stages. When necessary, risk mitigation mechanisms or process rollbacks can be triggered, creating a closed-loop correction framework.

Incorporating strategies for analyzing and managing risk propagation is not only a technical challenge but also a matter of system deployment and standardization. In high-stakes domains such as healthcare and financial risk management, establishing comprehensive safety standards and compliance protocols will be crucial. These protocols should treat dynamic risk propagation management as a critical component of evaluating and iterating knowledge retrieval and reasoning systems.

#### 8.3.3. Application Services

##### Validation of Logical Chain Completeness

While RAG with Reasoning can provide partially interpretable reasoning outputs, verifying the completeness of logical chains remains a challenge. Future research could integrate formal verification or symbolic reasoning techniques to ensure consistency and completeness across key reasoning nodes and intermediate conclusions. This would prevent logical gaps or illogical leaps in reasoning, offering robust regulatory support for high-stakes industries such as law and finance.

##### Intervenable Generation During Reasoning

Contemporary Agentic RAG often operate as â€black boxes,â€ rendering external interventions nearly impossible during generative reasoning tasks. However, providing mechanisms for human interventionâ€”such as through visualization or interactive interfacesâ€”could enable experts or users to perform manual corrections, initialize prior knowledge, or modify interim assumptions during the reasoning process. This would substantially enhance the systemâ€™s flexibility and safety.

Specifically, intervenable generation allows not only post hoc error corrections but also proactive identification and rectification of potential risks or biases at earlier stages. Interactive interpretable reasoning platforms or visualization tools grounded in knowledge graphs could empower users to scrutinize and influence reasoning workflows, thereby enhancing confidence and control in decision-making processes across diverse domains.

##### Risk Decision Interception Firewalls

In closed-loop automated tasks such as algorithmic trading or medical diagnostic decision-making, erroneous reasoning outputs can lead to catastrophic outcomes. To mitigate such risks, the system architecture should incorporate _risk decision interception firewalls_, which perform multidimensional validations at critical reasoning nodes or prior to outputting decisions. When confidence levels or high-risk indicators breach thresholds, these firewalls can block decision outputs or escalate them for stricter human review.

This mechanism serves as a â€œfinal line of defenseâ€ for RAG+Reasoning systems, ensuring decision security in large-scale automated information networks. It also provides a robust foundation for compliance and regulatory auditing, enabling safer deployment in critical applications.

##### Edge-Cloud Collaborative Retrieval and Reasoning

With the rapid development of IoT and 5G technologies, many scenarios demand on-site data collection and preliminary processing on edge devices, followed by high-level retrieval and reasoning tasks on cloud platforms. Efficiently partitioning tasks, allocating resources, and maintaining consistency between indexes and models across the edge-cloud continuum represent critical research directions.

Leveraging techniques such as lightweight model compression, distributed index synchronization, and communication optimization can ensure fast reasoning while maximizing resource utilization. Edge-cloud collaborative solutions are particularly impactful for real-time industrial monitoring and smart city applications, reducing network latency and bandwidth bottlenecks while ensuring accurate and timely inference outputs.

In summary, RAG+Reasoning systems present many untapped opportunities across various dimensions. Further research and practical validation could greatly improve their use in complex, high-risk scenarios while fueling new growth in GenAI.

## 9\. Future Trends

In this chapter, we summarize four major trends in technological advancements based on current research, aiming to elucidate and guide the potential future directions of RAG.

### 9.1. The Integration of RAG and Graph

Recent developments have witnessed a growing synergy between RAG systems and graph-based approaches. The intrinsic benefits of graph structures, such as explicit logical relationships and knowledge indexing, have enabled new paradigms for addressing challenges in global reasoning, dynamic data management, and personalized services within RAG systems.

Graph-structured knowledge organization frameworks offer a powerful alternative to traditional vector-based retrieval methods, excelling in modeling complex relationships and supporting global reasoning. For example, GraphRAGÂ (Edge etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib19)) combines hierarchical graph indexing with community detection to extract entity relationship networks from text corpora, enabling large-scale thematic analysis through hierarchical summaries. Building on this, PIKEÂ (Wang etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib83)) introduces a multi-level heterogeneous knowledge graph that organizes documents, semantic segments, and refined knowledge units into a three-layer hierarchy, improving extraction accuracy and multi-hop reasoning via atomized knowledge construction and task decomposition. For dynamic personalization, EMG-RAGÂ (Wang etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib90)) features a three-layer Editable Memory Graph architecture that structures memory data by ontology classification, subclass, and entity relationships, using reinforcement learning to enable real-time updates and multidimensional queries. Together, these advances leverage graph topologies to address the limitations of conventional RAG systemsâ€”such as one-dimensional representation and weak contextual linksâ€”enabling multilevel reasoning from local fact retrieval to global thematic summarization and forming a foundation for interpretable, adaptive RAG systems.

Symbolic Reasoning. Graph-structured symbolic reasoning methods leverage the multi-hop reasoning power of Knowledge Graphs (KG) to better manage complex semantic and logical relationships. Frameworks like HippoRAG2 and the Think-on-Graph (ToG)Â (Ma etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib61)) series exemplify this. HippoRAG2Â (GutiÃ©rrez etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib29)) builds open knowledge graphs and uses personalized PageRank with a dense-sparse coding approach inspired by brain memory, boosting performance in factual memory, semantic understanding, and multi-hop reasoning. Likewise, ToG-2 combines iterative retrieval of knowledge graphs and documents, using relationship discovery, entity pruning, and context-driven graph searches to integrate fine-grained information from unstructured text, enhancing implicit relationship detection.

Task Planning. Graph-based task planning in RAG systems enhances complex problem-solving by overcoming the limitations of traditional linear workflows, which struggle with multi-step or multimodal reasoning. These approaches build dynamic knowledge graphs, like Mind Maps, to explicitly model logical dependencies and context. For instance, the Agentic ReasoningÂ (Wu etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib93)) transforms reasoning chains into graph structures for entity extraction, relation identification, and community clustering, enabling dynamic path tracking and optimized retrieval, excelling in tasks like doctoral-level GPQAÂ (Rein etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib68)). Collaborative frameworks such as Co-STORM extend this to multi-agent scenarios, representing queries, tool calls, and knowledge integration as traversable graph nodes to support task decomposition and adaptive reasoning.

Tool Usage and Management. Graph-enhanced approaches to tool management overcome limitations of traditional dependency modeling by effectively capturing complex relationships like parameter passing, functional collaboration, and resource management. Graph RAG-Tool FusionÂ (Lumer etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib58)) models tools as graph nodes within a dual-layer architecture of core system APIs and domain-specific tools, encoding direct and indirect dependencies as edges. It uses a two-stage retrieval process: vector-based tool retrieval followed by a graph-based depth-first search to assemble dependency-compliant toolsets.

### 9.2. Multi-Model Collaboration

Multi-model collaboration has emerged as a pivotal strategy for enhancing task complexity handling and domain adaptability in RAG systemsÂ (Chen etÂ al., [2025a](https://arxiv.org/html/2504.15909v2#bib.bib14)). By integrating the strengths of different models, this approach achieves optimized performance. For example, the CR-PlannerÂ (Li etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib53)) combines general-purpose generation models (e.g., GPT-4) with domain-specific critic models (e.g., Llama-3-8B). This hybrid system dynamically orchestrates subgoal planning and execution evaluation, utilizing MCTS to generate high-quality training data. Similarly, UARÂ (Cheng etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib15))employs intent-aware and knowledge-requirement classifiers to dynamically trigger retrieval, decoupling lightweight classification tasks from resource-intensive decoding operations of LLMs. Furthermore, Adaptive-RAG Â (Jeong etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib42)) deploys small-complexity classifiers to route queries into different levels of processing strategies, balancing response speed for simple queries with deep reasoning for complex ones. These strategies form a closed â€generation-evaluationâ€loop, leveraging complementary strengths across models to achieve improved accuracy and computational efficiency.

### 9.3. Multi-Modal Collaboration

The breakthrough in Chain-of-Thought (CoT) capabilities of language models has catalyzed the transition of multimodal reasoning from perceptual-level integration to cognitive-level reasoning, promoting Multimodal Collaborative Reasoning as a key trendÂ (Bi etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib5)) By deeply integrating the logical reasoning capabilities of language models with the spatial-semantic representation of multimodal data, it significantly enhances information synthesis in complex scenariosÂ (Abootorabi etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib3)). For instance, in the medical domain, multimodal RAG systems such as MedCoTÂ (Liu etÂ al., [2024](https://arxiv.org/html/2504.15909v2#bib.bib57)) utilize hierarchical expert systems to integrate CT imaging and pathology reports, enabling knowledge graph validation of diagnostic hypotheses and reducing misdiagnosis risks. Future research will likely focus on robust cross-modal knowledge alignment, progressive knowledge distillation, and adaptive reasoning frameworks.

### 9.4. Customized Reinforcement Learning

The application of reinforcement learning (RL) in RAG systems has become instrumental in improving module coordination and enhancing overall efficiency. Recent studies focus on designing reward mechanisms tailored to the specific needs of RAG systems. Frameworks such as RAG-GymÂ (Xiong etÂ al., [2025b](https://arxiv.org/html/2504.15909v2#bib.bib97)) and DeepRAGÂ (Guan etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib25)) model reasoning processes using Markov Decision Processes and introduce fine-grained process supervision mechanisms. Additionally, ReARTeRÂ (Lee etÂ al., [2025](https://arxiv.org/html/2504.15909v2#bib.bib50)) and SmartRAGÂ (Gao etÂ al., [2024a](https://arxiv.org/html/2504.15909v2#bib.bib21)) incorporate trust-aware reward strategies and end-to-end policy optimization to achieve superior accuracy and robustness. Opportunities remain for further exploring automated reward modeling with LLMs to facilitate fine-grained supervision.

## 10\. Conclusion

This paper has systematically reviewed the synergistic integration of Retrieval-Augmented Generation (RAG) and reasoning, providing a formal definition of reasoning within the RAG framework as a structured, multi-step, goal-driven process that dynamically combines parametric and retrieved knowledge to address complex problems.

We presented a comprehensive taxonomy covering the purposes, collaboration paradigms, and implementation methods underlying RAG+Reasoning systems. The synergy enables more precise retrieval informed by logical analysis and enhances reasoning with contextually relevant, up-to-date knowledge beyond parametric limitations.

While the enhanced reasoning capabilities allow tackling complex knowledge-intensive tasks such as deep research, expert-level problem solving, and domain-specific decision support, practical challenges remain. These include computational and token costs that grow non-linearly, risks of overthinking leading to inefficiency and error propagation, and the lack of evaluation frameworks that effectively assess intermediate reasoning quality alongside final results.

To bridge the gap from theory to real-world application, we proposed practical design guidelines tailored to diverse domains like finance, healthcare, law, and personal assistants, emphasizing adaptability to heterogeneous, dynamic knowledge sources and strict requirements for output reliability and traceability.

Finally, we identified promising directions for future research, including graph-structured knowledge integration, multimodal and multi-model collaborative reasoning architectures, and advanced reinforcement learning techniques for optimizing retrieval-reasoning workflows.

Overall, this work establishes both a theoretical foundation and practical roadmap to drive the development of next-generation RAG+Reasoning systems capable of robust, transparent, and efficient cognition, paving the way for impactful applications across academia and industry.

## References

-   (1)
-   Abdallah etÂ al. (2025) Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, and Adam Jatowt. 2025. Rankify: A comprehensive python toolkit for retrieval, re-ranking, and retrieval-augmented generation. _arXiv preprint arXiv:2502.02464_ (2025).
-   Abootorabi etÂ al. (2025) MohammadÂ Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, MahdiehÂ Soleymani Baghshah, and Ehsaneddin Asgari. 2025. Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation. _arXiv preprint arXiv:2502.08826_ (2025).
-   Asai etÂ al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In _The Twelfth International Conference on Learning Representations_.
-   Bi etÂ al. (2025b) Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, etÂ al. 2025b. Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1). _arXiv preprint arXiv:2504.03151_ (2025).
-   Bi etÂ al. (2025a) Yuxi Bi, Yunfan Gao, and Haofen Wang. 2025a. StePO-Rec: Towards Personalized Outfit Styling Assistant via Knowledge-Guided Multi-Step Reasoning. _arXiv preprint arXiv:2504.09915_ (2025).
-   Chen etÂ al. (2025b) Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, JeffÂ Z Pan, etÂ al. 2025b. Learning to Reason with Search for LLMs via Reinforcement Learning. _arXiv preprint arXiv:2503.19470_ (2025).
-   Chen etÂ al. (2025f) PeterÂ Baile Chen, Yi Zhang, Michael Cafarella, and Dan Roth. 2025f. Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method. _arXiv preprint arXiv:2501.18539_ (2025).
-   Chen etÂ al. (2025c) Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. 2025c. Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. _arXiv preprint arXiv:2503.09567_ (2025).
-   Chen etÂ al. (2023) Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. Theoremqa: A theorem-driven question answering dataset. _arXiv preprint arXiv:2305.12524_ (2023).
-   Chen etÂ al. (2024) Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, etÂ al. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. _arXiv preprint arXiv:2412.21187_ (2024).
-   Chen etÂ al. (2025d) Yixiang Chen, Penglei Sun, Xiang Li, and Xiaowen Chu. 2025d. MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation. _arXiv preprint arXiv:2504.07724_ (2025).
-   Chen etÂ al. (2025e) Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. 2025e. Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning. _arXiv preprint arXiv:2501.15228_ (2025).
-   Chen etÂ al. (2025a) Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, and PhilipÂ S Yu. 2025a. Harnessing Multiple Large Language Models: A Survey on LLM Ensemble. _arXiv preprint arXiv:2502.18036_ (2025).
-   Cheng etÂ al. (2024) Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified active retrieval for retrieval augmented generation. _arXiv preprint arXiv:2406.12534_ (2024).
-   Cuadron etÂ al. (2025) Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, LuisÂ Gaspar Schroeder, Tian Xia, Huanzhi Mao, etÂ al. 2025. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks. _arXiv preprint arXiv:2502.08235_ (2025).
-   Dao and Le (2025) Alan Dao and Thinh Le. 2025. ReZero: Enhancing LLM search ability by trying one-more-time. arXiv:2504.11001Â \[cs.CL\] [https://arxiv.org/abs/2504.11001](https://arxiv.org/abs/2504.11001)
-   Dettmers etÂ al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. _Advances in neural information processing systems_ 36 (2023), 10088â€“10115.
-   Edge etÂ al. (2024) Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, RobertÂ Osazuwa Ness, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. _arXiv preprint arXiv:2404.16130_ (2024).
-   Fan etÂ al. (2025) Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. 2025. Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill? _arXiv preprint arXiv:2504.06514_ (2025).
-   Gao etÂ al. (2024a) Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. 2024a. SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback. _arXiv preprint arXiv:2410.18141_ (2024).
-   Gao etÂ al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_ (2023).
-   Gao etÂ al. (2024b) Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024b. Modular rag: Transforming rag systems into lego-like reconfigurable frameworks. _arXiv preprint arXiv:2407.21059_ (2024).
-   Gao etÂ al. (2025) Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, and SÂ Kevin Zhou. 2025. FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs. _arXiv preprint arXiv:2501.09957_ (2025).
-   Guan etÂ al. (2025) Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025. DeepRAG: Thinking to Retrieval Step by Step for Large Language Models. _arXiv preprint arXiv:2502.01142_ (2025).
-   Guo etÂ al. (2025a) Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, etÂ al. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. _arXiv preprint arXiv:2501.12948_ (2025).
-   Guo etÂ al. (2025b) Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, etÂ al. 2025b. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. _arXiv preprint arXiv:2501.12948_ (2025).
-   Guo etÂ al. (2024) Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag: Simple and fast retrieval-augmented generation. (2024).
-   GutiÃ©rrez etÂ al. (2025) BernalÂ JimÃ©nez GutiÃ©rrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From RAG to Memory: Non-Parametric Continual Learning for Large Language Models. _arXiv preprint arXiv:2502.14802_ (2025).
-   He etÂ al. (2024) Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, ZhenÂ Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, etÂ al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. _arXiv preprint arXiv:2402.14008_ (2024).
-   He etÂ al. (2025) Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, etÂ al. 2025. Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning? _arXiv preprint arXiv:2502.19361_ (2025).
-   Ho etÂ al. (2020) Xanh Ho, Anh-KhoaÂ Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. _arXiv preprint arXiv:2011.01060_ (2020).
-   Hong etÂ al. (2025) Yubin Hong, Chaofan Li, Jingyi Zhang, and Yingxia Shao. 2025. FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG. _arXiv preprint arXiv:2504.07103_ (2025).
-   Hongjin etÂ al. (2024) SU Hongjin, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Liu Haisu, Quan Shi, ZacharyÂ S Siegel, Michael Tang, etÂ al. 2024. BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In _The Thirteenth International Conference on Learning Representations_.
-   Hsu etÂ al. (2024) Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. 2024. Grounding by trying: Llms with reinforcement learning-enhanced retrieval. _arXiv preprint arXiv:2410.23214_ (2024).
-   Hu (2025) Jian Hu. 2025. REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models. _arXiv preprint arXiv:2501.03262_ (2025).
-   Hu etÂ al. (2025) Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. 2025. MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search. _arXiv preprint arXiv:2503.20757_ (2025).
-   Huot etÂ al. (2024) Fantine Huot, ReinaldÂ Kim Amplayo, Jennimaria Palomaki, AliceÂ Shoshana Jakobovits, Elizabeth Clark, and Mirella Lapata. 2024. Agentsâ€™ Room: Narrative Generation through Multi-step Collaboration. _arXiv preprint arXiv:2410.02603_ (2024).
-   Islam etÂ al. (2024) ShayekhÂ Bin Islam, MdÂ Asib Rahman, KSM Hossain, Enamul Hoque, Shafiq Joty, and MdÂ Rizwan Parvez. 2024. Open-rag: Enhanced retrieval-augmented reasoning with open-source large language models. _arXiv preprint arXiv:2410.01782_ (2024).
-   Jaech etÂ al. (2024) Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, etÂ al. 2024. Openai o1 system card. _arXiv preprint arXiv:2412.16720_ (2024).
-   Jain etÂ al. (2024) Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. _arXiv preprint arXiv:2403.07974_ (2024).
-   Jeong etÂ al. (2024) Soyeong Jeong, Jinheon Baek, Sukmin Cho, SungÂ Ju Hwang, and JongÂ C Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. _arXiv preprint arXiv:2403.14403_ (2024).
-   Jiang (2025) Pengcheng Jiang. 2025. DeepRetrieval: Powerful Query Generation for Information Retrieval with Reinforcement Learning. _arXiv preprint arXiv:2503.00223_ (2025).
-   Jiang etÂ al. (2024a) Yucheng Jiang, Yijia Shao, Dekun Ma, SinaÂ J Semnani, and MonicaÂ S Lam. 2024a. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. _arXiv preprint arXiv:2408.15232_ (2024).
-   Jiang etÂ al. (2024b) Yucheng Jiang, Yijia Shao, Dekun Ma, SinaÂ J Semnani, and MonicaÂ S Lam. 2024b. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. _arXiv preprint arXiv:2408.15232_ (2024).
-   Jiang etÂ al. (2023) Zhengbao Jiang, FrankÂ F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_. 7969â€“7992.
-   Joshi etÂ al. (2024) Ashutosh Joshi, SheikhÂ Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, and Juhi Naik. 2024. REAPER: Reasoning based retrieval planning for complex RAG systems. In _Proceedings of the 33rd ACM International Conference on Information and Knowledge Management_. 4621â€“4628.
-   Kwiatkowski etÂ al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, etÂ al. 2019. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_ 7 (2019), 453â€“466.
-   Lee etÂ al. (2024) Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A plan-then-retrieval augmented generation for generative large language models as decision makers. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. 6537â€“6555.
-   Lee etÂ al. (2025) Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi Li. 2025. ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation. _arXiv preprint arXiv:2503.21729_ (2025).
-   Li etÂ al. (2024c) Jinzheng Li, Jingshu Zhang, Hongguang Li, and Yiqing Shen. 2024c. An Agent Framework for Real-Time Financial Information Searching with Large Language Models. _arXiv preprint arXiv:2502.15684_ (2024).
-   Li etÂ al. (2025a) Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic search-enhanced large reasoning models. _arXiv preprint arXiv:2501.05366_ (2025).
-   Li etÂ al. (2024a) Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. 2024a. Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks. _arXiv preprint arXiv:2410.01428_ (2024).
-   Li etÂ al. (2024b) Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. 2024b. Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks. _arXiv preprint arXiv:2410.01428_ (2024).
-   Li etÂ al. (2025b) Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and Le Sun. 2025b. Deepsolution: Boosting complex engineering solution design via tree-based exploration and bi-point thinking. _arXiv preprint arXiv:2502.20730_ (2025).
-   Lightman etÂ al. (2023) Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Letâ€™s verify step by step. In _The Twelfth International Conference on Learning Representations_.
-   Liu etÂ al. (2024) Jiaxiang Liu, Yuan Wang, Jiawei Du, JoeyÂ Tianyi Zhou, and Zuozhu Liu. 2024. Medcot: Medical chain of thought via hierarchical expert. _arXiv preprint arXiv:2412.13736_ (2024).
-   Lumer etÂ al. (2025) Elias Lumer, PradeepÂ Honaganahalli Basavaraju, Myles Mason, JamesÂ A Burke, and VamseÂ Kumar Subbiah. 2025. Graph RAG-Tool Fusion. _arXiv preprint arXiv:2502.07223_ (2025).
-   Luo etÂ al. (2025) Haoran Luo, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, LuuÂ Anh Tuan, etÂ al. 2025. KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search. _arXiv preprint arXiv:2501.18922_ (2025).
-   Lyu etÂ al. (2025) Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2025. Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models. _ACM Transactions on Information Systems_ 43, 2 (2025), 1â€“32.
-   Ma etÂ al. (2024) Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2024. Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation. _arXiv preprint arXiv:2407.10805_ (2024).
-   Ma etÂ al. (2023) Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrieval-augmented large language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_. 5303â€“5315.
-   Mialon etÂ al. (2023) GrÃ©goire Mialon, ClÃ©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants. In _The Twelfth International Conference on Learning Representations_.
-   Muennighoff etÂ al. (2025) Niklas Muennighoff, Zitong Yang, Weijia Shi, XiangÂ Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. _arXiv preprint arXiv:2501.19393_ (2025).
-   Patil etÂ al. (2024) ShishirÂ G Patil, Tianjun Zhang, Xin Wang, and JosephÂ E Gonzalez. 2024. Gorilla: Large language model connected with massive apis. _Advances in Neural Information Processing Systems_ 37 (2024), 126544â€“126565.
-   Petroni etÂ al. (2020) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola DeÂ Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, etÂ al. 2020. KILT: a benchmark for knowledge intensive language tasks. _arXiv preprint arXiv:2009.02252_ (2020).
-   Pezeshkpour and Hruschka (2025) Pouya Pezeshkpour and Estevam Hruschka. 2025. Insight-RAG: Enhancing LLMs with Insight-Driven Augmentation. _arXiv preprint arXiv:2504.00187_ (2025).
-   Rein etÂ al. (2024) David Rein, BettyÂ Li Hou, AsaÂ Cooper Stickland, Jackson Petty, RichardÂ Yuanzhe Pang, Julien Dirani, Julian Michael, and SamuelÂ R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In _First Conference on Language Modeling_.
-   Shao etÂ al. (2023) Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. _arXiv preprint arXiv:2305.15294_ (2023).
-   Shao etÂ al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, etÂ al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. _arXiv preprint arXiv:2402.03300_ (2024).
-   Shi etÂ al. (2024a) Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024a. Can Language Models Solve Olympiad Programming? _arXiv preprint arXiv:2404.10952_ (2024).
-   Shi etÂ al. (2024b) Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024b. Can Language Models Solve Olympiad Programming? _arXiv preprint arXiv:2404.10952_ (2024).
-   Song etÂ al. (2025) Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, WayneÂ Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. _arXiv preprint arXiv:2503.05592_ (2025).
-   Srinivas and Runkana (2025) SakhinanaÂ Sagar Srinivas and Venkataramana Runkana. 2025. Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding. _arXiv preprint arXiv:2504.01281_ (2025).
-   Sui etÂ al. (2025) Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, etÂ al. 2025. Stop overthinking: A survey on efficient reasoning for large language models. _arXiv preprint arXiv:2503.16419_ (2025).
-   Sun etÂ al. (2025) Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and Han Li. 2025. ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding. _arXiv preprint arXiv:2501.07861_ (2025).
-   Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. _arXiv preprint arXiv:1803.06643_ (2018).
-   Tran etÂ al. (2024) Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. 2024. RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models. _arXiv preprint arXiv:2412.02830_ (2024).
-   Trivedi etÂ al. (2022a) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. _arXiv preprint arXiv:2212.10509_ (2022).
-   Trivedi etÂ al. (2022b) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b. MuSiQue: Multihop Questions via Single-hop Question Composition. _Transactions of the Association for Computational Linguistics_ 10 (2022), 539â€“554.
-   Vu etÂ al. (2023) Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, etÂ al. 2023. Freshllms: Refreshing large language models with search engine augmentation. _arXiv preprint arXiv:2310.03214_ (2023).
-   Wang etÂ al. (2025c) Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, and Dong Yu. 2025c. Donâ€™t Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls. _arXiv preprint arXiv:2502.11183_ (2025).
-   Wang etÂ al. (2025b) Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025b. PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation. _arXiv preprint arXiv:2501.11551_ (2025).
-   Wang etÂ al. (2025a) Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025a. Chain-of-Retrieval Augmented Generation. _arXiv preprint arXiv:2501.14342_ (2025).
-   Wang etÂ al. (2024e) Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, etÂ al. 2024e. Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation. _arXiv preprint arXiv:2410.08821_ (2024).
-   Wang etÂ al. (2024b) Siqi Wang, Chao Liang, Yunfan Gao, Yang Liu, Jing Li, and Haofen Wang. 2024b. Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT. In _Proceedings of the 32nd ACM International Conference on Multimedia_. 4757â€“4765.
-   Wang etÂ al. (2024c) Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, and Zhicheng Dou. 2024c. Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation. _arXiv preprint arXiv:2406.05654_ (2024).
-   Wang etÂ al. (2023) Xidong Wang, GuimingÂ Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, etÂ al. 2023. Cmb: A comprehensive medical benchmark in chinese. _arXiv preprint arXiv:2308.08833_ (2023).
-   Wang etÂ al. (2024d) Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, etÂ al. 2024d. Searching for best practices in retrieval-augmented generation. _arXiv preprint arXiv:2407.01219_ (2024).
-   Wang etÂ al. (2024a) Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, and Wei Shi. 2024a. Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs. _arXiv preprint arXiv:2409.19401_ (2024).
-   Wang etÂ al. (2025d) Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linpeng Tang, Wentao Zhang, etÂ al. 2025d. RARE: Retrieval-Augmented Reasoning Modeling. _arXiv preprint arXiv:2503.23513_ (2025).
-   Weng etÂ al. (2024) Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2024. Cycleresearcher: Improving automated research via automated review. _arXiv preprint arXiv:2411.00816_ (2024).
-   Wu etÂ al. (2025b) Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025b. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research. _arXiv preprint arXiv:2502.04644_ (2025).
-   Wu etÂ al. (2025a) Wenjie Wu, Yongcheng Jing, Yingjie Wang, Wenbin Hu, and Dacheng Tao. 2025a. Graph-augmented reasoning: Evolving step-by-step knowledge graph retrieval for llm reasoning. _arXiv preprint arXiv:2503.01642_ (2025).
-   Xi etÂ al. (2025) Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, and Huajun Chen. 2025. OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking. _arXiv preprint arXiv:2501.09751_ (2025).
-   Xiao etÂ al. (2025) Liang Xiao, Wen Dai, Shuai Chen, Bin Qin, Chongyang Shi, Haopeng Jing, and Tianyu Guo. 2025. Retrieval-Augmented Generation by Evidence Retroactivity in LLMs. _arXiv preprint arXiv:2501.05475_ (2025).
-   Xiong etÂ al. (2025b) Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, etÂ al. 2025b. Rag-gym: Optimizing reasoning and search agents with process supervision. _arXiv preprint arXiv:2502.13957_ (2025).
-   Xiong etÂ al. (2025c) Guanming Xiong, Haochen Li, and Wen Zhao. 2025c. MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering. _arXiv preprint arXiv:2502.13428_ (2025).
-   Xiong etÂ al. (2025a) Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, and JÃ¼rgen Schmidhuber. 2025a. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models. _arXiv preprint arXiv:2503.08275_ (2025).
-   Xu etÂ al. (2025) Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, etÂ al. 2025. Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models. _arXiv preprint arXiv:2501.09686_ (2025).
-   Xu etÂ al. (2024) Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Chaojun Xiao, Zhiyuan Liu, Ge Yu, and Chenyan Xiong. 2024. ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents. _arXiv preprint arXiv:2402.13547_ (2024).
-   Yan etÂ al. (2025) Ruiran Yan, Zheng Liu, and Defu Lian. 2025. O1 embedder: Let retrievers think before action. _arXiv preprint arXiv:2502.07555_ (2025).
-   Zhang etÂ al. (2024) Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei Zhang. 2024. Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering. _arXiv preprint arXiv:2408.11875_ (2024).
-   Zhang etÂ al. (2025) Zhuocheng Zhang, Yang Feng, and Min Zhang. 2025. LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers. _arXiv preprint arXiv:2502.18139_ (2025).
-   Zhao etÂ al. (2024) Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and NoahÂ A Smith. 2024. Set the clock: Temporal alignment of pretrained language models. _arXiv preprint arXiv:2402.16797_ (2024).
-   Zhao etÂ al. (2025) Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chunyan Miao. 2025. MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot. _arXiv preprint arXiv:2502.04413_ (2025).
-   Zheng etÂ al. (2025) Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments. _arXiv preprint arXiv:2504.03160_ (2025).
-   Zhong etÂ al. (2025) Yijie Zhong, Feifan Wu, Mengying Guo, Xiaolian Zhang, Meng Wang, and Haofen Wang. 2025. Meta-PKE: Memory-Enhanced Task-Adaptive Personal Knowledge Extraction in Daily Life. _Information Processing & Management_ 62, 4 (2025), 104097.
-   Zhou etÂ al. (2024) Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. 2024. Metacognitive retrieval-augmented large language models. In _Proceedings of the ACM Web Conference 2024_. 1453â€“1463.
-   Zhu etÂ al. (2025b) Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, and Weinan Zhang. 2025b. Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning. _arXiv preprint arXiv:2502.14361_ (2025).
-   Zhu etÂ al. (2025a) Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. 2025a. Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering. _arXiv preprint arXiv:2502.14245_ (2025).

## Appendix

### Agentic RAG Symbol Reference System

The following table presents a complete symbol reference system with formally defined mathematical notations for all core concepts.

Table 3. Basic states and system components

Table 4. Action space and policy definitions

Table 5. State transition mechanisms

Table 6. Feedback and optimization components

Table 7. Submodule-specific symbols

### Symbol Design Hierarchy

-   â€¢
    
    Base states/actions: Standard font (St,atsubscriptğ‘†ğ‘¡subscriptğ‘ğ‘¡S\_{t},a\_{t}italic\_S start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT)
    
-   â€¢
    
    Sets/spaces: Calligraphic font (ğ’œ,ğ’¦tğ’œsubscriptğ’¦ğ‘¡\\mathcal{A},\\mathcal{K}\_{t}caligraphic\_A , caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT)
    
-   â€¢
    
    Core mechanism functions: Uppercase Greek (Î¨,Î“Î¨Î“\\Psi,\\Gammaroman\_Î¨ , roman\_Î“)
    
-   â€¢
    
    Operational functions: Calligraphic font (â„›,ğ’¯aâ„›subscriptğ’¯ğ‘\\mathcal{R},\\mathcal{T}\_{a}caligraphic\_R , caligraphic\_T start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT)
    
-   â€¢
    
    Auxiliary functions: Lowercase Greek (Î´,Ï•ğ›¿italic-Ï•\\delta,\\phiitalic\_Î´ , italic\_Ï•) or blackboard bold (ğ•€ğ•€\\mathbb{I}blackboard\_I)
    

### Annotation Guidelines

-   â€¢
    
    Symbol disambiguation:
    
    -   â€“
        
        â„›â„›\\mathcal{R}caligraphic\_R strictly denotes retrieval function (vs. reward Rğ‘…Ritalic\_R)
        
    -   â€“
        
        Î´ğ›¿\\deltaitalic\_Î´ exclusively represents state transitions (vs. branch selector Ïˆğœ“\\psiitalic\_Ïˆ)
        
    
-   â€¢
    
    Dynamic extensions:
    
    -   â€“
        
        Action space ğ’œğ’œ\\mathcal{A}caligraphic\_A and knowledge base ğ’¦tsubscriptğ’¦ğ‘¡\\mathcal{K}\_{t}caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT support incremental updates: ğ’¦t+1\=ğ’¦tâŠ•Retrieveâ¢(qt)subscriptğ’¦ğ‘¡1direct-sumsubscriptğ’¦ğ‘¡Retrievesubscriptğ‘ğ‘¡\\mathcal{K}\_{t+1}=\\mathcal{K}\_{t}\\oplus\\text{Retrieve}(q\_{t})caligraphic\_K start\_POSTSUBSCRIPT italic\_t + 1 end\_POSTSUBSCRIPT = caligraphic\_K start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âŠ• Retrieve ( italic\_q start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT )